1, TITLE: Learning to Understand Child-directed and Adult-directed Speech
https://www.aclweb.org/anthology/2020.acl-main.1
AUTHORS:                Lieke Gelderloos, Grzegorz ChrupaÅ‚a, Afra Alishahi
HIGHLIGHT:              This study explores the effect of child-directed speech when learning to extract semantic information from
speech directly.

2, TITLE: Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts
https://www.aclweb.org/anthology/2020.acl-main.2
AUTHORS:                Alex Rinaldi, Jean Fox Tree, Snigdha Chaturvedi
HIGHLIGHT:              We propose JLPC, a model that analyzes interview transcripts to identify depression while jointly categorizing
interview prompts into latent categories.

3, TITLE: Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling
https://www.aclweb.org/anthology/2020.acl-main.3
AUTHORS:                Zihan Liu, Genta Indra Winata, Peng Xu, Pascale Fung
HIGHLIGHT:              In this paper, we propose a Coarse-to-fine approach (Coach) for cross-domain slot filling.

4, TITLE: Designing Precise and Robust Dialogue Response Evaluators
https://www.aclweb.org/anthology/2020.acl-main.4
AUTHORS:                Tianyu Zhao, Divesh Lala, Tatsuya Kawahara
HIGHLIGHT:              In this work, we propose to build a reference-free evaluator and exploit the power of semi-supervised training
and pretrained (masked) language models.

5, TITLE: Dialogue State Tracking with Explicit Slot Connection Modeling
https://www.aclweb.org/anthology/2020.acl-main.5
AUTHORS:                Yawen Ouyang, Moxin Chen, Xinyu Dai, Yinggong Zhao, Shujian Huang, Jiajun CHEN
HIGHLIGHT:              To handle these phenomena, we propose a Dialogue State Tracking with Slot Connections (DST-SC) model to
explicitly consider slot correlations across different domains.

6, TITLE: Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy
https://www.aclweb.org/anthology/2020.acl-main.6
AUTHORS:                Xiexiong Lin, Weiyu Jian, Jianshan He, Taifeng Wang, Wei Chu
HIGHLIGHT:              To address this issue, this paper proposes a method that uses recurrent knowledge interaction among response
decoding steps to incorporate appropriate knowledge.

7, TITLE: Guiding Variational Response Generator to Exploit Persona
https://www.aclweb.org/anthology/2020.acl-main.7
AUTHORS:                Bowen Wu, MengYuan Li, Zongsheng Wang, Yifu Chen, Derek F. Wong, qihang feng, Junhong Huang,
Baoxun Wang
HIGHLIGHT:              This paper proposes to adopt the personality-related characteristics of human conversations into variational
response generators, by designing a specific conditional variational autoencoder based deep model with two new regularization terms
employed to the loss function, so as to guide the optimization towards the direction of generating both persona-aware and relevant
responses.

8, TITLE: Large Scale Multi-Actor Generative Dialog Modeling
https://www.aclweb.org/anthology/2020.acl-main.8
AUTHORS:                Alex Boyd, Raul Puri, Mohammad Shoeybi, Mostofa Patwary, Bryan Catanzaro
HIGHLIGHT:              This work introduces the Generative Conversation Control model, an augmented and fine-tuned GPT-2
language model that conditions on past reference conversations to probabilistically model multi-turn conversations in the actor's
persona.

9, TITLE: PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable
https://www.aclweb.org/anthology/2020.acl-main.9
AUTHORS:                Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang
HIGHLIGHT:              Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of
conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering.

10, TITLE:              Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network
https://www.aclweb.org/anthology/2020.acl-main.10
AUTHORS:                Yangming Li, Kaisheng Yao, Libo Qin, Wanxiang Che, Xiaolong Li, Ting Liu
HIGHLIGHT:              In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue
act (DA) properly generated in output sentences.

11, TITLE:              Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations
https://www.aclweb.org/anthology/2020.acl-main.11
AUTHORS:                Samuel Coope, Tyler Farghly, Daniela Gerz, Ivan VuliÄ‡, Matthew Henderson
HIGHLIGHT:              We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based
span extraction task.

12, TITLE:              Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking
https://www.aclweb.org/anthology/2020.acl-main.12
AUTHORS:                Giovanni Campagna, Agata Foryciarz, Mehrad Moradshahi, Monica Lam
HIGHLIGHT:              This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain
training data are all synthesized from an abstract dialogue model and the ontology of the domain.

13, TITLE:              A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle
https://www.aclweb.org/anthology/2020.acl-main.13
AUTHORS:                Shyh-Shiun Hung, Hen-Hsen Huang, Hsin-Hsi Chen
HIGHLIGHT:              This work proposes a standalone, complete Chinese discourse parser for practical applications.

14, TITLE:              TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition
https://www.aclweb.org/anthology/2020.acl-main.14
AUTHORS:                Ruifang He, Jian Wang, Fengyu Guo, Yugui Han
HIGHLIGHT:              Therefore, we propose a novel TransS-driven joint learning architecture to address the issues.

15, TITLE:              A Study of Non-autoregressive Model for Sequence Generation
https://www.aclweb.org/anthology/2020.acl-main.15
AUTHORS:                Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, sheng zhao, Tie-Yan Liu
HIGHLIGHT:              To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of
different NAR sequence generation tasks.

16, TITLE:              Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage
https://www.aclweb.org/anthology/2020.acl-main.16
AUTHORS:                Ashish V. Thapliyal, Radu Soricut
HIGHLIGHT:              We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages directly at
training time both existing English annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it
generates first an English caption and then a corresponding target-language caption.

17, TITLE:              Fact-based Text Editing
https://www.aclweb.org/anthology/2020.acl-main.17
AUTHORS:                Hayate Iso, Chao Qiao, Hang Li
HIGHLIGHT:              We propose a novel text editing task, referred to as \textit{fact-based text editing}, in which the goal is to revise
a given document to better describe the facts in a knowledge base (e.g., several triples).

18, TITLE:              Few-Shot NLG with Pre-Trained Language Model
https://www.aclweb.org/anthology/2020.acl-main.18
AUTHORS:                Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, William Yang Wang
HIGHLIGHT:              In this work, we propose the new task of few-shot natural language generation.

19, TITLE:              Fluent Response Generation for Conversational Question Answering
https://www.aclweb.org/anthology/2020.acl-main.19
AUTHORS:                Ashutosh Baheti, Alan Ritter, Kevin Small
HIGHLIGHT:              In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate
fluent grammatical answer responses while maintaining correctness.

20, TITLE:              Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical
Conditional VAEs
https://www.aclweb.org/anthology/2020.acl-main.20
AUTHORS:                Dong Bok Lee, Seanie Lee, Woo Tae Jeong, Donghwan Kim, Sung Ju Hwang
HIGHLIGHT:             In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs
given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their
consistency.

21, TITLE:             Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction
https://www.aclweb.org/anthology/2020.acl-main.21
AUTHORS:               Zi Chai, Xiaojun Wan
HIGHLIGHT:             To this end, we generate questions in a semi-autoregressive way. Our model divides questions into different
groups and generates each group of them in parallel.

22, TITLE:             Neural Syntactic Preordering for Controlled Paraphrase Generation
https://www.aclweb.org/anthology/2020.acl-main.22
AUTHORS:               Tanya Goyal, Greg Durrett
HIGHLIGHT:             Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly
"reorder" the source sentence and guide our neural paraphrasing model.

23, TITLE:             Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders
https://www.aclweb.org/anthology/2020.acl-main.23
AUTHORS:               Yu Duan, Canwen Xu, Jiaxin Pei, Jialong Han, Chenliang Li
HIGHLIGHT:             In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE)
towards flexible conditional text generation.

24, TITLE:             Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order
https://www.aclweb.org/anthology/2020.acl-main.24
AUTHORS:               Yi Liao, Xin Jiang, Qun Liu
HIGHLIGHT:             In this paper, we propose a probabilistic masking scheme for the masked language model, which we call
probabilistically masked language model (PMLM).

25, TITLE:             Reverse Engineering Configurations of Neural Text Generation Models
https://www.aclweb.org/anthology/2020.acl-main.25
AUTHORS:               Yi Tay, Dara Bahri, Che Zheng, Clifford Brunk, Donald Metzler, Andrew Tomkins
HIGHLIGHT:             In the spirit of better understanding generative text models and their artifacts, we propose the new task of
distinguishing which of several variants of a given model generated some piece of text.

26, TITLE:             Review-based Question Generation with Adaptive Instance Transfer and Augmentation
https://www.aclweb.org/anthology/2020.acl-main.26
AUTHORS:               Qian Yu, Lidong Bing, Qiong Zhang, Wai Lam, Luo Si
HIGHLIGHT:             To obtain proper training instances for the generation model, we propose an iterative learning framework with
adaptive instance transfer and augmentation.

27, TITLE:             TAG : Type Auxiliary Guiding for Code Comment Generation
https://www.aclweb.org/anthology/2020.acl-main.27
AUTHORS:               Ruichu Cai, Zhihao Liang, Boyan Xu, zijian li, Yuexing Hao, Yao Chen
HIGHLIGHT:             In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the
code comment generation task which considers the source code as an N-ary tree with type information associated with each node.

28, TITLE:             Unsupervised Paraphrasing by Simulated Annealing
https://www.aclweb.org/anthology/2020.acl-main.28
AUTHORS:               Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song
HIGHLIGHT:             We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing.

29, TITLE:             A Joint Model for Document Segmentation and Segment Labeling
https://www.aclweb.org/anthology/2020.acl-main.29
AUTHORS:               Joe Barrow, Rajiv Jain, Vlad Morariu, Varun Manjunatha, Douglas Oard, Philip Resnik
HIGHLIGHT:             We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and
labeling segments.

30, TITLE:             Contextualized Weak Supervision for Text Classification
https://www.aclweb.org/anthology/2020.acl-main.30
AUTHORS:               Dheeraj Mekala, Jingbo Shang
HIGHLIGHT:             In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text
classification.

31, TITLE:             Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks
https://www.aclweb.org/anthology/2020.acl-main.31
AUTHORS:               Yufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu, Zhongzhen Wen, Liang Wang
HIGHLIGHT:             Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via
GNN.

32, TITLE:             Neural Topic Modeling with Bidirectional Adversarial Training
https://www.aclweb.org/anthology/2020.acl-main.32
AUTHORS:               Rui Wang, Xuemeng Hu, Deyu Zhou, Yulan He, Yuxuan Xiong, Chenchen Ye, Haiyang Xu
HIGHLIGHT:             To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial
Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling.

33, TITLE:             Text Classification with Negative Supervision
https://www.aclweb.org/anthology/2020.acl-main.33
AUTHORS:               Sora Ohashi, Junya Takayama, Tomoyuki Kajiwara, Chenhui Chu, Yuki Arase
HIGHLIGHT:             To address this problem, we propose a simple multitask learning model that uses negative supervision.

34, TITLE:             Content Word Aware Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.34
AUTHORS:               Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita
HIGHLIGHT:             To address this limitation, we first utilize word frequency information to distinguish between content and
function words in a sentence, and then design a content word-aware NMT to improve translation performance.

35, TITLE:             Evaluating Explanation Methods for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.35
AUTHORS:               Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, Shuming Shi
HIGHLIGHT:             To this end, it proposes a principled metric based on fidelity in regard to the predictive behavior of the NMT
model.

36, TITLE:             Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.36
AUTHORS:               Junliang Guo, Linli Xu, Enhong Chen
HIGHLIGHT:             In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-
autoregressive neural machine translation{\textasciitilde}(NAT).

37, TITLE:             Learning Source Phrase Representations for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.37
AUTHORS:               Hongfei Xu, Josef van Genabith, Deyi Xiong, Qiuhui Liu, Jingyi Zhang
HIGHLIGHT:             In this paper, we first propose an attentive phrase representation generation mechanism which is able to
generate phrase representations from corresponding token representations. In addition, we incorporate the generated phrase
representations into the Transformer translation model to enhance its ability to capture long-distance relationships.

38, TITLE:             Lipschitz Constrained Parameter Initialization for Deep Transformers
https://www.aclweb.org/anthology/2020.acl-main.38
AUTHORS:               Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong, Jingyi Zhang
HIGHLIGHT:             In this paper, we first empirically demonstrate that a simple modification made in the official implementation,
which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep
Transformers.

39, TITLE:             Location Attention for Extrapolation to Longer Sequences
https://www.aclweb.org/anthology/2020.acl-main.39
AUTHORS:               Yann Dubois, Gautier Dagan, Dieuwke Hupkes, Elia Bruni
HIGHLIGHT:             In this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle
it. We then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization to
sequences that are longer than the training ones.

40, TITLE:             Multiscale Collaborative Deep Models for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.40
AUTHORS:               Xiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongxiang Weng, Weihua Luo
HIGHLIGHT:             In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that
are substantially deeper than those used previously.

41, TITLE:             Norm-Based Curriculum Learning for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.41
AUTHORS:               Xuebo Liu, Houtim Lai, Derek F. Wong, Lidia S. Chao
HIGHLIGHT:             In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based
curriculum learning method.

42, TITLE:             Opportunistic Decoding with Timely Correction for Simultaneous Translation
https://www.aclweb.org/anthology/2020.acl-main.42
AUTHORS:               Renjie Zheng, Mingbo Ma, Baigong Zheng, Kaibo Liu, Liang Huang
HIGHLIGHT:             We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a
certain mount of extra words at each step to keep the audience on track with the latest information.

43, TITLE:             A Formal Hierarchy of RNN Architectures
https://www.aclweb.org/anthology/2020.acl-main.43
AUTHORS:               William Merrill, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah A. Smith, Eran Yahav
HIGHLIGHT:             We develop a formal hierarchy of the expressive capacity of RNN architectures.

44, TITLE:             A Three-Parameter Rank-Frequency Relation in Natural Languages
https://www.aclweb.org/anthology/2020.acl-main.44
AUTHORS:               Chenchen Ding, Masao Utiyama, Eiichiro Sumita
HIGHLIGHT:             We present that, the rank-frequency relation in textual data follows $f \propto r^{-\alpha}(r+\gamma)^{-
\beta}$, where $f$ is the token frequency and $r$ is the rank by frequency, with ($\alpha$, $\beta$, $\gamma$) as parameters.

45, TITLE:             Dice Loss for Data-imbalanced NLP Tasks
https://www.aclweb.org/anthology/2020.acl-main.45
AUTHORS:               Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, Jiwei Li
HIGHLIGHT:             In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-
imbalanced NLP tasks.

46, TITLE:             Emergence of Syntax Needs Minimal Supervision
https://www.aclweb.org/anthology/2020.acl-main.46
AUTHORS:               Rapha&euml;l Bailly, Kata G&aacute;bor
HIGHLIGHT:             This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit
syntax-specific guidance.

47, TITLE:             Language Models as an Alternative Evaluator of Word Order Hypotheses: A Case Study in Japanese
https://www.aclweb.org/anthology/2020.acl-main.47
AUTHORS:               Tatsuki Kuribayashi, Takumi Ito, Jun Suzuki, Kentaro Inui
HIGHLIGHT:             In this study, we explore whether the LM-based method is valid for analyzing the word order.

48, TITLE:             GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media
https://www.aclweb.org/anthology/2020.acl-main.48
AUTHORS:               Yi-Ju Lu, Cheng-Te Li
HIGHLIGHT:             Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we
aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious
retweeters and the words they concern.

49, TITLE:             Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection
https://www.aclweb.org/anthology/2020.acl-main.49
AUTHORS:               Lei Zhong, Juan Cao, Qiang Sheng, Junbo Guo, Ziang Wang
HIGHLIGHT:             To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-
GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy
detection.

50, TITLE:             Predicting the Topical Stance and Political Leaning of Media using Tweets
https://www.aclweb.org/anthology/2020.acl-main.50
AUTHORS:                Peter Stefanov, Kareem Darwish, Atanas Atanasov, Preslav Nakov
HIGHLIGHT:              In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter
users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to
characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the
target polarizing topic.

51, TITLE:              Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora
https://www.aclweb.org/anthology/2020.acl-main.51
AUTHORS:                Hila Gonen, Ganesh Jawahar, Djam&eacute; Seddah, Yoav Goldberg
HIGHLIGHT:              We propose an alternative approach that does not use vector space alignment, and instead considers the
neighbors of each word.

52, TITLE:              CDL: Curriculum Dual Learning for Emotion-Controllable Response Generation
https://www.aclweb.org/anthology/2020.acl-main.52
AUTHORS:                Lei Shen, Yang Feng
HIGHLIGHT:              To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which
extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries
alternatively.

53, TITLE:              Efficient Dialogue State Tracking by Selectively Overwriting Memory
https://www.aclweb.org/anthology/2020.acl-main.53
AUTHORS:                Sungdong Kim, Sohee Yang, Gyuwan Kim, Sang-Woo Lee
HIGHLIGHT:              Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting
mechanism for more efficient DST.

54, TITLE:              End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2
https://www.aclweb.org/anthology/2020.acl-main.54
AUTHORS:                Donghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, Kee-Eung Kim
HIGHLIGHT:              In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges
above.

55, TITLE:              Evaluating Dialogue Generation Systems via Response Selection
https://www.aclweb.org/anthology/2020.acl-main.55
AUTHORS:                Shiki Sato, Reina Akama, Hiroki Ouchi, Jun Suzuki, Kentaro Inui
HIGHLIGHT:              Specifically, we propose to construct test sets filtering out some types of false candidates: (i) those unrelated to
the ground-truth response and (ii) those acceptable as appropriate responses.

56, TITLE:              Gated Convolutional Bidirectional Attention-based Model for Off-topic Spoken Response Detection
https://www.aclweb.org/anthology/2020.acl-main.56
AUTHORS:                Yefei Zha, Ruobing Li, Hui Lin
HIGHLIGHT:              In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on
both seen and unseen prompts.

57, TITLE:              Learning Low-Resource End-To-End Goal-Oriented Dialog for Fast and Reliable System Deployment
https://www.aclweb.org/anthology/2020.acl-main.57
AUTHORS:                Yinpei Dai, Hangyu Li, Chengguang Tang, Yongbin Li, Jian Sun, Xiaodan Zhu
HIGHLIGHT:              In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-
learning approaches and human-machine collaboration.

58, TITLE:              Learning to Tag OOV Tokens by Integrating Contextual Representation and Background Knowledge
https://www.aclweb.org/anthology/2020.acl-main.58
AUTHORS:                Keqing He, Yuanmeng Yan, Weiran XU
HIGHLIGHT:              In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation
of input text and the large-scale lexical background knowledge.

59, TITLE:              Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition
https://www.aclweb.org/anthology/2020.acl-main.59
AUTHORS:                Ryuichi Takanobu, Runze Liang, Minlie Huang
HIGHLIGHT:              To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning,
which regards both the system and the user as the dialog agents.

60, TITLE:             Paraphrase Augmented Task-Oriented Dialog Generation
https://www.aclweb.org/anthology/2020.acl-main.60
AUTHORS:               Silin Gao, Yichi Zhang, Zhijian Ou, Zhou Yu
HIGHLIGHT:             We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase
model and a response generation model to improve the dialog generation performance.

61, TITLE:             Response-Anticipated Memory for On-Demand Knowledge Integration in Response Generation
https://www.aclweb.org/anthology/2020.acl-main.61
AUTHORS:               Zhiliang Tian, Wei Bi, Dongkyu Lee, Lanqing Xue, YIPING SONG, Xiaojiang Liu, Nevin L. Zhang
HIGHLIGHT:             In this paper, we propose to create the document memory with some anticipated responses in mind.

62, TITLE:             Semi-Supervised Dialogue Policy Learning via Stochastic Reward Estimation
https://www.aclweb.org/anthology/2020.acl-main.62
AUTHORS:               Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang
HIGHLIGHT:             To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning.

63, TITLE:             Towards Unsupervised Language Understanding and Generation by Joint Dual Learning
https://www.aclweb.org/anthology/2020.acl-main.63
AUTHORS:               Shang-Yu Su, Chao-Wei Huang, Yun-Nung Chen
HIGHLIGHT:             However, the prior work still learned both components in a supervised manner; instead, this paper introduces a
general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised
learning algorithms to train language understanding and generation models in a joint fashion.

64, TITLE:             USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation
https://www.aclweb.org/anthology/2020.acl-main.64
AUTHORS:               Shikib Mehri, Maxine Eskenazi
HIGHLIGHT:             To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog.

65, TITLE:             Explicit Semantic Decomposition for Definition Generation
https://www.aclweb.org/anthology/2020.acl-main.65
AUTHORS:               Jiahuan Li, Yu Bao, Shujian Huang, Xinyu Dai, Jiajun CHEN
HIGHLIGHT:             In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which
explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition
generation.

66, TITLE:             Improved Natural Language Generation via Loss Truncation
https://www.aclweb.org/anthology/2020.acl-main.66
AUTHORS:               Daniel Kang, Tatsunori Hashimoto
HIGHLIGHT:             We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples
as a way to optimize for distinguishability.

67, TITLE:             Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks
https://www.aclweb.org/anthology/2020.acl-main.67
AUTHORS:               Yanbin Zhao, Lu Chen, Zhi Chen, Ruisheng Cao, Su Zhu, Kai Yu
HIGHLIGHT:             In this work, we propose a novel graph encoding framework which can effectively explore the edge relations.

68, TITLE:             Rigid Formats Controlled Text Generation
https://www.aclweb.org/anthology/2020.acl-main.68
AUTHORS:               Piji Li, Haisong Zhang, Xiaojiang Liu, Shuming Shi
HIGHLIGHT:             Therefore, we propose a simple and elegant framework named SongNet to tackle this problem.

69, TITLE:             Syn-QG: Syntactic and Shallow Semantic Rules for Question Generation
https://www.aclweb.org/anthology/2020.acl-main.69
AUTHORS:               Kaustubh Dhole, Christopher D. Manning
HIGHLIGHT:             We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal
dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-
answer pairs.

70, TITLE:             An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering
https://www.aclweb.org/anthology/2020.acl-main.70
AUTHORS:               Jay Kumar, Junming Shao, Salah Uddin, Wazir Ali
HIGHLIGHT:             Therefore, in this paper, we propose an Online Semantic-enhanced Dirichlet Model for short sext stream
clustering, called OSDM, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and
clusters each arriving short text automatically in an online way.

71, TITLE:             Generative Semantic Hashing Enhanced via Boltzmann Machines
https://www.aclweb.org/anthology/2020.acl-main.71
AUTHORS:               Lin Zheng, Qinliang Su, Dinghan Shen, Changyou Chen
HIGHLIGHT:             In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of
Boltzmann machine as the variational posterior.

72, TITLE:             Interactive Construction of User-Centric Dictionary for Text Analytics
https://www.aclweb.org/anthology/2020.acl-main.72
AUTHORS:               Ryosuke Kohita, Issei Yoshida, Hiroshi Kanayama, Tetsuya Nasukawa
HIGHLIGHT:             To optimize the interaction, we propose a new algorithm that effectively captures an analyst's intention starting
from only a small number of sample terms.

73, TITLE:             Tree-Structured Neural Topic Model
https://www.aclweb.org/anthology/2020.acl-main.73
AUTHORS:               Masaru Isonuma, Junichiro Mori, Danushka Bollegala, Ichiro Sakata
HIGHLIGHT:             This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an
infinite number of branches.

74, TITLE:             Unsupervised FAQ Retrieval with Question Generation and BERT
https://www.aclweb.org/anthology/2020.acl-main.74
AUTHORS:               Yosi Mass, Boaz Carmeli, Haggai Roitman, David Konopnicki
HIGHLIGHT:             We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models.

75, TITLE:             ``The Boating Store Had Its Best Sail Ever'': Pronunciation-attentive Contextualized Pun Recognition
https://www.aclweb.org/anthology/2020.acl-main.75
AUTHORS:               Yichao Zhou, Jyun-Yu Jiang, Jieyu Zhao, Kai-Wei Chang, Wei Wang
HIGHLIGHT:             In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human
humor, detect if a sentence contains puns and locate them in the sentence.

76, TITLE:             Fast and Accurate Deep Bidirectional Language Representations for Unsupervised Learning
https://www.aclweb.org/anthology/2020.acl-main.76
AUTHORS:               Joongbo Shin, Yoonhyung Lee, Seunghyun Yoon, Kyomin Jung
HIGHLIGHT:             To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based
Text Autoencoder (T-TA).

77, TITLE:             Fine-grained Interest Matching for Neural News Recommendation
https://www.aclweb.org/anthology/2020.acl-main.77
AUTHORS:               Heyuan Wang, Fangzhao Wu, Zheng Liu, Xing Xie
HIGHLIGHT:             In this paper, we propose FIM, a Fine-grained Interest Matching method for neural news recommendation.

78, TITLE:             Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder
https://www.aclweb.org/anthology/2020.acl-main.78
AUTHORS:               Fan Zhou, Shengming Zhang, Yi Yang
HIGHLIGHT:             To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-
head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC).
79, TITLE:             Interpreting Twitter User Geolocation
https://www.aclweb.org/anthology/2020.acl-main.79
AUTHORS:               Ting Zhong, Tianliang Wang, Fan Zhou, Goce Trajcevski, Kunpeng Zhang, Yi Yang
HIGHLIGHT:             In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the
importance of training users when predicting the locations of the testing users.

80, TITLE:             Modeling Code-Switch Languages Using Bilingual Parallel Corpus
https://www.aclweb.org/anthology/2020.acl-main.80
AUTHORS:               Grandee Lee, Haizhou Li
HIGHLIGHT:             We propose a bilingual attention language model (BALM) that simultaneously performs language modeling
objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency.

81, TITLE:             SpellGCN: Incorporating Phonological and Visual Similarities into Language Models for Chinese Spelling
Check
https://www.aclweb.org/anthology/2020.acl-main.81
AUTHORS:               Xingyi Cheng, Weidi Xu, Kunlong Chen, Shaohua Jiang, Feng Wang, Taifeng Wang, Wei Chu, Yuan Qi
HIGHLIGHT:             This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC
via a specialized graph convolutional network (SpellGCN).

82, TITLE:             Spelling Error Correction with Soft-Masked BERT
https://www.aclweb.org/anthology/2020.acl-main.82
AUTHORS:               Shaohua Zhang, Haoran Huang, Jicong Liu, Hang Li
HIGHLIGHT:             In this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a
network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what
we call soft-masking technique.

83, TITLE:             A Frame-based Sentence Representation for Machine Reading Comprehension
https://www.aclweb.org/anthology/2020.acl-main.83
AUTHORS:               Shaoru Guo, Ru Li, Hongye Tan, Xiaoli Li, Yong Guan, Hongyan Zhao, Yueping Zhang
HIGHLIGHT:             To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs
frame semantic knowledge to facilitate sentence modelling.

84, TITLE:             A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation
https://www.aclweb.org/anthology/2020.acl-main.84
AUTHORS:               Jan Deriu, Katsiaryna Mlynchyk, Philippe Schl&auml;pfer, Alvaro Rodrigo, Dirk von Gr&uuml;nigen, Nicolas
Kaiser, Kurt Stockinger, Eneko Agirre, Mark Cieliebak
HIGHLIGHT:             In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over
structured data.

85, TITLE:             Contextualized Sparse Representations for Real-Time Open-Domain Question Answering
https://www.aclweb.org/anthology/2020.acl-main.85
AUTHORS:               Jinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi, Jaewoo Kang
HIGHLIGHT:             In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized
sparse representation (Sparc).

86, TITLE:             Dynamic Sampling Strategies for Multi-Task Reading Comprehension
https://www.aclweb.org/anthology/2020.acl-main.86
AUTHORS:               Ananth Gottumukkala, Dheeru Dua, Sameer Singh, Matt Gardner
HIGHLIGHT:             We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task
model’s current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task
sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning.

87, TITLE:             Enhancing Answer Boundary Detection for Multilingual Machine Reading Comprehension
https://www.aclweb.org/anthology/2020.acl-main.87
AUTHORS:               Fei Yuan, Linjun Shou, Xuanyu Bai, Ming Gong, Yaobo Liang, Nan Duan, Yan Fu, Daxin Jiang
HIGHLIGHT:             In this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary
supervision: (1) A mixed MRC task, which translates the question or passage to other languages and builds cross-lingual question-
passage pairs; (2) A language-agnostic knowledge masking task by leveraging knowledge phrases mined from web.

88, TITLE:             Explicit Memory Tracker with Coarse-to-Fine Reasoning for Conversational Machine Reading
https://www.aclweb.org/anthology/2020.acl-main.88
AUTHORS:               Yifan Gao, Chien-Sheng Wu, Shafiq Joty, Caiming Xiong, Richard Socher, Irwin King, Michael Lyu, Steven
C.H. Hoi
HIGHLIGHT:             In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit
Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision.

89, TITLE:             Injecting Numerical Reasoning Skills into Language Models
https://www.aclweb.org/anthology/2020.acl-main.89
AUTHORS:                 Mor Geva, Ankit Gupta, Jonathan Berant
HIGHLIGHT:               In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can
inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.

90, TITLE:               Learning to Identify Follow-Up Questions in Conversational Question Answering
https://www.aclweb.org/anthology/2020.acl-main.90
AUTHORS:                 Souvik Kundu, Qian Lin, Hwee Tou Ng
HIGHLIGHT:               In this paper, we introduce a new follow-up question identification task.

91, TITLE:               Query Graph Generation for Answering Multi-hop Complex Questions from Knowledge Bases
https://www.aclweb.org/anthology/2020.acl-main.91
AUTHORS:                 Yunshi Lan, Jing Jiang
HIGHLIGHT:               In this paper, we handle both types of complexity at the same time.

92, TITLE:               A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers
https://www.aclweb.org/anthology/2020.acl-main.92
AUTHORS:                 Shen-yun Miao, Chao-Chun Liang, Keh-Yih Su
HIGHLIGHT:               We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and
problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers.

93, TITLE:               Improving Image Captioning Evaluation by Considering Inter References Variance
https://www.aclweb.org/anthology/2020.acl-main.93
AUTHORS:                 Yanzhi Yi, Hangyu Deng, Jinglu Hu
HIGHLIGHT:               In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend
BERTScore with a few new features appropriately for image captioning evaluation.

94, TITLE:               Revisiting the Context Window for Cross-lingual Word Embeddings
https://www.aclweb.org/anthology/2020.acl-main.94
AUTHORS:                 Ryokan Ri, Yoshimasa Tsuruoka
HIGHLIGHT:               In this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual
embeddings trained with different context windows.

95, TITLE:               Moving Down the Long Tail of Word Sense Disambiguation with Gloss Informed Bi-encoders
https://www.aclweb.org/anthology/2020.acl-main.95
AUTHORS:                 Terra Blevins, Luke Zettlemoyer
HIGHLIGHT:               We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and
(2) the dictionary definition, or gloss, of each sense.

96, TITLE:               Code-Switching Patterns Can Be an Effective Route to Improve Performance of Downstream NLP
Applications: A Case Study of Humour, Sarcasm and Hate Speech Detection
https://www.aclweb.org/anthology/2020.acl-main.96
AUTHORS:                 Srijan Bansal, Vishal Garimella, Ayush Suhane, Jasabanta Patro, Animesh Mukherjee
HIGHLIGHT:               In this paper, we demonstrate how code-switching patterns can be utilised to improve various downstream NLP
applications.

97, TITLE:               DTCA: Decision Tree-based Co-Attention Networks for Explainable Claim Verification
https://www.aclweb.org/anthology/2020.acl-main.97
AUTHORS:                 Lianwei Wu, Yuan Rao, yongqiang zhao, Hao Liang, Ambreen Nazir
HIGHLIGHT:               In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for
explainable claim verification.

98, TITLE:               Towards Conversational Recommendation over Multi-Type Dialogs
https://www.aclweb.org/anthology/2020.acl-main.98
AUTHORS:                 Zeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu
HIGHLIGHT:               We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots
can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking
into account user's interests and feedback.

99, TITLE:               Unknown Intent Detection Using Gaussian Mixture Model with an Application to Zero-shot Intent
Classification
https://www.aclweb.org/anthology/2020.acl-main.99
AUTHORS:               Guangfeng Yan, Lu Fan, Qimai Li, Han Liu, Xiaotong Zhang, Xiao-Ming Wu, Albert Y.S. Lam
HIGHLIGHT:             This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection.

100, TITLE:            Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen
https://www.aclweb.org/anthology/2020.acl-main.100
AUTHORS:               Yixin Cao, Ruihao Shui, Liangming Pan, Min-Yen Kan, Zhiyuan Liu, Tat-Seng Chua
HIGHLIGHT:             We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of
alleviating such cognitive biases.

101, TITLE:            Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints
https://www.aclweb.org/anthology/2020.acl-main.101
AUTHORS:               Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, Changyou Chen
HIGHLIGHT:             In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the
goal.

102, TITLE:            Dynamic Memory Induction Networks for Few-Shot Text Classification
https://www.aclweb.org/anthology/2020.acl-main.102
AUTHORS:               Ruiying Geng, Binhua Li, Yongbin Li, Jian Sun, Xiaodan Zhu
HIGHLIGHT:             This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification.

103, TITLE:            Exclusive Hierarchical Decoding for Deep Keyphrase Generation
https://www.aclweb.org/anthology/2020.acl-main.103
AUTHORS:               Wang Chen, Hou Pong Chan, Piji Li, Irwin King
HIGHLIGHT:             To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a
hierarchical decoding process and either a soft or a hard exclusion mechanism.

104, TITLE:            Hierarchy-Aware Global Model for Hierarchical Text Classification
https://www.aclweb.org/anthology/2020.acl-main.104
AUTHORS:               Jie Zhou, Chunping Ma, Dingkun Long, Guangwei Xu, Ning Ding, Haoyu Zhang, Pengjun Xie, Gongshen Liu
HIGHLIGHT:             In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders
for modeling label dependencies.

105, TITLE:            Keyphrase Generation for Scientific Document Retrieval
https://www.aclweb.org/anthology/2020.acl-main.105
AUTHORS:               Florian Boudin, Ygor Gallina, Akiko Aizawa
HIGHLIGHT:             This study provides empirical evidence that such models can significantly improve retrieval performance, and
introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation
models.

106, TITLE:            A Graph Auto-encoder Model of Derivational Morphology
https://www.aclweb.org/anthology/2020.acl-main.106
AUTHORS:               Valentin Hofmann, Hinrich Sch&uuml;tze, Janet Pierrehumbert
HIGHLIGHT:             We present a graph auto-encoder that learns embeddings capturing information about the compatibility of
affixes and stems in derivation.

107, TITLE:            Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell
https://www.aclweb.org/anthology/2020.acl-main.107
AUTHORS:               Djam&eacute; Seddah, Farah Essaidi, Amal Fethi, Matthieu Futeral, Benjamin Muller, Pedro Javier Ortiz
Su&aacute;rez, Beno&icirc;t Sagot, Abhishek Srivastava
HIGHLIGHT:             We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African
Arabic dialect known for its frequent usage of code-switching.

108, TITLE:            Crawling and Preprocessing Mailing Lists At Scale for Dialog Analysis
https://www.aclweb.org/anthology/2020.acl-main.108
AUTHORS:               Janek Bevendorff, Khalid Al Khatib, Martin Potthast, Benno Stein
HIGHLIGHT:             This paper introduces the Webis Gmane Email Corpus 2019, the largest publicly available and fully
preprocessed email corpus to date.

109, TITLE:            Fine-Grained Analysis of Cross-Linguistic Syntactic Divergences
https://www.aclweb.org/anthology/2020.acl-main.109
AUTHORS:                 Dmitry Nikolaev, Ofir Arviv, Taelin Karidi, Neta Kenneth, Veronika Mitnik, Lilja Maria Saeboe, Omri Abend
HIGHLIGHT:               We propose a framework for extracting divergence patterns for any language pair from a parallel corpus,
building on Universal Dependencies.

110, TITLE:              Generating Counter Narratives against Online Hate Speech: Data and Strategies
https://www.aclweb.org/anthology/2020.acl-main.110
AUTHORS:                 Serra Sinem TekiroÄŸlu, Yi-Ling Chung, Marco Guerini
HIGHLIGHT:               Being aware of the aforementioned limitations, we present a study on how to collect responses to hate
effectively, employing large scale unsupervised language models such as GPT-2 for the generation of silver data, and the best
annotation strategies/neural architectures that can be used for data filtering before expert validation/post-editing.

111, TITLE:              KLEJ: Comprehensive Benchmark for Polish Language Understanding
https://www.aclweb.org/anthology/2020.acl-main.111
AUTHORS:                 Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik
HIGHLIGHT:               To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language
understanding, accompanied by an online leaderboard.

112, TITLE:              Learning and Evaluating Emotion Lexicons for 91 Languages
https://www.aclweb.org/anthology/2020.acl-main.112
AUTHORS:                 Sven Buechel, Susanna R&uuml;cker, Udo Hahn
HIGHLIGHT:               In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion
lexicons for any target language.

113, TITLE:              Multi-Hypothesis Machine Translation Evaluation
https://www.aclweb.org/anthology/2020.acl-main.113
AUTHORS:                 Marina Fomicheva, Lucia Specia, Francisco Guzm&aacute;n
HIGHLIGHT:               In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference
we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations;
(ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether.

114, TITLE:              Multimodal Quality Estimation for Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.114
AUTHORS:                 Shu Okabe, Fr&eacute;d&eacute;ric Blain, Lucia Specia
HIGHLIGHT:               We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual
modalities for Multimodal QE.

115, TITLE:              PuzzLing Machines: A Challenge on Learning From Small Data
https://www.aclweb.org/anthology/2020.acl-main.115
AUTHORS:                 G&ouml;zde G&uuml;l Åžahin, Yova Kementchedjhieva, Phillip Rust, Iryna Gurevych
HIGHLIGHT:               To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing
Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students.

116, TITLE:              The SOFC-Exp Corpus and Neural Approaches to Information Extraction in the Materials Science Domain
https://www.aclweb.org/anthology/2020.acl-main.116
AUTHORS:                 Annemarie Friedrich, Heike Adel, Federico Tomazic, Johannes Hingerl, Renou Benteau, Anika Marusczyk,
Lukas Lange
HIGHLIGHT:               This paper presents a new challenging information extraction task in the domain of materials science.

117, TITLE:              The TechQA Dataset
https://www.aclweb.org/anthology/2020.acl-main.117
AUTHORS:                 Vittorio Castelli, Rishav Chakravarti, Saswati Dana, Anthony Ferritto, Radu Florian, Martin Franz, Dinesh
Garg, Dinesh Khandelwal, Scott McCarley, Michael McCawley, Mohamed Nasr, Lin Pan, Cezar Pendus, John Pitrelli, Saurabh Pujar,
Salim Roukos, Andrzej Sakrajda, Avi Sil, Rosario Uceda-Sosa, Todd Ward, Rong Zhang
HIGHLIGHT:               We introduce TECHQA, a domain-adaptation question answering dataset for the technical support domain.

118, TITLE:              iSarcasm: A Dataset of Intended Sarcasm
https://www.aclweb.org/anthology/2020.acl-main.118
AUTHORS:                 Silviu Oprea, Walid Magdy
HIGHLIGHT:               We show the limitations of previous labelling methods in capturing intended sarcasm and introduce the
iSarcasm dataset of tweets labeled for sarcasm directly by their authors.

119, TITLE:             AMR Parsing via Graph-Sequence Iterative Inference
https://www.aclweb.org/anthology/2020.acl-main.119
AUTHORS:                Deng Cai, Wai Lam
HIGHLIGHT:              We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence
and the incrementally constructed graph.

120, TITLE:             A Large-Scale Multi-Document Summarization Dataset from the Wikipedia Current Events Portal
https://www.aclweb.org/anthology/2020.acl-main.120
AUTHORS:                Demian Gholipour Ghalandari, Chris Hokamp, Nghia The Pham, John Glover, Georgiana Ifrim
HIGHLIGHT:              This work presents a new dataset for MDS that is large both in the total number of document clusters and in the
size of individual clusters.

121, TITLE:             Attend, Translate and Summarize: An Efficient Method for Neural Cross-Lingual Summarization
https://www.aclweb.org/anthology/2020.acl-main.121
AUTHORS:                Junnan Zhu, Yu Zhou, Jiajun Zhang, Chengqing Zong
HIGHLIGHT:              In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-
lingual summary.

122, TITLE:             Examining the State-of-the-Art in News Timeline Summarization
https://www.aclweb.org/anthology/2020.acl-main.122
AUTHORS:                Demian Gholipour Ghalandari, Georgiana Ifrim
HIGHLIGHT:              In this paper, we compare different TLS strategies using appropriate evaluation frameworks, and propose a
simple and effective combination of methods that improves over the stateof-the-art on all tested benchmarks.

123, TITLE:             Improving Truthfulness of Headline Generation
https://www.aclweb.org/anthology/2020.acl-main.123
AUTHORS:                Kazuki Matsumaru, Sho Takase, Naoaki Okazaki
HIGHLIGHT:              This paper explores improving the truthfulness in headline generation on two popular datasets.

124, TITLE:             SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization
https://www.aclweb.org/anthology/2020.acl-main.124
AUTHORS:                Yang Gao, Wei Zhao, Steffen Eger
HIGHLIGHT:              We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo
reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token
alignment techniques.

125, TITLE:             Self-Attention Guided Copy Mechanism for Abstractive Summarization
https://www.aclweb.org/anthology/2020.acl-main.125
AUTHORS:                Song Xu, Haoran Li, Peng Yuan, Youzheng Wu, Xiaodong He, Bowen Zhou
HIGHLIGHT:              In this work, we propose a Transformer-based model to enhance the copy mechanism.

126, TITLE:             Beyond User Self-Reported Likert Scale Ratings: A Comparison Model for Automatic Dialog Evaluation
https://www.aclweb.org/anthology/2020.acl-main.126
AUTHORS:                Weixin Liang, James Zou, Zhou Yu
HIGHLIGHT:              To alleviate this problem, we formulate dialog evaluation as a comparison task.

127, TITLE:             Conversational Word Embedding for Retrieval-Based Dialog System
https://www.aclweb.org/anthology/2020.acl-main.127
AUTHORS:                Wentao Ma, Yiming Cui, Ting Liu, Dong Wang, Shijin Wang, Guoping Hu
HIGHLIGHT:              In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the
conversation pairs {\textless}post, reply{\textgreater} to learn word embedding.

128, TITLE:             Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection
Network
https://www.aclweb.org/anthology/2020.acl-main.128
AUTHORS:                Yutai Hou, Wanxiang Che, Yongkui Lai, Zhihan Zhou, Yijia Liu, Han Liu, Ting Liu
HIGHLIGHT:              In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot).

129, TITLE:             Learning Dialog Policies from Weak Demonstrations
https://www.aclweb.org/anthology/2020.acl-main.129
AUTHORS:                Gabriel Gordon-Hall, Philip John Gorinski, Shay B. Cohen
HIGHLIGHT:              We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap
between the datasets and the environment.

130, TITLE:             MuTual: A Dataset for Multi-Turn Dialogue Reasoning
https://www.aclweb.org/anthology/2020.acl-main.130
AUTHORS:                Leyang Cui, Yu Wu, Shujie Liu, Yue Zhang, Ming Zhou
HIGHLIGHT:              To facilitate the conversation reasoning research, we introduce MuTual, a novel dataset for Multi-Turn dialogue
Reasoning, consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams.

131, TITLE:             You Impress Me: Dialogue Generation via Mutual Persona Perception
https://www.aclweb.org/anthology/2020.acl-main.131
AUTHORS:                Qian Liu, Yihong Chen, Bei Chen, Jian-Guang LOU, Zixuan Chen, Bin Zhou, Dongmei Zhang
HIGHLIGHT:              Motivated by this, we propose P{\^{}}2 Bot, a transmitter-receiver based framework with the aim of explicitly
modeling understanding.

132, TITLE:             Bridging Anaphora Resolution as Question Answering
https://www.aclweb.org/anthology/2020.acl-main.132
AUTHORS:                Yufang Hou
HIGHLIGHT:              In this paper, we cast bridging anaphora resolution as question answering based on context.

133, TITLE:             Dialogue Coherence Assessment Without Explicit Dialogue Act Labels
https://www.aclweb.org/anthology/2020.acl-main.133
AUTHORS:                Mohsen Mesgar, Sebastian B&uuml;cker, Iryna Gurevych
HIGHLIGHT:              We address these issues by introducing a novel approach to dialogue coherence assessment.

134, TITLE:             Fast and Accurate Non-Projective Dependency Tree Linearization
https://www.aclweb.org/anthology/2020.acl-main.134
AUTHORS:                Xiang Yu, Simon Tannert, Ngoc Thang Vu, Jonas Kuhn
HIGHLIGHT:              We propose a graph-based method to tackle the dependency tree linearization task.

135, TITLE:             Semantic Graphs for Generating Deep Questions
https://www.aclweb.org/anthology/2020.acl-main.135
AUTHORS:                Liangming Pan, Yuxi Xie, Yansong Feng, Tat-Seng Chua, Min-Yen Kan
HIGHLIGHT:              This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex
questions that require reasoning over multiple pieces of information about the input passage.

136, TITLE:             A Novel Cascade Binary Tagging Framework for Relational Triple Extraction
https://www.aclweb.org/anthology/2020.acl-main.136
AUTHORS:                Zhepei Wei, Jianlin Su, Yue Wang, Yuan Tian, Yi Chang
HIGHLIGHT:              In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel
cascade binary tagging framework (CasRel) derived from a principled problem formulation.

137, TITLE:             In Layman's Terms: Semi-Open Relation Extraction from Scientific Texts
https://www.aclweb.org/anthology/2020.acl-main.137
AUTHORS:                Ruben Kruiper, Julian Vincent, Jessica Chen-Burger, Marc Desmulliez, Ioannis Konstas
HIGHLIGHT:              In this work we combine the output of both types of systems to achieve Semi-Open Relation Extraction, a new
task that we explore in the Biology domain.

138, TITLE:             NAT: Noise-Aware Training for Robust Neural Sequence Labeling
https://www.aclweb.org/anthology/2020.acl-main.138
AUTHORS:                Marcin Namysl, Sven Behnke, Joachim K&ouml;hler
HIGHLIGHT:              To this end, we formulate the noisy sequence labeling problem, where the input may undergo an unknown
noising process and propose two Noise-Aware Training (NAT) objectives that improve robustness of sequence labeling performed on
perturbed input: Our data augmentation method trains a neural model using a mixture of clean and noisy samples, whereas our
stability training algorithm encourages the model to create a noise-invariant latent representation.

139, TITLE:             Named Entity Recognition without Labelled Data: A Weak Supervision Approach
https://www.aclweb.org/anthology/2020.acl-main.139
AUTHORS:               Pierre Lison, Jeremy Barnes, Aliaksandr Hubin, Samia Touileb
HIGHLIGHT:             This paper presents a simple but powerful approach to learn NER models in the absence of labelled data
through weak supervision.

140, TITLE:            Probing Linguistic Features of Sentence-Level Representations in Relation Extraction
https://www.aclweb.org/anthology/2020.acl-main.140
AUTHORS:               Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig
HIGHLIGHT:             We introduce 14 probing tasks targeting linguistic properties relevant to RE, and we use them to study
representations learned by more than 40 different encoder architecture and linguistic feature combinations trained on two datasets,
TACRED and SemEval 2010 Task 8.

141, TITLE:            Reasoning with Latent Structure Refinement for Document-Level Relation Extraction
https://www.aclweb.org/anthology/2020.acl-main.141
AUTHORS:               Guoshun Nan, Zhijiang Guo, Ivan Sekulic, Wei Lu
HIGHLIGHT:             Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a
novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph.

142, TITLE:            TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task
https://www.aclweb.org/anthology/2020.acl-main.142
AUTHORS:               Christoph Alt, Aleksandra Gabryszak, Leonhard Hennig
HIGHLIGHT:             In this paper, we investigate the questions: Have we reached a performance ceiling or is there still room for
improvement?

143, TITLE:            Bilingual Dictionary Based Neural Machine Translation without Using Parallel Sentences
https://www.aclweb.org/anthology/2020.acl-main.143
AUTHORS:               Xiangyu Duan, Baijun Ji, Hao Jia, Min Tan, Min Zhang, Boxing Chen, Weihua Luo, Yue Zhang
HIGHLIGHT:             In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but
can refer to a ground-truth bilingual dictionary.

144, TITLE:            Boosting Neural Machine Translation with Similar Translations
https://www.aclweb.org/anthology/2020.acl-main.144
AUTHORS:               Jitao XU, Josep Crego, Jean Senellart
HIGHLIGHT:             This paper explores data augmentation methods for training Neural Machine Translation to make use of similar
translations, in a comparable way a human translator employs fuzzy matches.

145, TITLE:            Character-Level Translation with Self-attention
https://www.aclweb.org/anthology/2020.acl-main.145
AUTHORS:               Yingqiang Gao, Nikola I. Nikolov, Yuhuang Hu, Richard H.R. Hahnloser
HIGHLIGHT:             We explore the suitability of self-attention models for character-level neural machine translation.

146, TITLE:            End-to-End Neural Word Alignment Outperforms GIZA++
https://www.aclweb.org/anthology/2020.acl-main.146
AUTHORS:               Thomas Zenkel, Joern Wuebker, John DeNero
HIGHLIGHT:             We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three
data sets.

147, TITLE:            Enhancing Machine Translation with Dependency-Aware Self-Attention
https://www.aclweb.org/anthology/2020.acl-main.147
AUTHORS:               Emanuele Bugliarello, Naoaki Okazaki
HIGHLIGHT:             In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model
and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially
for long sentences and in low-resource scenarios.

148, TITLE:            Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation
https://www.aclweb.org/anthology/2020.acl-main.148
AUTHORS:               Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich
HIGHLIGHT:             We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying
typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures.

149, TITLE:            It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual
Information
https://www.aclweb.org/anthology/2020.acl-main.149
AUTHORS:               Emanuele Bugliarello, Sabrina J. Mielke, Antonios Anastasopoulos, Ryan Cotterell, Naoaki Okazaki
HIGHLIGHT:             In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of
machine translation difficulty that exploits the probabilistic nature of most neural machine translation models.

150, TITLE:            Language-aware Interlingua for Multilingual Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.150
AUTHORS:               Changfeng Zhu, Heng Yu, Shanbo Cheng, Weihua Luo
HIGHLIGHT:             In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture.

151, TITLE:            On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation
https://www.aclweb.org/anthology/2020.acl-main.151
AUTHORS:               Wei Zhao, Goran GlavaÅ¡, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger
HIGHLIGHT:             In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly
compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual
encoders.

152, TITLE:            Parallel Sentence Mining by Constrained Decoding
https://www.aclweb.org/anthology/2020.acl-main.152
AUTHORS:               Pinzhen Chen, Nikolay Bogoychev, Kenneth Heafield, Faheem Kirefu
HIGHLIGHT:             We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine
translation.

153, TITLE:            Self-Attention with Cross-Lingual Position Representation
https://www.aclweb.org/anthology/2020.acl-main.153
AUTHORS:               Liang Ding, Longyue Wang, Dacheng Tao
HIGHLIGHT:             In this paper, we augment SANs with \textit{cross-lingual position representations} to model the bilingually
aware latent structure for the input sentence.

154, TITLE:            ``You Sound Just Like Your Father'' Commercial Machine Translation Systems Include Stylistic Biases
https://www.aclweb.org/anthology/2020.acl-main.154
AUTHORS:               Dirk Hovy, Federico Bianchi, Tommaso Fornaciari
HIGHLIGHT:             We show that as a consequence, the output of three commercial machine translation systems (Bing, DeepL,
Google) make demographically diverse samples from five languages "sound" older and more male than the original.

155, TITLE:            MMPE: A Multi-Modal Interface for Post-Editing Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.155
AUTHORS:               Nico Herbig, Tim D&uuml;wel, Santanu Pal, Kalliopi Meladaki, Mahsa Monshizadeh, Antonio Kr&uuml;ger,
Josef van Genabith
HIGHLIGHT:             Since this paradigm shift offers potential for modalities other than mouse and keyboard, we present MMPE, the
first prototype to combine traditional input modes with pen, touch, and speech modalities for PE of MT.

156, TITLE:            A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages
https://www.aclweb.org/anthology/2020.acl-main.156
AUTHORS:               Pedro Javier Ortiz Su&aacute;rez, Laurent Romary, Beno&icirc;t Sagot
HIGHLIGHT:             We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering
and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages.

157, TITLE:            Will-They-Won't-They: A Very Large Dataset for Stance Detection on Twitter
https://www.aclweb.org/anthology/2020.acl-main.157
AUTHORS:               Costanza Conforti, Jakob Berndt, Mohammad Taher Pilehvar, Chryssi Giannitsarou, Flavio Toxvaerd, Nigel
Collier
HIGHLIGHT:             We present a new challenging stance detection dataset, called Will-They-Won't-They (WT--WT), which
contains 51,284 tweets in English, making it by far the largest available dataset of the type.

158, TITLE:            A Systematic Assessment of Syntactic Generalization in Neural Language Models
https://www.aclweb.org/anthology/2020.acl-main.158
AUTHORS:               Jennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox, Roger Levy
HIGHLIGHT:            We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20
combinations of model types and data sizes on a set of 34 English-language syntactic test suites.

159, TITLE:           Inflecting When There's No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models
for German Plurals
https://www.aclweb.org/anthology/2020.acl-main.159
AUTHORS:              Kate McCurdy, Sharon Goldwater, Adam Lopez
HIGHLIGHT:            We conclude that modern neural models may still struggle with minority-class generalization.

160, TITLE:           Overestimation of Syntactic Representation in Neural Language Models
https://www.aclweb.org/anthology/2020.acl-main.160
AUTHORS:              Jordan Kodner, Nitish Gupta
HIGHLIGHT:            We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with
two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.

161, TITLE:           Suspense in Short Stories is Predicted By Uncertainty Reduction over Neural Story Representation
https://www.aclweb.org/anthology/2020.acl-main.161
AUTHORS:              David Wilmot, Frank Keller
HIGHLIGHT:            We propose a hierarchical language model that encodes stories and computes surprise and uncertainty
reduction.

162, TITLE:           You Don't Have Time to Read This: An Exploration of Document Reading Time Prediction
https://www.aclweb.org/anthology/2020.acl-main.162
AUTHORS:              Orion Weller, Jordan Hildebrandt, Ilya Reznik, Christopher Challis, E. Shannon Tass, Quinn Snell, Kevin Seppi
HIGHLIGHT:            We seek to extend these works by examining whether or not document level predictions are effective, given
additional information such as subject matter, font characteristics, and readability metrics.

163, TITLE:           A Generative Model for Joint Natural Language Understanding and Generation
https://www.aclweb.org/anthology/2020.acl-main.163
AUTHORS:              Bo-Hsiang Tseng, Jianpeng Cheng, Yimai Fang, David Vandyke
HIGHLIGHT:            In this work, we propose a generative model which couples NLU and NLG through a shared latent variable.

164, TITLE:           Automatic Detection of Generated Text is Easiest when Humans are Fooled
https://www.aclweb.org/anthology/2020.acl-main.164
AUTHORS:              Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck
HIGHLIGHT:            Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies-top-
{\_}k{\_}, nucleus sampling, and untruncated random sampling-and show that improvements in decoding methods have primarily
optimized for fooling humans.

165, TITLE:           Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing
https://www.aclweb.org/anthology/2020.acl-main.165
AUTHORS:              Haoming Jiang, Chen Liang, Chong Wang, Tuo Zhao
HIGHLIGHT:            To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each
domain, on which we apply word-level, adaptive and layer-wise domain mixing.

166, TITLE:           Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation
https://www.aclweb.org/anthology/2020.acl-main.166
AUTHORS:              Jun Xu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu
HIGHLIGHT:            To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent
prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and
controllable dialog.

167, TITLE:           GPT-too: A Language-Model-First Approach for AMR-to-Text Generation
https://www.aclweb.org/anthology/2020.acl-main.167
AUTHORS:              Manuel Mager, Ram&oacute;n Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu
Florian, Salim Roukos
HIGHLIGHT:            In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle
consistency-based re-scoring.

168, TITLE:           Learning to Update Natural Language Comments Based on Code Changes
https://www.aclweb.org/anthology/2020.acl-main.168
AUTHORS:                Sheena Panthaplackel, Pengyu Nie, Milos Gligoric, Junyi Jessy Li, Raymond Mooney
HIGHLIGHT:              We propose an approach that learns to correlate changes across two distinct language representations, to
generate a sequence of edits that are applied to the existing comment to reflect the source code modifications.

169, TITLE:             Politeness Transfer: A Tag and Generate Approach
https://www.aclweb.org/anthology/2020.acl-main.169
AUTHORS:                Aman Madaan, Amrith Setlur, Tanmay Parekh, Barnabas Poczos, Graham Neubig, Yiming Yang, Ruslan
Salakhutdinov, Alan W Black, Shrimai Prabhumoye
HIGHLIGHT:              This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite
sentences while preserving the meaning.

170, TITLE:             BPE-Dropout: Simple and Effective Subword Regularization
https://www.aclweb.org/anthology/2020.acl-main.170
AUTHORS:                Ivan Provilkov, Dmitrii Emelianenko, Elena Voita
HIGHLIGHT:              We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with
conventional BPE.

171, TITLE:             Improving Non-autoregressive Neural Machine Translation with Monolingual Data
https://www.aclweb.org/anthology/2020.acl-main.171
AUTHORS:                Jiawei Zhou, Phillip Keung
HIGHLIGHT:              Under this framework, we leverage large monolingual corpora to improve the NAR model's performance, with
the goal of transferring the AR model's generalization ability while preventing overfitting.

172, TITLE:             Attend to Medical Ontologies: Content Selection for Clinical Abstractive Summarization
https://www.aclweb.org/anthology/2020.acl-main.172
AUTHORS:                Sajad Sotudeh Gharebagh, Nazli Goharian, Ross Filice
HIGHLIGHT:              In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting
salient ontological terms into the summarizer.

173, TITLE:             On Faithfulness and Factuality in Abstractive Summarization
https://www.aclweb.org/anthology/2020.acl-main.173
AUTHORS:                Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan McDonald
HIGHLIGHT:              In this paper we have analyzed limitations of these models for abstractive document summarization and found
that these models are highly prone to hallucinate content that is unfaithful to the input document.

174, TITLE:             Screenplay Summarization Using Latent Narrative Structure
https://www.aclweb.org/anthology/2020.acl-main.174
AUTHORS:                Pinelopi Papalampidi, Frank Keller, Lea Frermann, Mirella Lapata
HIGHLIGHT:              In this paper, we propose to explicitly incorporate the underlying structure of narratives into general
unsupervised and supervised extractive summarization models.

175, TITLE:             Unsupervised Opinion Summarization with Noising and Denoising
https://www.aclweb.org/anthology/2020.acl-main.175
AUTHORS:                Reinald Kim Amplayo, Mirella Lapata
HIGHLIGHT:              In this paper we enable the use of supervised learning for the setting where there are only documents available
(e.g., product or business reviews) without ground truth summaries.

176, TITLE:             A Tale of Two Perplexities: Sensitivity of Neural Language Models to Lexical Retrieval Deficits in Dementia
of the Alzheimer's Type
https://www.aclweb.org/anthology/2020.acl-main.176
AUTHORS:                Trevor Cohen, Serguei Pakhomov
HIGHLIGHT:              In this paper, we interrogate neural LMs trained on participants with and without dementia by using synthetic
narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency.

177, TITLE:             Probing Linguistic Systematicity
https://www.aclweb.org/anthology/2020.acl-main.177
AUTHORS:                Emily Goodwin, Koustuv Sinha, Timothy J. Oâ€™Donnell
HIGHLIGHT:              We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of
metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically,
and discuss why such forms of generalization may be unsatisfying.

178, TITLE:               Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models
https://www.aclweb.org/anthology/2020.acl-main.178
AUTHORS:                  Maarten Sap, Eric horvitz, Yejin Choi, Noah A. Smith, James Pennebaker
HIGHLIGHT:                We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled
events.

179, TITLE:               Recurrent Neural Network Language Models Always Learn English-Like Relative Clause Attachment
https://www.aclweb.org/anthology/2020.acl-main.179
AUTHORS:                  Forrest Davis, Marten van Schijndel
HIGHLIGHT:                Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple
simultaneous valid interpretations, where stark grammaticality differences are absent.

180, TITLE:               Speakers enhance contextually confusable words
https://www.aclweb.org/anthology/2020.acl-main.180
AUTHORS:                  Eric Meinhardt, Eric Bakovic, Leon Bergen
HIGHLIGHT:                We develop a measure of contextual confusability during word recognition based on psychoacoustic data.
Applying this measure to naturalistic speech corpora, we find evidence suggesting that speakers alter their productions to make
contextually more confusable words easier to understand.

181, TITLE:               What determines the order of adjectives in English? Comparing efficiency-based theories using dependency
treebanks
https://www.aclweb.org/anthology/2020.acl-main.181
AUTHORS:                  Richard Futrell, William Dyer, Greg Scontras
HIGHLIGHT:                The four theories we test are subjectivity (Scontras et al., 2017), information locality (Futrell, 2019), integration
cost (Dyer, 2017), and information gain, which we introduce.

182, TITLE:               ``None of the Above'': Measure Uncertainty in Dialog Response Retrieval
https://www.aclweb.org/anthology/2020.acl-main.182
AUTHORS:                  Yulan Feng, Shikib Mehri, Maxine Eskenazi, Tiancheng Zhao
HIGHLIGHT:                This paper discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our
experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus.

183, TITLE:               Can You Put it All Together: Evaluating Conversational Agents' Ability to Blend Skills
https://www.aclweb.org/anthology/2020.acl-main.183
AUTHORS:                  Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, Y-Lan Boureau
HIGHLIGHT:                In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from
simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass
several skills at all training stages.

184, TITLE:               Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs
https://www.aclweb.org/anthology/2020.acl-main.184
AUTHORS:                  Houyu Zhang, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu
HIGHLIGHT:                This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense
knowledge graphs to explicitly model conversation flows.

185, TITLE:               Negative Training for Neural Dialogue Response Generation
https://www.aclweb.org/anthology/2020.acl-main.185
AUTHORS:                  Tianxing He, James Glass
HIGHLIGHT:                In this work, we propose a framework named "Negative Training" to minimize such behaviors.

186, TITLE:               Recursive Template-based Frame Generation for Task Oriented Dialog
https://www.aclweb.org/anthology/2020.acl-main.186
AUTHORS:                  Rashmi Gangadharaiah, Balakrishnan Narayanaswamy
HIGHLIGHT:                We propose a recursive, hierarchical frame-based representation and show how to learn it from data.

187, TITLE:               Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback
https://www.aclweb.org/anthology/2020.acl-main.187
AUTHORS:                  Ahmed Elgohary, saghar Hosseini, Ahmed Hassan Awadallah
HIGHLIGHT:             In this paper, we investigate a more interactive scenario where humans can further interact with the system by
providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial
utterance.

188, TITLE:            Calibrating Structured Output Predictors for Natural Language Processing
https://www.aclweb.org/anthology/2020.acl-main.188
AUTHORS:               Abhyuday Jagannatha, hong yu
HIGHLIGHT:             In this study, we propose a general calibration scheme for output entities of interest in neural network based
structured prediction models.

189, TITLE:            Active Imitation Learning with Noisy Guidance
https://www.aclweb.org/anthology/2020.acl-main.189
AUTHORS:               Kiant&eacute; Brantley, Hal Daum&eacute; III, Amr Sharaf
HIGHLIGHT:             To combat this query complexity, we consider an active learning setting in which the learning algorithm has
additional access to a much cheaper noisy heuristic that provides noisy guidance.

190, TITLE:            ExpBERT: Representation Engineering with Natural Language Explanations
https://www.aclweb.org/anthology/2020.acl-main.190
AUTHORS:               Shikhar Murty, Pang Wei Koh, Percy Liang
HIGHLIGHT:             In this paper, we allow model developers to specify these types of inductive biases as natural language
explanations.

191, TITLE:            GAN-BERT: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled
Examples
https://www.aclweb.org/anthology/2020.acl-main.191
AUTHORS:               Danilo Croce, Giuseppe Castellucci, Roberto Basili
HIGHLIGHT:             In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled
data in a generative adversarial setting.

192, TITLE:            Generalizing Natural Language Analysis through Span-relation Representations
https://www.aclweb.org/anthology/2020.acl-main.192
AUTHORS:               Zhengbao Jiang, Wei Xu, Jun Araki, Graham Neubig
HIGHLIGHT:             In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified
format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different
tasks.

193, TITLE:            Learning to Contextually Aggregate Multi-Source Supervision for Sequence Labeling
https://www.aclweb.org/anthology/2020.acl-main.193
AUTHORS:               Ouyu Lan, Xiao Huang, Bill Yuchen Lin, He Jiang, Liyuan Liu, Xiang Ren
HIGHLIGHT:             In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations
from multiple sources (e.g., crowd annotation, cross-domain data).

194, TITLE:            MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification
https://www.aclweb.org/anthology/2020.acl-main.194
AUTHORS:               Jiaao Chen, Zichao Yang, Diyi Yang
HIGHLIGHT:             This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly
designed data augmentation method called TMix.

195, TITLE:            MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices
https://www.aclweb.org/anthology/2020.acl-main.195
AUTHORS:               Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou
HIGHLIGHT:             In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model.

196, TITLE:            On Importance Sampling-Based Evaluation of Latent Language Models
https://www.aclweb.org/anthology/2020.acl-main.196
AUTHORS:               Robert L Logan IV, Matt Gardner, Sameer Singh
HIGHLIGHT:             In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM.

197, TITLE:            SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled
Regularized Optimization
https://www.aclweb.org/anthology/2020.acl-main.197
AUTHORS:                Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Tuo Zhao
HIGHLIGHT:              To address such an issue in a principled manner, we propose a new learning framework for robust and efficient
fine-tuning for pre-trained models to attain better generalization performance.

198, TITLE:             Stolen Probability: A Structural Weakness of Neural Language Models
https://www.aclweb.org/anthology/2020.acl-main.198
AUTHORS:                David Demeter, Gregory Kimmel, Doug Downey
HIGHLIGHT:              We present numerical, theoretical and empirical analyses which show that words on the interior of the convex
hull in the embedding space have their probability bounded by the probabilities of the words on the hull.

199, TITLE:             Taxonomy Construction of Unseen Domains via Graph-based Cross-Domain Knowledge Transfer
https://www.aclweb.org/anthology/2020.acl-main.199
AUTHORS:                Chao Shang, Sarthak Dash, Md. Faisal Mahbub Chowdhury, Nandana Mihindukulasooriya, Alfio Gliozzo
HIGHLIGHT:              In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy
construction task.

200, TITLE:             To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks
https://www.aclweb.org/anthology/2020.acl-main.200
AUTHORS:                Sinong Wang, Madian Khabsa, Hao Ma
HIGHLIGHT:              This paper examines the benefits of pretrained models as a function of the number of training samples used in
the downstream task.

201, TITLE:             Why Overfitting Isn't Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries
https://www.aclweb.org/anthology/2020.acl-main.201
AUTHORS:                Mozhi Zhang, Yoshinari Fujinuma, Michael J. Paul, Jordan Boyd-Graber
HIGHLIGHT:              We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs
closer in the embedding space and overfits the training dictionary.

202, TITLE:             XtremeDistil: Multi-stage Distillation for Massive Multilingual Models
https://www.aclweb.org/anthology/2020.acl-main.202
AUTHORS:                Subhabrata Mukherjee, Ahmed Hassan Awadallah
HIGHLIGHT:              In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER).

203, TITLE:             A Girl Has A Name: Detecting Authorship Obfuscation
https://www.aclweb.org/anthology/2020.acl-main.203
AUTHORS:                Asad Mahmood, Zubair Shafiq, Padmini Srinivasan
HIGHLIGHT:              In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an
adversarial threat model.

204, TITLE:             DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference
https://www.aclweb.org/anthology/2020.acl-main.204
AUTHORS:                Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin
HIGHLIGHT:              We propose a simple but effective method, DeeBERT, to accelerate BERT inference.

205, TITLE:             Efficient Strategies for Hierarchical Text Classification: External Knowledge and Auxiliary Tasks
https://www.aclweb.org/anthology/2020.acl-main.205
AUTHORS:                Kervy Rivas Rojas, Gina Bustamante, Arturo Oncevay, Marco Antonio Sobrevilla Cabezudo
HIGHLIGHT:              In hierarchical text classification, we perform a sequence of inference steps to predict the category of a
document from top to bottom of a given class taxonomy.

206, TITLE:             Investigating the effect of auxiliary objectives for the automated grading of learner English speech
transcriptions
https://www.aclweb.org/anthology/2020.acl-main.206
AUTHORS:                Hannah Craighead, Andrew Caines, Paula Buttery, Helen Yannakoudakis
HIGHLIGHT:              Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion
that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main
task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (L1).

207, TITLE:             SPECTER: Document-level Representation Learning using Citation-informed Transformers
https://www.aclweb.org/anthology/2020.acl-main.207
AUTHORS:              Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel Weld
HIGHLIGHT:            We propose SPECTER, a new method to generate document-level embedding of scientific papers based on
pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.

208, TITLE:           Semantic Scaffolds for Pseudocode-to-Code Generation
https://www.aclweb.org/anthology/2020.acl-main.208
AUTHORS:              Ruiqi Zhong, Mitchell Stern, Dan Klein
HIGHLIGHT:            We propose a method for program generation based on semantic scaffolds, lightweight structures representing
the high-level semantic and syntactic composition of a program.

209, TITLE:           Can We Predict New Facts with Open Knowledge Graph Embeddings? A Benchmark for Open Link Prediction
https://www.aclweb.org/anthology/2020.acl-main.209
AUTHORS:              Samuel Broscheit, Kiril Gashteovski, Yanjie Wang, Rainer Gemulla
HIGHLIGHT:            In this paper, we investigate whether it is possible to infer new facts directly from the open knowledge graph
without any canonicalization or any supervision from curated knowledge.

210, TITLE:           INFOTABS: Inference on Tables as Semi-structured Data
https://www.aclweb.org/anthology/2020.acl-main.210
AUTHORS:              Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, Vivek Srikumar
HIGHLIGHT:            In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only
comprehending the meaning of text fragments, but also implicit relationships between them.

211, TITLE:           Interactive Machine Comprehension with Information Seeking Agents
https://www.aclweb.org/anthology/2020.acl-main.211
AUTHORS:              Xingdi Yuan, Jie Fu, Marc-Alexandre C&ocirc;t&eacute;, Yi Tay, Chris Pal, Adam Trischler
HIGHLIGHT:            In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially
observable environments.

212, TITLE:           Syntactic Data Augmentation Increases Robustness to Inference Heuristics
https://www.aclweb.org/anthology/2020.acl-main.212
AUTHORS:              Junghyun Min, R. Thomas McCoy, Dipanjan Das, Emily Pitler, Tal Linzen
HIGHLIGHT:            We explore several methods to augment standard training sets with syntactically informative examples,
generated by applying syntactic transformations to sentences from the MNLI corpus.

213, TITLE:           Improved Speech Representations with Multi-Target Autoregressive Predictive Coding
https://www.aclweb.org/anthology/2020.acl-main.213
AUTHORS:              Yu-An Chung, James Glass
HIGHLIGHT:            In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by
training the model to make more accurate future predictions.

214, TITLE:           Integrating Multimodal Information in Large Pretrained Transformers
https://www.aclweb.org/anthology/2020.acl-main.214
AUTHORS:              Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe
Morency, Ehsan Hoque
HIGHLIGHT:            In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG).

215, TITLE:           MultiQT: Multimodal learning for real-time question tracking in speech
https://www.aclweb.org/anthology/2020.acl-main.215
AUTHORS:              Jakob D. Havtorn, Jan Latko, Joakim Edin, Lars Maal&oslash;e, Lasse Borgholt, Lorenzo Belgrano, Nicolai
Jacobsen, Regitze Sdun, Å½eljko AgiÄ‡
HIGHLIGHT:            We propose a novel multimodal approach to real-time sequence labeling in speech.

216, TITLE:           Multimodal and Multiresolution Speech Recognition with Transformers
https://www.aclweb.org/anthology/2020.acl-main.216
AUTHORS:              Georgios Paraskevopoulos, Srinivas Parthasarathy, Aparna Khare, Shiva Sundaram
HIGHLIGHT:            This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based
architecture.

217, TITLE:           Phone Features Improve Speech Translation
https://www.aclweb.org/anthology/2020.acl-main.217
AUTHORS:                Elizabeth Salesky, Alan W Black
HIGHLIGHT:              We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that
cascades remain stronger baselines.

218, TITLE:             Grounding Conversations with Improvised Dialogues
https://www.aclweb.org/anthology/2020.acl-main.218
AUTHORS:                Hyundong Cho, Jonathan May
HIGHLIGHT:              We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting
them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier.

219, TITLE:             Image-Chat: Engaging Grounded Conversations
https://www.aclweb.org/anthology/2020.acl-main.219
AUTHORS:                Kurt Shuster, Samuel Humeau, Antoine Bordes, Jason Weston
HIGHLIGHT:              In this work we study large-scale architectures and datasets for this goal.

220, TITLE:             Learning an Unreferenced Metric for Online Dialogue Evaluation
https://www.aclweb.org/anthology/2020.acl-main.220
AUTHORS:                Koustuv Sinha, Prasanna Parthasarathi, Jasmine Wang, Ryan Lowe, William L. Hamilton, Joelle Pineau
HIGHLIGHT:              Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to
extract latent representations of utterances, and leverages the temporal transitions that exist between them.

221, TITLE:             Neural Generation of Dialogue Response Timings
https://www.aclweb.org/anthology/2020.acl-main.221
AUTHORS:                Matthew Roddy, Naomi Harte
HIGHLIGHT:              We propose neural models that simulate the distributions of these response offsets, taking into account the
response turn as well as the preceding turn.

222, TITLE:             The Dialogue Dodecathlon: Open-Domain Knowledge and Image Grounded Conversational Agents
https://www.aclweb.org/anthology/2020.acl-main.222
AUTHORS:                Kurt Shuster, Da JU, Stephen Roller, Emily Dinan, Y-Lan Boureau, Jason Weston
HIGHLIGHT:              We introduce dodecaDialogue: a set of 12 tasks that measures if a conversational agent can communicate
engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and
situations, and perceive and converse about images.

223, TITLE:             Automatic Poetry Generation from Prosaic Text
https://www.aclweb.org/anthology/2020.acl-main.223
AUTHORS:                Tim Van de Cruys
HIGHLIGHT:              In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and
literary aspects needed for poetry generation.

224, TITLE:             Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation
https://www.aclweb.org/anthology/2020.acl-main.224
AUTHORS:                Chao Zhao, Marilyn Walker, Snigdha Chaturvedi
HIGHLIGHT:              To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph
structure, but can also cater to the linear structure of the output text.

225, TITLE:             Enabling Language Models to Fill in the Blanks
https://www.aclweb.org/anthology/2020.acl-main.225
AUTHORS:                Chris Donahue, Mina Lee, Percy Liang
HIGHLIGHT:              We present a simple approach for \textit{text infilling}, the task of predicting missing spans of text at any
position in a document.

226, TITLE:             INSET: Sentence Infilling with INter-SEntential Transformer
https://www.aclweb.org/anthology/2020.acl-main.226
AUTHORS:                Yichen Huang, Yizhe Zhang, Oussama Elachqar, Yu Cheng
HIGHLIGHT:              In this paper, we propose a framework to decouple the challenge and address these three aspects respectively,
leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2.

227, TITLE:             Improving Adversarial Text Generation by Modeling the Distant Future
https://www.aclweb.org/anthology/2020.acl-main.227
AUTHORS:               Ruiyi Zhang, Changyou Chen, Zhe Gan, Wenlin Wang, Dinghan Shen, Guoyin Wang, Zheng Wen, Lawrence
Carin
HIGHLIGHT:             We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the
aforementioned issues.

228, TITLE:            Simple and Effective Retrieve-Edit-Rerank Text Generation
https://www.aclweb.org/anthology/2020.acl-main.228
AUTHORS:               Nabil Hossain, Marjan Ghazvininejad, Luke Zettlemoyer
HIGHLIGHT:             We propose to extend this framework with a simple and effective post-generation ranking approach.

229, TITLE:            BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby Steps
https://www.aclweb.org/anthology/2020.acl-main.229
AUTHORS:               Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene Ie, Fei Sha
HIGHLIGHT:             In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of
shorter ones.

230, TITLE:            Cross-media Structured Common Space for Multimedia Event Extraction
https://www.aclweb.org/anthology/2020.acl-main.230
AUTHORS:               Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng Ji, Shih-Fu Chang
HIGHLIGHT:             We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured
representations of semantic information from textual and visual data into a common embedding space.

231, TITLE:            Learning to Segment Actions from Observation and Narration
https://www.aclweb.org/anthology/2020.acl-main.231
AUTHORS:               Daniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen Clark, Aida Nematzadeh
HIGHLIGHT:             We apply a generative segmental model of task structure, guided by narration, to action segmentation in video.

232, TITLE:            Learning to execute instructions in a Minecraft dialogue
https://www.aclweb.org/anthology/2020.acl-main.232
AUTHORS:               Prashant Jayannavar, Anjali Narayan-Chen, Julia Hockenmaier
HIGHLIGHT:             We define the subtask of predicting correct action sequences (block placements and removals) in a given game
context, and show that capturing B's past actions as well as B's perspective leads to a significant improvement in performance on this
challenging language understanding problem.

233, TITLE:            MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning
https://www.aclweb.org/anthology/2020.acl-main.233
AUTHORS:               Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara Berg, Mohit Bansal
HIGHLIGHT:             Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART),
which uses a memory module to augment the transformer architecture.

234, TITLE:            What is Learned in Visually Grounded Neural Syntax Acquisition
https://www.aclweb.org/anthology/2020.acl-main.234
AUTHORS:               Noriyuki Kojima, Hadar Averbuch-Elor, Alexander Rush, Yoav Artzi
HIGHLIGHT:             We also find that a simple lexical signal of noun concreteness plays the main role in the model’s predictions as
opposed to more complex syntactic reasoning.

235, TITLE:            A Batch Normalized Inference Network Keeps the KL Vanishing Away
https://www.aclweb.org/anthology/2020.acl-main.235
AUTHORS:               Qile Zhu, Wei Bi, Xiaojiang Liu, Xiyao Ma, Xiaolin Li, Dapeng Wu
HIGHLIGHT:             We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to
prevent posterior collapse by keeping the expectation of the KL's distribution positive.

236, TITLE:            Contextual Embeddings: When Are They Worth It?
https://www.aclweb.org/anthology/2020.acl-main.236
AUTHORS:               Simran Arora, Avner May, Jian Zhang, Christopher R&eacute;
HIGHLIGHT:             We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in
performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline-random word embeddings-focusing
on the impact of the training set size and the linguistic properties of the task.

237, TITLE:             Interactive Classification by Asking Informative Questions
https://www.aclweb.org/anthology/2020.acl-main.237
AUTHORS:                Lili Yu, Howard Chen, Sida I. Wang, Tao Lei, Yoav Artzi
HIGHLIGHT:              We study the potential for interaction in natural language classification.

238, TITLE:             Knowledge Graph Embedding Compression
https://www.aclweb.org/anthology/2020.acl-main.238
AUTHORS:                Mrinmaya Sachan
HIGHLIGHT:              Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG
as a vector of discrete codes and then composes the embeddings from these codes.

239, TITLE:             Low Resource Sequence Tagging using Sentence Reconstruction
https://www.aclweb.org/anthology/2020.acl-main.239
AUTHORS:                Tal Perl, Sriram Chaudhury, Raja Giryes
HIGHLIGHT:              Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can
improve the performance of various baselines.

240, TITLE:             Masked Language Model Scoring
https://www.aclweb.org/anthology/2020.acl-main.240
AUTHORS:                Julian Salazar, Davis Liang, Toan Q. Nguyen, Katrin Kirchhoff
HIGHLIGHT:              We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.

241, TITLE:             Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding
https://www.aclweb.org/anthology/2020.acl-main.241
AUTHORS:                Yun Tang, Jing Huang, Guangtao Wang, Xiaodong He, Bowen Zhou
HIGHLIGHT:              In this work, we propose a novel distance-based approach for knowledge graph link prediction.

242, TITLE:             Posterior Calibrated Training on Sentence Classification Tasks
https://www.aclweb.org/anthology/2020.acl-main.242
AUTHORS:                Taehee Jung, Dongyeop Kang, Hua Cheng, Lucas Mentch, Thomas Schaaf
HIGHLIGHT:              Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly
optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.

243, TITLE:             Posterior Control of Blackbox Generation
https://www.aclweb.org/anthology/2020.acl-main.243
AUTHORS:                Xiang Lisa Li, Alexander Rush
HIGHLIGHT:              In this work, we consider augmenting neural generation models with discrete control states learned through a
structured latent-variable approach.

244, TITLE:             Pretrained Transformers Improve Out-of-Distribution Robustness
https://www.aclweb.org/anthology/2020.acl-main.244
AUTHORS:                Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, Dawn Song
HIGHLIGHT:              We examine which factors affect robustness, finding that larger models are not necessarily more robust,
distillation can be harmful, and more diverse pretraining data can enhance robustness.

245, TITLE:             Robust Encodings: A Framework for Combating Adversarial Typos
https://www.aclweb.org/anthology/2020.acl-main.245
AUTHORS:                Erik Jones, Robin Jia, Aditi Raghunathan, Percy Liang
HIGHLIGHT:              In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness,
without making compromises on model architecture.

246, TITLE:             Showing Your Work Doesn't Always Work
https://www.aclweb.org/anthology/2020.acl-main.246
AUTHORS:                Raphael Tang, Jaejun Lee, Ji Xin, Xinyu Liu, Yaoliang Yu, Jimmy Lin
HIGHLIGHT:              One exemplar publication, titled “Show Your Work: Improved Reporting of Experimental Results” (Dodge et
al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational
budget. In the present work, we critically examine this paper. As far as statistical generalizability is concerned, we find unspoken
pitfalls and caveats with this approach.

247, TITLE:             Span Selection Pre-training for Question Answering
https://www.aclweb.org/anthology/2020.acl-main.247
AUTHORS:                Michael Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G P Shrivatsa Bhargav, Dinesh
Garg, Avi Sil
HIGHLIGHT:              In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-
training from memorization to understanding.

248, TITLE:             Topological Sort for Sentence Ordering
https://www.aclweb.org/anthology/2020.acl-main.248
AUTHORS:                Shrimai Prabhumoye, Ruslan Salakhutdinov, Alan W Black
HIGHLIGHT:              In this paper, we propose a new framing of this task as a constraint solving problem and introduce a new
technique to solve it.

249, TITLE:             Weight Poisoning Attacks on Pretrained Models
https://www.aclweb.org/anthology/2020.acl-main.249
AUTHORS:                Keita Kurita, Paul Michel, Graham Neubig
HIGHLIGHT:              In this paper, we show that it is possible to construct "weight poisoning" attacks where pre-trained weights are
injected with vulnerabilities that expose "backdoors" after fine-tuning, enabling the attacker to manipulate the model prediction simply
by injecting an arbitrary keyword.

250, TITLE:             schuBERT: Optimizing Elements of BERT
https://www.aclweb.org/anthology/2020.acl-main.250
AUTHORS:                Ashish Khetan, Zohar Karnin
HIGHLIGHT:              In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model.

251, TITLE:             ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.251
AUTHORS:                Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel
HIGHLIGHT:              We propose to train a non-autoregressive machine translation model to minimize the energy defined by a
pretrained autoregressive model.

252, TITLE:             Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.252
AUTHORS:                Aditya Siddhant, Ankur Bapna, Yuan Cao, Orhan Firat, Mia Chen, Sneha Kudugunta, Naveen Arivazhagan,
Yonghui Wu
HIGHLIGHT:              In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-
supervision in multilingual NMT.

253, TITLE:             On The Evaluation of Machine Translation SystemsTrained With Back-Translation
https://www.aclweb.org/anthology/2020.acl-main.253
AUTHORS:                Sergey Edunov, Myle Ott, Marcâ€™Aurelio Ranzato, Michael Auli
HIGHLIGHT:              In this work, we show that this conjecture is not empirically supported and that back-translation improves
translation quality of both naturally occurring text as well as translationese according to professional human translators.

254, TITLE:             Simultaneous Translation Policies: From Fixed to Adaptive
https://www.aclweb.org/anthology/2020.acl-main.254
AUTHORS:                Baigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma, Hairong Liu, Liang Huang
HIGHLIGHT:              We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed
policies.

255, TITLE:             Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by
Incorporating Knowledge Graph Information
https://www.aclweb.org/anthology/2020.acl-main.255
AUTHORS:                Michele Bevilacqua, Roberto Navigli
HIGHLIGHT:              We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised
architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural
architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set.

256, TITLE:             Glyph2Vec: Learning Chinese Out-of-Vocabulary Word Embedding from Glyphs
https://www.aclweb.org/anthology/2020.acl-main.256
AUTHORS:                Hong-You Chen, SZ-HAN YU, Shou-de Lin
HIGHLIGHT:             We present a multi-modal model, \textit{Glyph2Vec}, to tackle Chinese out-of-vocabulary word embedding
problem.

257, TITLE:            Multidirectional Associative Optimization of Function-Specific Word Representations
https://www.aclweb.org/anthology/2020.acl-main.257
AUTHORS:               Daniela Gerz, Ivan VuliÄ‡, Marek Rei, Roi Reichart, Anna Korhonen
HIGHLIGHT:             We present a neural framework for learning associations between interrelated groups of words such as the ones
found in Subject-Verb-Object (SVO) structures.

258, TITLE:            Predicting Degrees of Technicality in Automatic Terminology Extraction
https://www.aclweb.org/anthology/2020.acl-main.258
AUTHORS:               Anna H&auml;tty, Dominik Schlechtweg, Michael Dorna, Sabine Schulte im Walde
HIGHLIGHT:             We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network
model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison
internally.

259, TITLE:            Verbal Multiword Expressions for Identification of Metaphor
https://www.aclweb.org/anthology/2020.acl-main.259
AUTHORS:               Omid Rohanian, Marek Rei, Shiva Taslimipoor, Le An Ha
HIGHLIGHT:             This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design
of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs.

260, TITLE:            Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer
https://www.aclweb.org/anthology/2020.acl-main.260
AUTHORS:               Jieyu Zhao, Subhabrata Mukherjee, saghar Hosseini, Kai-Wei Chang, Ahmed Hassan Awadallah
HIGHLIGHT:             In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP
applications.

261, TITLE:            Give Me Convenience and Give Her Death: Who Should Decide What Uses of NLP are Appropriate, and on
What Basis?
https://www.aclweb.org/anthology/2020.acl-main.261
AUTHORS:               kobi leins, Jey Han Lau, Timothy Baldwin
HIGHLIGHT:             We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was
a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with
making such a decision, and on what basis.

262, TITLE:            Is Your Classifier Actually Biased? Measuring Fairness under Uncertainty with Bernstein Bounds
https://www.aclweb.org/anthology/2020.acl-main.262
AUTHORS:               Kawin Ethayarajh
HIGHLIGHT:             In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a
confidence interval.
Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias?

263, TITLE:            It's Morphin' Time! Combating Linguistic Discrimination with Inflectional Perturbations
https://www.aclweb.org/anthology/2020.acl-main.263
AUTHORS:               Samson Tan, Shafiq Joty, Min-Yen Kan, Richard Socher
HIGHLIGHT:             We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial
examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them
for a single epoch significantly improves robustness without sacrificing performance on clean data.

264, TITLE:            Mitigating Gender Bias Amplification in Distribution by Posterior Regularization
https://www.aclweb.org/anthology/2020.acl-main.264
AUTHORS:               Shengyu Jia, Tao Meng, Jieyu Zhao, Kai-Wei Chang
HIGHLIGHT:             In this paper, we investigate the gender bias amplification issue from the distribution perspective and
demonstrate that the bias is amplified in the view of predicted probability distribution over labels.

265, TITLE:            Towards Understanding Gender Bias in Relation Extraction
https://www.aclweb.org/anthology/2020.acl-main.265
AUTHORS:               Andrew Gaut, Tony Sun, Shirlyn Tang, Yuxin Huang, Jing Qian, Mai ElSherief, Jieyu Zhao, Diba Mirza,
Elizabeth Belding, Kai-Wei Chang, William Yang Wang
HIGHLIGHT:             In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences
including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems.

266, TITLE:            A Probabilistic Generative Model for Typographical Analysis of Early Modern Printing
https://www.aclweb.org/anthology/2020.acl-main.266
AUTHORS:               Kartik Goyal, Chris Dyer, Christopher Warren, Maxwell Gâ€™Sell, Taylor Berg-Kirkpatrick
HIGHLIGHT:             We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early
Modern documents.

267, TITLE:            Attentive Pooling with Learnable Norms for Text Representation
https://www.aclweb.org/anthology/2020.acl-main.267
AUTHORS:               Chuhan Wu, Fangzhao Wu, Tao Qi, Xiaohui Cui, Yongfeng Huang
HIGHLIGHT:             In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation.

268, TITLE:            Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks
https://www.aclweb.org/anthology/2020.acl-main.268
AUTHORS:               Fynn Schr&ouml;der, Chris Biemann
HIGHLIGHT:             We propose new methods to automatically assess the similarity of sequence tagging datasets to identify
beneficial auxiliary data for MTL or TL setups.

269, TITLE:            How Does Selective Mechanism Improve Self-Attention Networks?
https://www.aclweb.org/anthology/2020.acl-main.269
AUTHORS:               Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, Zhaopeng Tu
HIGHLIGHT:             In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented
with a flexible and universal Gumbel-Softmax.

270, TITLE:            Improving Transformer Models by Reordering their Sublayers
https://www.aclweb.org/anthology/2020.acl-main.270
AUTHORS:               Ofir Press, Noah A. Smith, Omer Levy
HIGHLIGHT:             We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it
improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or
training time.

271, TITLE:            Single Model Ensemble using Pseudo-Tags and Distinct Vectors
https://www.aclweb.org/anthology/2020.acl-main.271
AUTHORS:               Ryosuke Kuwabara, Jun Suzuki, Hideki Nakayama
HIGHLIGHT:             In this study, we propose a novel method that replicates the effects of a model ensemble with a single model.

272, TITLE:            Zero-shot Text Classification via Reinforced Self-training
https://www.aclweb.org/anthology/2020.acl-main.272
AUTHORS:               Zhiquan Ye, Yuxia Geng, Jiaoyan Chen, Jingmin Chen, Xiaoxiao Xu, SuHang Zheng, Feng Wang, Jun Zhang,
Huajun Chen
HIGHLIGHT:             To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled
data.

273, TITLE:            A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.273
AUTHORS:               Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, Jiebo Luo
HIGHLIGHT:             To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.

274, TITLE:            A Relaxed Matching Procedure for Unsupervised BLI
https://www.aclweb.org/anthology/2020.acl-main.274
AUTHORS:               Xu Zhao, Zihao Wang, Yong Zhang, Hao Wu
HIGHLIGHT:             Thus We propose a relaxed matching procedure to find a more precise matching between two languages.

275, TITLE:            Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.275
AUTHORS:               Xuanli He, Gholamreza Haffari, Mohammad Norouzi
HIGHLIGHT:             This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing
sentences into subword units.

276, TITLE:            Geometry-aware domain adaptation for unsupervised alignment of word embeddings
https://www.aclweb.org/anthology/2020.acl-main.276
AUTHORS:               Pratik Jawanpuria, Mayank Meghwanshi, Bamdev Mishra
HIGHLIGHT:             We propose a novel manifold based geometric approach for learning unsupervised alignment of word
embeddings between the source and the target languages.

277, TITLE:            Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.277
AUTHORS:               Qiu Ran, Yankai Lin, Peng Li, Jie Zhou
HIGHLIGHT:             To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which
generates a translation as a sequence of segments.

278, TITLE:            On the Inference Calibration of Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.278
AUTHORS:               Shuo Wang, Zhaopeng Tu, Shuming Shi, Yang Liu
HIGHLIGHT:             By carefully designing experiments on three language pairs, our work provides in-depth analyses of the
correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of
interesting findings that might help humans better analyze, understand and improve NMT models.

279, TITLE:            Camouflaged Chinese Spam Content Detection with Semi-supervised Generative Active Learning
https://www.aclweb.org/anthology/2020.acl-main.279
AUTHORS:               Zhuoren Jiang, Zhe Gao, Yu Duan, Yangyang Kang, Changlong Sun, Qiong Zhang, Xiaozhong Liu
HIGHLIGHT:             We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance,
efficiency, and text camouflage problems of Chinese text spam detection task.

280, TITLE:            Distinguish Confusing Law Articles for Legal Judgment Prediction
https://www.aclweb.org/anthology/2020.acl-main.280
AUTHORS:               Nuo Xu, Pinghui Wang, Long Chen, Li Pan, Xiaoyan Wang, Junzhou Zhao
HIGHLIGHT:             In this paper, we present an end-to-end model, LADAN, to solve the task of LJP.

281, TITLE:            Hiring Now: A Skill-Aware Multi-Attention Model for Job Posting Generation
https://www.aclweb.org/anthology/2020.acl-main.281
AUTHORS:               Liting Liu, Jie Liu, Wenzheng Zhang, Ziming Chi, Wenxuan Shi, Yalou Huang
HIGHLIGHT:             To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text
generation problem to generate job requirements according to the job descriptions.

282, TITLE:            HyperCore: Hyperbolic and Co-graph Representation for Automatic ICD Coding
https://www.aclweb.org/anthology/2020.acl-main.282
AUTHORS:               Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao, Shengping Liu, Weifeng Chong
HIGHLIGHT:             In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above
problem.

283, TITLE:            Hyperbolic Capsule Networks for Multi-Label Classification
https://www.aclweb.org/anthology/2020.acl-main.283
AUTHORS:               Boli Chen, Xin Huang, Lin Xiao, Liping Jing
HIGHLIGHT:             Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which
have two merits.

284, TITLE:            Improving Segmentation for Technical Support Problems
https://www.aclweb.org/anthology/2020.acl-main.284
AUTHORS:               Kushal Chauhan, Abhirut Gupta
HIGHLIGHT:             In this paper, we address the problem of segmentation for technical support questions.

285, TITLE:            MOOCCube: A Large-scale Data Repository for NLP Applications in MOOCs
https://www.aclweb.org/anthology/2020.acl-main.285
AUTHORS:               Jifan Yu, Gan Luo, Tong Xiao, Qingyang Zhong, Yuquan Wang, wenzheng feng, Junyi Luo, Chenyu Wang,
Lei Hou, Juanzi Li, Zhiyuan Liu, Jie Tang
HIGHLIGHT:             Therefore, we present MOOCCube, a large-scale data repository of over 700 MOOC courses, 100k concepts, 8
million student behaviors with an external resource.

286, TITLE:             Towards Interpretable Clinical Diagnosis with Bayesian Network Ensembles Stacked on Entity-Aware CNNs
https://www.aclweb.org/anthology/2020.acl-main.286
AUTHORS:                Jun Chen, Xiaoya Dai, Quan Yuan, Chao Lu, Haifeng Huang
HIGHLIGHT:              In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network
Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis
system.

287, TITLE:             Analyzing the Persuasive Effect of Style in News Editorial Argumentation
https://www.aclweb.org/anthology/2020.acl-main.287
AUTHORS:                Roxanne El Baff, Henning Wachsmuth, Khalid Al Khatib, Benno Stein
HIGHLIGHT:              In contrast, this paper studies how important the style of news editorials is to achieve persuasion.

288, TITLE:             ECPE-2D: Emotion-Cause Pair Extraction based on Joint Two-Dimensional Representation, Interaction and
Prediction
https://www.aclweb.org/anthology/2020.acl-main.288
AUTHORS:                Zixiang Ding, Rui Xia, Jianfei Yu
HIGHLIGHT:              To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-
Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme.

289, TITLE:             Effective Inter-Clause Modeling for End-to-End Emotion-Cause Pair Extraction
https://www.aclweb.org/anthology/2020.acl-main.289
AUTHORS:                Penghui Wei, Jiahao Zhao, Wenji Mao
HIGHLIGHT:              In this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair
candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end
extraction.

290, TITLE:             Embarrassingly Simple Unsupervised Aspect Extraction
https://www.aclweb.org/anthology/2020.acl-main.290
AUTHORS:                St&eacute;phan Tulkens, Andreas van Cranenburgh
HIGHLIGHT:              We present a simple but effective method for aspect identification in sentiment analysis.

291, TITLE:             Enhancing Cross-target Stance Detection with Transferable Semantic-Emotion Knowledge
https://www.aclweb.org/anthology/2020.acl-main.291
AUTHORS:                Bowen Zhang, Min Yang, Xutao Li, Yunming Ye, Xiaofei Xu, Kuai Dai
HIGHLIGHT:              In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance
detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across
different targets.

292, TITLE:             KinGDOM: Knowledge-Guided DOMain Adaptation for Sentiment Analysis
https://www.aclweb.org/anthology/2020.acl-main.292
AUTHORS:                Deepanway Ghosal, Devamanyu Hazarika, Abhinaba Roy, Navonil Majumder, Rada Mihalcea, Soujanya Poria
HIGHLIGHT:              In this paper, we take a novel perspective on this task by exploring the role of external commonsense
knowledge.

293, TITLE:             Modelling Context and Syntactical Features for Aspect-based Sentiment Analysis
https://www.aclweb.org/anthology/2020.acl-main.293
AUTHORS:                Minh Hieu Phan, Philip O. Ogunbona
HIGHLIGHT:              This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for
syntactical learning.

294, TITLE:             Parallel Data Augmentation for Formality Style Transfer
https://www.aclweb.org/anthology/2020.acl-main.294
AUTHORS:                Yi Zhang, Tao Ge, Xu SUN
HIGHLIGHT:              In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods
for this task to obtain useful sentence pairs with easily accessible models and systems.

295, TITLE:             Relational Graph Attention Network for Aspect-based Sentiment Analysis
https://www.aclweb.org/anthology/2020.acl-main.295
AUTHORS:                Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, Rui Wang
HIGHLIGHT:               In this paper, we address this problem by means of effective encoding of syntax information.

296, TITLE:              SpanMlt: A Span-based Multi-Task Learning Framework for Pair-wise Aspect and Opinion Terms Extraction
https://www.aclweb.org/anthology/2020.acl-main.296
AUTHORS:                 He Zhao, Longtao Huang, Rong Zhang, Quan Lu, hui xue
HIGHLIGHT:               To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms
Extraction (PAOTE).

297, TITLE:              Syntax-Aware Opinion Role Labeling with Dependency Graph Convolutional Networks
https://www.aclweb.org/anthology/2020.acl-main.297
AUTHORS:                 Bo Zhang, Yue Zhang, Rui Wang, Zhenghua Li, Min Zhang
HIGHLIGHT:               In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating
different representations.

298, TITLE:              Towards Better Non-Tree Argument Mining: Proposition-Level Biaffine Parsing with Task-Specific
Parameterization
https://www.aclweb.org/anthology/2020.acl-main.298
AUTHORS:                 Gaku Morio, Hiroaki Ozaki, Terufumi Morishita, Yuta Koreeda, Kohsuke Yanai
HIGHLIGHT:               In this paper, we focus on non-tree argument mining with a neural model.

299, TITLE:              A Span-based Linearization for Constituent Trees
https://www.aclweb.org/anthology/2020.acl-main.299
AUTHORS:                 Yang Wei, Yuanbin Wu, Man Lan
HIGHLIGHT:               We propose a novel linearization of a constituent tree, together with a new locally normalized model.

300, TITLE:              An Empirical Comparison of Unsupervised Constituency Parsing Methods
https://www.aclweb.org/anthology/2020.acl-main.300
AUTHORS:                 Jun Li, Yifan Cao, Jiong Cai, Yong Jiang, Kewei Tu
HIGHLIGHT:               In this paper, we first examine experimental settings used in previous work and propose to standardize the
settings for better comparability between methods.

301, TITLE:              Efficient Constituency Parsing by Pointing
https://www.aclweb.org/anthology/2020.acl-main.301
AUTHORS:                 Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty, Xiaoli Li
HIGHLIGHT:               We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks.

302, TITLE:              Efficient Second-Order TreeCRF for Neural Dependency Parsing
https://www.aclweb.org/anthology/2020.acl-main.302
AUTHORS:                 Yu Zhang, Zhenghua Li, Min Zhang
HIGHLIGHT:               To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large
matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation.

303, TITLE:              Representations of Syntax [MASK] Useful: Effects of Constituency and Dependency Structure in Recursive
LSTMs
https://www.aclweb.org/anthology/2020.acl-main.303
AUTHORS:                 Michael Lepori, Tal Linzen, R. Thomas McCoy
HIGHLIGHT:               We evaluate which of these two representational schemes more effectively introduces biases for syntactic
structure that increase performance on the subject-verb agreement prediction task.

304, TITLE:              Structure-Level Knowledge Distillation For Multilingual Sequence Labeling
https://www.aclweb.org/anthology/2020.acl-main.304
AUTHORS:                 Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Fei Huang, Kewei Tu
HIGHLIGHT:               In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by
distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student).

305, TITLE:              Dynamic Online Conversation Recommendation
https://www.aclweb.org/anthology/2020.acl-main.305
AUTHORS:                 Xingshan Zeng, Jing Li, Lu Wang, Zhiming Mao, Kam-Fai Wong
HIGHLIGHT:               Concretely, we propose a neural architecture to exploit changes of user interactions and interests over time, to
predict which discussions they are likely to enter.

306, TITLE:             Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal
Transformer
https://www.aclweb.org/anthology/2020.acl-main.306
AUTHORS:                Jianfei Yu, Jing Jiang, Li Yang, Rui Xia
HIGHLIGHT:              In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts.

307, TITLE:             Stock Embeddings Acquired from News Articles and Price History, and an Application to Portfolio
Optimization
https://www.aclweb.org/anthology/2020.acl-main.307
AUTHORS:                Xin Du, Kumiko Tanaka-Ishii
HIGHLIGHT:              In contrast, this paper presents a method to encode the influence of news articles through a vector representation
of stocks called a \textit{stock embedding}.

308, TITLE:             What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context
https://www.aclweb.org/anthology/2020.acl-main.308
AUTHORS:                Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov, Ahmed Ali, James Glass, Preslav
Nakov
HIGHLIGHT:              Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium,
and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium's audience on social media).

309, TITLE:             An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language
Models
https://www.aclweb.org/anthology/2020.acl-main.309
AUTHORS:                Hiroshi Noji, Hiroya Takamura
HIGHLIGHT:              We explore the utilities of explicit negative examples in training neural language models.

310, TITLE:             On the Robustness of Language Encoders against Grammatical Errors
https://www.aclweb.org/anthology/2020.acl-main.310
AUTHORS:                Fan Yin, Quanyu Long, Tao Meng, Kai-Wei Chang
HIGHLIGHT:              We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and
RoBERTa) when confronted with natural grammatical errors.

311, TITLE:             Roles and Utilization of Attention Heads in Transformer-based Neural Language Models
https://www.aclweb.org/anthology/2020.acl-main.311
AUTHORS:                Jae-young Jo, Sung-Hyon Myaeng
HIGHLIGHT:              Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively
simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance
improvements on the downstream tasks.

312, TITLE:             Understanding Attention for Text Classification
https://www.aclweb.org/anthology/2020.acl-main.312
AUTHORS:                Xiaobing Sun, Wei Lu
HIGHLIGHT:              In this work, we present a study on understanding the internal mechanism of attention by looking into the
gradient update process, checking its behavior when approaching a local minimum during training.

313, TITLE:             A Relational Memory-based Embedding Model for Triple Classification and Search Personalization
https://www.aclweb.org/anthology/2020.acl-main.313
AUTHORS:                Dai Quoc Nguyen, Tu Nguyen, Dinh Phung
HIGHLIGHT:              To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network
to encode potential dependencies in relationship triples.

314, TITLE:             Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods
https://www.aclweb.org/anthology/2020.acl-main.314
AUTHORS:                Ning Miao, Yuxuan Song, Hao Zhou, Lei Li
HIGHLIGHT:              In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by
truncating and transferring the probability mass from over-estimated regions to under-estimated ones.

315, TITLE:             Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention
https://www.aclweb.org/anthology/2020.acl-main.315
AUTHORS:               Yanzeng Li, Bowen Yu, Xue Mengge, Tingwen Liu
HIGHLIGHT:             Hence, we propose a novel word-aligned attention to exploit explicit word information, which is
complementary to various character-based Chinese pre-trained language models.

316, TITLE:            On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond
https://www.aclweb.org/anthology/2020.acl-main.316
AUTHORS:               Chen Wu, Prince Zizhuang Wang, William Yang Wang
HIGHLIGHT:             To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the
same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching.

317, TITLE:            SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions
https://www.aclweb.org/anthology/2020.acl-main.317
AUTHORS:               Mao Ye, Chengyue Gong, Qiang Liu
HIGHLIGHT:             In this work, we propose a certified robust method based on a new randomized smoothing technique, which
constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties
of the ensemble to provably certify the robustness.

318, TITLE:            A Graph-based Coarse-to-fine Method for Unsupervised Bilingual Lexicon Induction
https://www.aclweb.org/anthology/2020.acl-main.318
AUTHORS:               Shuo Ren, Shujie Liu, Ming Zhou, Shuai Ma
HIGHLIGHT:             To deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons
in a coarse-to-fine way.

319, TITLE:            A Reinforced Generation of Adversarial Examples for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.319
AUTHORS:               wei zou, Shujian Huang, Jun Xie, Xinyu Dai, Jiajun CHEN
HIGHLIGHT:             Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this
issue by generating adversarial examples via a new paradigm based on reinforcement learning.

320, TITLE:            A Retrieve-and-Rewrite Initialization Method for Unsupervised Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.320
AUTHORS:               Shuo Ren, Yu Wu, Shujie Liu, Ming Zhou, Shuai Ma
HIGHLIGHT:             In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised
translation models.

321, TITLE:            A Simple and Effective Unified Encoder for Document-Level Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.321
AUTHORS:               Shuming Ma, Dongdong Zhang, Ming Zhou
HIGHLIGHT:             In this work, we propose a simple and effective unified encoder that can outperform the baseline models of
dual-encoder models in terms of BLEU and METEOR scores.

322, TITLE:            Does Multi-Encoder Help? A Case Study on Context-Aware Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.322
AUTHORS:               Bei Li, Hui Liu, Ziyang Wang, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, changliang li
HIGHLIGHT:             In this paper, we investigate multi-encoder approaches in document-level neural machine translation (NMT).

323, TITLE:            Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change
https://www.aclweb.org/anthology/2020.acl-main.323
AUTHORS:               Hongfei Xu, Josef van Genabith, Deyi Xiong, Qiuhui Liu
HIGHLIGHT:             To improve the efficiency of our approach for large models, we propose a sampling approach to select gradients
of parameters sensitive to the batch size.

324, TITLE:            Knowledge Distillation for Multilingual Unsupervised Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.324
AUTHORS:               Haipeng Sun, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Tiejun Zhao
HIGHLIGHT:             In this paper, we empirically introduce a simple method to translate between thirteen languages using a single
encoder and a single decoder, making use of multilingual data to improve UNMT for all language pairs.

325, TITLE:            Lexically Constrained Neural Machine Translation with Levenshtein Transformer
https://www.aclweb.org/anthology/2020.acl-main.325
AUTHORS:               Raymond Hendy Susanto, Shamil Chollampatt, Liling Tan
HIGHLIGHT:             This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine
translation.

326, TITLE:            On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.326
AUTHORS:               Chaojun Wang, Rico Sennrich
HIGHLIGHT:             In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate
hallucinations under domain shift.

327, TITLE:            Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model
https://www.aclweb.org/anthology/2020.acl-main.327
AUTHORS:               Kosuke Takahashi, Katsuhito Sudoh, Satoshi Nakamura
HIGHLIGHT:             We propose an automatic evaluation method of machine translation that uses source language sentences
regarded as additional pseudo references.

328, TITLE:            ChartDialogs: Plotting from Natural Language Instructions
https://www.aclweb.org/anthology/2020.acl-main.328
AUTHORS:               Yutong Shao, Ndapa Nakashole
HIGHLIGHT:             This paper presents the problem of conversational plotting agents that carry out plotting actions from natural
language instructions.

329, TITLE:            GLUECoS: An Evaluation Benchmark for Code-Switched NLP
https://www.aclweb.org/anthology/2020.acl-main.329
AUTHORS:               Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, Monojit Choudhury
HIGHLIGHT:             We present an evaluation benchmark, GLUECoS, for code-switched languages, that spans several NLP tasks in
English-Hindi and English-Spanish.

330, TITLE:            MATINF: A Jointly Labeled Large-Scale Dataset for Classification, Question Answering and Summarization
https://www.aclweb.org/anthology/2020.acl-main.330
AUTHORS:               Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, Chenliang Li
HIGHLIGHT:             We propose MATINF, the first jointly labeled large-scale dataset for classification, question answering and
summarization.

331, TITLE:            MIND: A Large-scale Dataset for News Recommendation
https://www.aclweb.org/anthology/2020.acl-main.331
AUTHORS:               Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie,
Jianfeng Gao, Winnie Wu, Ming Zhou
HIGHLIGHT:             In this paper, we present a large-scale dataset named MIND for news recommendation.

332, TITLE:            That is a Known Lie: Detecting Previously Fact-Checked Claims
https://www.aclweb.org/anthology/2020.acl-main.332
AUTHORS:               Shaden Shaar, Nikolay Babulkov, Giovanni Da San Martino, Preslav Nakov
HIGHLIGHT:             Interestingly, despite the importance of the task, it has been largely ignored by the research community so far.
Here, we aim to bridge this gap.

333, TITLE:            Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation
https://www.aclweb.org/anthology/2020.acl-main.333
AUTHORS:               Bo Pang, Erik Nijkamp, Wenjuan Han, Linqi Zhou, Yixian Liu, Kewei Tu
HIGHLIGHT:             In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues.

334, TITLE:            BiRRE: Learning Bidirectional Residual Relation Embeddings for Supervised Hypernymy Detection
https://www.aclweb.org/anthology/2020.acl-main.334
AUTHORS:               Chengyu Wang, XIAOFENG HE
HIGHLIGHT:             In this work, we revisit supervised distributional models for hypernymy detection.

335, TITLE:            Biomedical Entity Representations with Synonym Marginalization
https://www.aclweb.org/anthology/2020.acl-main.335
AUTHORS:               Mujeen Sung, Hwisang Jeon, Jinhyuk Lee, Jaewoo Kang
HIGHLIGHT:             In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of
entities.

336, TITLE:            Hypernymy Detection for Low-Resource Languages via Meta Learning
https://www.aclweb.org/anthology/2020.acl-main.336
AUTHORS:               Changlong Yu, Jialong Han, Haisong Zhang, Wilfred Ng
HIGHLIGHT:             This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages.

337, TITLE:            Investigating Word-Class Distributions in Word Vector Spaces
https://www.aclweb.org/anthology/2020.acl-main.337
AUTHORS:               Ryohei Sasano, Anna Korhonen
HIGHLIGHT:             This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a
pre-trained word vector space.

338, TITLE:            Aspect Sentiment Classification with Document-level Sentiment Preference Modeling
https://www.aclweb.org/anthology/2020.acl-main.338
AUTHORS:               Xiao Chen, Changlong Sun, Jingjing Wang, Shoushan Li, Luo Si, Min Zhang, Guodong Zhou
HIGHLIGHT:             In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual
sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all
the related aspects (namely inter-aspect sentiment tendency).

339, TITLE:            Don't Eclipse Your Arts Due to Small Discrepancies: Boundary Repositioning with a Pointer Network for
Aspect Extraction
https://www.aclweb.org/anthology/2020.acl-main.339
AUTHORS:               Zhenkai Wei, Yu Hong, Bowei Zou, Meng Cheng, Jianmin YAO
HIGHLIGHT:             In this paper, we propose to utilize a pointer network for repositioning the boundaries.

340, TITLE:            Relation-Aware Collaborative Learning for Unified Aspect-Based Sentiment Analysis
https://www.aclweb.org/anthology/2020.acl-main.340
AUTHORS:               Zhuang Chen, Tieyun Qian
HIGHLIGHT:             In order to fully exploit these relations, we propose a Relation-Aware Collaborative Learning (RACL)
framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a
stacked multi-layer network.

341, TITLE:            SentiBERT: A Transferable Transformer-Based Architecture for Compositional Sentiment Semantics
https://www.aclweb.org/anthology/2020.acl-main.341
AUTHORS:               Da Yin, Tao Meng, Kai-Wei Chang
HIGHLIGHT:             We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics.

342, TITLE:            Transition-based Directed Graph Construction for Emotion-Cause Pair Extraction
https://www.aclweb.org/anthology/2020.acl-main.342
AUTHORS:               Chuang Fan, Chaofa Yuan, Jiachen Du, Lin Gui, Min Yang, Ruifeng Xu
HIGHLIGHT:             Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like
directed graph construction.

343, TITLE:            CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality
https://www.aclweb.org/anthology/2020.acl-main.343
AUTHORS:               Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, Kaicheng Yang
HIGHLIGHT:             In this paper, we introduce a Chinese single- and multi-modal sentiment analysis dataset, CH-SIMS, which
contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations.

344, TITLE:            Curriculum Pre-training for End-to-End Speech Translation
https://www.aclweb.org/anthology/2020.acl-main.344
AUTHORS:               Chengyi Wang, Yu Wu, Shujie Liu, Ming Zhou, Zhenglu Yang
HIGHLIGHT:             Inspired by this, we propose a curriculum pre-training method that includes an elementary course for
transcription learning and two advanced courses for understanding the utterance and mapping words in two languages.

345, TITLE:            How Accents Confound: Probing for Accent Information in End-to-End Speech Recognition Systems
https://www.aclweb.org/anthology/2020.acl-main.345
AUTHORS:               Archiki Prasad, Preethi Jyothi
HIGHLIGHT:             In this work, we present a detailed analysis of how accent information is reflected in the internal representation
of speech in an end-to-end automatic speech recognition (ASR) system.

346, TITLE:            Improving Disfluency Detection by Self-Training a Self-Attentive Model
https://www.aclweb.org/anthology/2020.acl-main.346
AUTHORS:               Paria Jamshid Lou, Mark Johnson
HIGHLIGHT:             However, we show that self-training — a semi-supervised technique for incorporating unlabeled data — sets a
new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal
to the pre-trained contextualized word representations.

347, TITLE:            Learning Spoken Language Representations with Neural Lattice Language Modeling
https://www.aclweb.org/anthology/2020.acl-main.347
AUTHORS:               Chao-Wei Huang, Yun-Nung Chen
HIGHLIGHT:             We propose a framework that trains neural lattice language models to provide contextualized representations for
spoken language understanding tasks.

348, TITLE:            Meta-Transfer Learning for Code-Switched Speech Recognition
https://www.aclweb.org/anthology/2020.acl-main.348
AUTHORS:               Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, Peng Xu, Pascale Fung
HIGHLIGHT:             We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched
speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets.

349, TITLE:            Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association
https://www.aclweb.org/anthology/2020.acl-main.349
AUTHORS:               Nan Xu, Zhixiong Zeng, Wenji Mao
HIGHLIGHT:             To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-
modality contrast in the associated context.

350, TITLE:            SimulSpeech: End-to-End Simultaneous Speech to Text Translation
https://www.aclweb.org/anthology/2020.acl-main.350
AUTHORS:               Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao QIN, Zhou Zhao, Tie-Yan Liu
HIGHLIGHT:             In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which
translates speech in source language to text in target language concurrently.

351, TITLE:            Towards end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy
conversations
https://www.aclweb.org/anthology/2020.acl-main.351
AUTHORS:               Karan Singla, Zhuohao Chen, David Atkins, Shrikanth Narayanan
HIGHLIGHT:             We propose a novel framework for predicting utterance level labels directly from speech features, thus
removing the dependency on first generating transcripts, and transcription free behavioral coding.

352, TITLE:            Neural Temporal Opinion Modelling for Opinion Prediction on Twitter
https://www.aclweb.org/anthology/2020.acl-main.352
AUTHORS:               Lixing Zhu, Yulan He, Deyu Zhou
HIGHLIGHT:             In this paper, we model users' tweet posting behaviour as a temporal point process to jointly predict the posting
time and the stance label of the next tweet given a user's historical tweet sequence and tweets posted by their neighbours.

353, TITLE:            It Takes Two to Lie: One to Lie, and One to Listen
https://www.aclweb.org/anthology/2020.acl-main.353
AUTHORS:               Denis Peskov, Benny Cheng, Ahmed Elgohary, Joe Barrow, Cristian Danescu-Niculescu-Mizil, Jordan Boyd-
Graber
HIGHLIGHT:             We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven
players compete for world domination by forging and breaking alliances with each other.

354, TITLE:            Learning Implicit Text Generation via Feature Matching
https://www.aclweb.org/anthology/2020.acl-main.354
AUTHORS:               Inkit Padhi, Pierre Dognin, Ke Bai, C&iacute;cero Nogueira dos Santos, Vijil Chenthamarakshan, Youssef
Mroueh, Payel Das
HIGHLIGHT:             In this paper, we present new GFMN formulations that are effective for sequential data.

355, TITLE:              Two Birds, One Stone: A Simple, Unified Model for Text Generation from Structured and Unstructured Data
https://www.aclweb.org/anthology/2020.acl-main.355
AUTHORS:                 Hamidreza Shahidi, Ming Li, Jimmy Lin
HIGHLIGHT:               In this work, we show that this is also the case for text generation from structured and unstructured data.

356, TITLE:              Bayesian Hierarchical Words Representation Learning
https://www.aclweb.org/anthology/2020.acl-main.356
AUTHORS:                 Oren Barkan, Idan Rejwan, Avi Caciularu, Noam Koenigstein
HIGHLIGHT:               This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm.

357, TITLE:              Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning
https://www.aclweb.org/anthology/2020.acl-main.357
AUTHORS:                 Alexandre Tamborrino, Nicola Pellican&ograve;, Baptiste Pannier, Pascal Voitot, Louise Naudin
HIGHLIGHT:               In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and
leverages the masked language modeling head tuned during the pre-training phase.

358, TITLE:              SEEK: Segmented Embedding of Knowledge Graphs
https://www.aclweb.org/anthology/2020.acl-main.358
AUTHORS:                 Wentao Xu, Shun Zheng, Liang He, Bin Shao, Jian Yin, Tie-Yan Liu
HIGHLIGHT:               To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive
relational expressiveness without increasing the model complexity.

359, TITLE:              Selecting Backtranslated Data from Multiple Sources for Improved Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.359
AUTHORS:                 Xabier Soto, Dimitar Shterionov, Alberto Poncelas, Andy Way
HIGHLIGHT:               In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT
systems has on new MT systems.

360, TITLE:              Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture
https://www.aclweb.org/anthology/2020.acl-main.360
AUTHORS:                 Christopher Brix, Parnia Bahar, Hermann Ney
HIGHLIGHT:               On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show
that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new
combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity.

361, TITLE:              A Self-Training Method for Machine Reading Comprehension with Soft Evidence Extraction
https://www.aclweb.org/anthology/2020.acl-main.361
AUTHORS:                 Yilin Niu, Fangkai Jiao, Mantong Zhou, Ting Yao, jingfang xu, Minlie Huang
HIGHLIGHT:               To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor
with auto-generated evidence labels in an iterative process.

362, TITLE:              Graph-to-Tree Learning for Solving Math Word Problems
https://www.aclweb.org/anthology/2020.acl-main.362
AUTHORS:                 Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, Ee-Peng Lim
HIGHLIGHT:               In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-
based encoder and tree-based decoder to generate better solution expressions.

363, TITLE:              An Effectiveness Metric for Ordinal Classification: Formal Properties and Experimental Results
https://www.aclweb.org/anthology/2020.acl-main.363
AUTHORS:                 Enrique Amigo, Julio Gonzalo, Stefano Mizzaro, Jorge Carrillo-de-Albornoz
HIGHLIGHT:               In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted
on Measurement Theory and Information Theory.

364, TITLE:              Adaptive Compression of Word Embeddings
https://www.aclweb.org/anthology/2020.acl-main.364
AUTHORS:                 Yeachan Kim, Kang-Min Kim, SangKeun Lee
HIGHLIGHT:               In this paper, we propose a novel method to adaptively compress word embeddings.

365, TITLE:              Analysing Lexical Semantic Change with Contextualised Word Representations
https://www.aclweb.org/anthology/2020.acl-main.365
AUTHORS:                 Mario Giulianelli, Marco Del Tredici, Raquel Fern&aacute;ndez
HIGHLIGHT:               This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised
word representations.

366, TITLE:              Autoencoding Keyword Correlation Graph for Document Clustering
https://www.aclweb.org/anthology/2020.acl-main.366
AUTHORS:                 Billy Chiu, Sunil Kumar Sahu, Derek Thomas, Neha Sengupta, Mohammady Mahdy
HIGHLIGHT:               To address this, we present a novel graph-based representation for document clustering that builds a
\textit{graph autoencoder} (GAE) on a Keyword Correlation Graph.

367, TITLE:              Autoencoding Pixies: Amortised Variational Inference with Graph Convolutions for Functional Distributional
Semantics
https://www.aclweb.org/anthology/2020.acl-main.367
AUTHORS:                 Guy Emerson
HIGHLIGHT:               In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional
Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference.

368, TITLE:              BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance
https://www.aclweb.org/anthology/2020.acl-main.368
AUTHORS:                 Timo Schick, Hinrich Sch&uuml;tze
HIGHLIGHT:               In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful
architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input
representations for deep language models.

369, TITLE:              CluBERT: A Cluster-Based Approach for Learning Sense Distributions in Multiple Languages
https://www.aclweb.org/anthology/2020.acl-main.369
AUTHORS:                 Tommaso Pasini, Federico Scozzafava, Bianca Scarlini
HIGHLIGHT:               To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing
the distributions of word senses from a corpus of raw sentences.

370, TITLE:              Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis
https://www.aclweb.org/anthology/2020.acl-main.370
AUTHORS:                 Chunning Du, Haifeng Sun, Jingyu Wang, Qi Qi, Jianxin Liao
HIGHLIGHT:               In this paper, we investigate how to efficiently apply the pre-training language model BERT on the
unsupervised domain adaptation.

371, TITLE:              From Arguments to Key Points: Towards Automatic Argument Summarization
https://www.aclweb.org/anthology/2020.acl-main.371
AUTHORS:                 Roy Bar-Haim, Lilach Eden, Roni Friedman, Yoav Kantor, Dan Lahav, Noam Slonim
HIGHLIGHT:               We propose to represent such summaries as a small set of talking points, termed \textit{key points}, each scored
according to its salience.
We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task.

372, TITLE:              GoEmotions: A Dataset of Fine-Grained Emotions
https://www.aclweb.org/anthology/2020.acl-main.372
AUTHORS:                 Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, Sujith Ravi
HIGHLIGHT:               We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for
27 emotion categories or Neutral.

373, TITLE:              He said ``who's gonna take care of your children when you are at ACL?'': Reported Sexist Acts are Not Sexist
https://www.aclweb.org/anthology/2020.acl-main.373
AUTHORS:                 Patricia Chiril, V&eacute;ronique MORICEAU, Farah Benamara, Alda Mari, Gloria Origgi, Marl&egrave;ne
Coulomb-Gully
HIGHLIGHT:               We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis
studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a
combination of several tweet's vectorial representations (word embeddings, linguistic features, and various generalization strategies).

374, TITLE:              SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis
https://www.aclweb.org/anthology/2020.acl-main.374
AUTHORS:                 Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng Wang, feng wu
HIGHLIGHT:            In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified
sentiment representation for multiple sentiment analysis tasks.

375, TITLE:           Do Neural Language Models Show Preferences for Syntactic Formalisms?
https://www.aclweb.org/anthology/2020.acl-main.375
AUTHORS:              Artur Kulmizev, Vinit Ravishankar, Mostafa Abdou, Joakim Nivre
HIGHLIGHT:            In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by
language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across
different languages.

376, TITLE:           Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing
https://www.aclweb.org/anthology/2020.acl-main.376
AUTHORS:              Daniel Fern&aacute;ndez-Gonz&aacute;lez, Carlos G&oacute;mez-Rodr&iacute;guez
HIGHLIGHT:            In this paper, we show that these results can be improved by using an in-order linearization instead.

377, TITLE:           Exact yet Efficient Graph Parsing, Bi-directional Locality and the Constructivist Hypothesis
https://www.aclweb.org/anthology/2020.acl-main.377
AUTHORS:              Yajie Ye, Weiwei Sun
HIGHLIGHT:            We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large
Hyperedge Replacement Grammars (HRGs).

378, TITLE:           Max-Margin Incremental CCG Parsing
https://www.aclweb.org/anthology/2020.acl-main.378
AUTHORS:              MiloÅ¡ StanojeviÄ‡, Mark Steedman
HIGHLIGHT:            Instead, we tackle all of these three biases at the same time using an improved version of beam search
optimisation that minimises all beam search violations instead of minimising only the biggest violation.

379, TITLE:           Neural Reranking for Dependency Parsing: An Evaluation
https://www.aclweb.org/anthology/2020.acl-main.379
AUTHORS:              Bich-Ngoc Do, Ines Rehbein
HIGHLIGHT:            In the paper, we re-assess the potential of successful neural reranking models from the literature on English and
on two morphologically rich(er) languages, German and Czech.

380, TITLE:           Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimination in Text Classifications with
Instance Weighting
https://www.aclweb.org/anthology/2020.acl-main.380
AUTHORS:              Guanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Conghui Zhu, Tiejun Zhao
HIGHLIGHT:            In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from
the non-discrimination distribution to the discrimination distribution.

381, TITLE:           Analyzing analytical methods: The case of phonology in neural models of spoken language
https://www.aclweb.org/anthology/2020.acl-main.381
AUTHORS:              Grzegorz ChrupaÅ‚a, Bertrand Higy, Afra Alishahi
HIGHLIGHT:            As a step in this direction we study the case of representations of phonology in neural network models of
spoken language.

382, TITLE:           Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations
https://www.aclweb.org/anthology/2020.acl-main.382
AUTHORS:              Oana-Maria Camburu, Brendan Shillingford, Pasquale Minervini, Thomas Lukasiewicz, Phil Blunsom
HIGHLIGHT:            In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations,
such as "Because there is a dog in the image."

383, TITLE:           Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT
https://www.aclweb.org/anthology/2020.acl-main.383
AUTHORS:              Zhiyong Wu, Yun Chen, Ben Kao, Qun Liu
HIGHLIGHT:            Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained
language models (e.g., BERT).

384, TITLE:           Probing for Referential Information in Language Models
https://www.aclweb.org/anthology/2020.acl-main.384
AUTHORS:               Ionut-Teodor Sorodoc, Kristina Gulordava, Gemma Boleda
HIGHLIGHT:             We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis
on a coreference annotated corpus.

385, TITLE:            Quantifying Attention Flow in Transformers
https://www.aclweb.org/anthology/2020.acl-main.385
AUTHORS:               Samira Abnar, Willem Zuidema
HIGHLIGHT:             In this paper, we consider the problem of quantifying this flow of information through self-attention.

386, TITLE:            Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?
https://www.aclweb.org/anthology/2020.acl-main.386
AUTHORS:               Alon Jacovi, Yoav Goldberg
HIGHLIGHT:             We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and
focus on the faithfulness criteria.

387, TITLE:            Towards Transparent and Explainable Attention Models
https://www.aclweb.org/anthology/2020.acl-main.387
AUTHORS:               Akash Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, Mitesh M. Khapra, Balaji Vasan Srinivasan,
Balaraman Ravindran
HIGHLIGHT:             To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-
driven training objective that ensures that the hidden representations learned at different time steps are diverse.

388, TITLE:            Tchebycheff Procedure for Multi-task Text Classification
https://www.aclweb.org/anthology/2020.acl-main.388
AUTHORS:               Yuren Mao, Shuang Yun, Weiwei Liu, Bo Du
HIGHLIGHT:             To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task
classification problems without convex assumption.

389, TITLE:            Modeling Word Formation in English--German Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.389
AUTHORS:               Marion Weller-Di Marco, Alexander Fraser
HIGHLIGHT:             This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word
segmentation approach that goes beyond splitting into substrings by considering fusional morphology.

390, TITLE:            Empowering Active Learning to Jointly Optimize System and User Demands
https://www.aclweb.org/anthology/2020.acl-main.390
AUTHORS:               Ji-Ung Lee, Christian M. Meyer, Iryna Gurevych
HIGHLIGHT:             In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting
objectives of the active learning system (training efficiently) and the user (receiving useful instances).

391, TITLE:            Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error
Correction
https://www.aclweb.org/anthology/2020.acl-main.391
AUTHORS:               Masahiro Kaneko, Masato Mita, Shun Kiyono, Jun Suzuki, Kentaro Inui
HIGHLIGHT:             This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as
BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC).

392, TITLE:            Graph Neural News Recommendation with Unsupervised Preference Disentanglement
https://www.aclweb.org/anthology/2020.acl-main.392
AUTHORS:               Linmei Hu, Siyong Xu, Chen Li, Cheng Yang, Chuan Shi, Nan Duan, Xing Xie, Ming Zhou
HIGHLIGHT:             In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News
Recommendation model with Unsupervised Preference Disentanglement, named GNUD.

393, TITLE:            Identifying Principals and Accessories in a Complex Case based on the Comprehension of Fact Description
https://www.aclweb.org/anthology/2020.acl-main.393
AUTHORS:               Yakun Hu, Zhunchen Luo, Wenhan Chao
HIGHLIGHT:             In this paper, we study the problem of identifying the principals and accessories from the fact description with
multiple defendants in a criminal case.

394, TITLE:            Joint Modelling of Emotion and Abusive Language Detection
https://www.aclweb.org/anthology/2020.acl-main.394
AUTHORS:               Santhosh Rajamanickam, Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova
HIGHLIGHT:             In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a
multi-task learning framework that allows one task to inform the other.

395, TITLE:            Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep
Natural Language Understanding
https://www.aclweb.org/anthology/2020.acl-main.395
AUTHORS:               Sebastian Weigelt, Vanessa Steurer, Tobias Hey, Walter F. Tichy
HIGHLIGHT:             We examine how to teach intelligent systems new functions, expressed in natural language.

396, TITLE:            Toxicity Detection: Does Context Really Matter?
https://www.aclweb.org/anthology/2020.acl-main.396
AUTHORS:               John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Nithum Thain, Ion Androutsopoulos
HIGHLIGHT:             We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and
(b) does conditioning on context improve performance of toxicity detection systems?

397, TITLE:            AMR Parsing with Latent Structural Information
https://www.aclweb.org/anthology/2020.acl-main.397
AUTHORS:               Qiji Zhou, Yue Zhang, Donghong Ji, Hao Tang
HIGHLIGHT:             We investigate parsing AMR with explicit dependency structures and interpretable latent structures.

398, TITLE:            TaPas: Weakly Supervised Table Parsing via Pre-training
https://www.aclweb.org/anthology/2020.acl-main.398
AUTHORS:               Jonathan Herzig, Pawel Krzysztof Nowak, Thomas M&uuml;ller, Francesco Piccinno, Julian Eisenschlos
HIGHLIGHT:             In this paper, we present TaPas, an approach to question answering over tables without generating logical
forms.

399, TITLE:            Target Inference in Argument Conclusion Generation
https://www.aclweb.org/anthology/2020.acl-main.399
AUTHORS:               Milad Alshomary, Shahbaz Syed, Martin Potthast, Henning Wachsmuth
HIGHLIGHT:             We develop two complementary target inference approaches: one ranks premise targets and selects the top-
ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural
network.

400, TITLE:            Multimodal Transformer for Multimodal Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.400
AUTHORS:               Shaowei Yao, Xiaojun Wan
HIGHLIGHT:             In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT.

401, TITLE:            Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment
and Emotion Analysis
https://www.aclweb.org/anthology/2020.acl-main.401
AUTHORS:               Dushyant Singh Chauhan, Dhanush S R, Asif Ekbal, Pushpak Bhattacharyya
HIGHLIGHT:             In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a
multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario.

402, TITLE:            Towards Emotion-aided Multi-modal Dialogue Act Classification
https://www.aclweb.org/anthology/2020.acl-main.402
AUTHORS:               Tulika Saha, Aditya Patra, Sriparna Saha, Pushpak Bhattacharyya
HIGHLIGHT:             In this work, we address the role of \textit{both} multi-modality and emotion recognition (ER) in DAC.

403, TITLE:            Analyzing Political Parody in Social Media
https://www.aclweb.org/anthology/2020.acl-main.403
AUTHORS:               Antonios Maronikolakis, Danae S&aacute;nchez Villegas, Daniel Preotiuc-Pietro, Nikolaos Aletras
HIGHLIGHT:             In this paper, we present the first computational study of parody.

404, TITLE:            Masking Actor Information Leads to Fairer Political Claims Detection
https://www.aclweb.org/anthology/2020.acl-main.404
AUTHORS:               Erenay Dayanik, Sebastian Pad&oacute;
HIGHLIGHT:              We propose two simple debiasing methods which mask proper names and pronouns during training of the
model, thus removing personal information bias.

405, TITLE:             When do Word Embeddings Accurately Reflect Surveys on our Beliefs About People?
https://www.aclweb.org/anthology/2020.acl-main.405
AUTHORS:                Kenneth Joseph, Jonathan Morgan
HIGHLIGHT:              Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about
certain kinds of people as measured via traditional survey methods.

406, TITLE:             ``Who said it, and Why?'' Provenance for Natural Language Claims
https://www.aclweb.org/anthology/2020.acl-main.406
AUTHORS:                Yi Zhang, Zachary Ives, Dan Roth
HIGHLIGHT:              This paper suggests that the key to a longer-term, holistic, and systematic approach to navigating this
information pollution is capturing the provenance of claims.

407, TITLE:             Compositionality and Generalization In Emergent Languages
https://www.aclweb.org/anthology/2020.acl-main.407
AUTHORS:                Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, Marco Baroni
HIGHLIGHT:              In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar
ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language
compositionality.

408, TITLE:             ERASER: A Benchmark to Evaluate Rationalized NLP Models
https://www.aclweb.org/anthology/2020.acl-main.408
AUTHORS:                Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C.
Wallace
HIGHLIGHT:              We propose the \textbf{E}valuating \textbf{R}ationales \textbf{A}nd \textbf{S}imple \textbf{E}nglish
\textbf{R}easoning (\textbf{ERASER} a benchmark to advance research on interpretable models in NLP.

409, TITLE:             Learning to Faithfully Rationalize by Construction
https://www.aclweb.org/anthology/2020.acl-main.409
AUTHORS:                Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C. Wallace
HIGHLIGHT:              We propose a simpler variant of this approach that provides faithful explanations by construction. In our
scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels
over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on
snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex.

410, TITLE:             Clinical Reading Comprehension: A Thorough Analysis of the emrQA Dataset
https://www.aclweb.org/anthology/2020.acl-main.410
AUTHORS:                Xiang Yue, Bernal Jimenez Gutierrez, Huan Sun
HIGHLIGHT:              In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC)
task.

411, TITLE:             DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering
https://www.aclweb.org/anthology/2020.acl-main.411
AUTHORS:                Qingqing Cao, Harsh Trivedi, Aruna Balasubramanian, Niranjan Balasubramanian
HIGHLIGHT:              We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide
and passage-wide self-attentions in the lower layers.

412, TITLE:             Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings
https://www.aclweb.org/anthology/2020.acl-main.412
AUTHORS:                Apoorv Saxena, Aditay Tripathi, Partha Talukdar
HIGHLIGHT:              We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing
multi-hop KGQA over sparse KGs.

413, TITLE:             Template-Based Question Generation from Retrieved Sentences for Improved Unsupervised Question
Answering
https://www.aclweb.org/anthology/2020.acl-main.413
AUTHORS:                Alexander Fabbri, Patrick Ng, Zhiguo Wang, Ramesh Nallapati, Bing Xiang
HIGHLIGHT:              We propose an unsupervised approach to training QA models with generated pseudo-training data.

414, TITLE:              Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop Question Answering
https://www.aclweb.org/anthology/2020.acl-main.414
AUTHORS:                 Vikas Yadav, Steven Bethard, Mihai Surdeanu
HIGHLIGHT:               We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas:
(a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe
embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which
(c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications.

415, TITLE:              A Corpus for Large-Scale Phonetic Typology
https://www.aclweb.org/anthology/2020.acl-main.415
AUTHORS:                 Elizabeth Salesky, Eleanor Chodroff, Tiago Pimentel, Matthew Wiesner, Ryan Cotterell, Alan W Black, Jason
Eisner
HIGHLIGHT:               We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and
estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and
sibilants.

416, TITLE:              Dscorer: A Fast Evaluation Metric for Discourse Representation Structure Parsing
https://www.aclweb.org/anthology/2020.acl-main.416
AUTHORS:                 Jiangming Liu, Shay B. Cohen, Mirella Lapata
HIGHLIGHT:               We introduce Dscorer, an efficient new metric which converts box-style DRSs to graphs and then measures the
overlap of n-grams.

417, TITLE:              ParaCrawl: Web-Scale Acquisition of Parallel Corpora
https://www.aclweb.org/anthology/2020.acl-main.417
AUTHORS:                 Marta Ba&ntilde;&oacute;n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel
Espl&agrave;-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere,
Gema Ram&iacute;rez-S&aacute;nchez, Elsa Sarr&iacute;as, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, Jaume
Zaragoza
HIGHLIGHT:               We report on methods to create the largest publicly available parallel corpora by crawling the web, using open
source software.

418, TITLE:              Toward Gender-Inclusive Coreference Resolution
https://www.aclweb.org/anthology/2020.acl-main.418
AUTHORS:                 Yang Trista Cao, Hal Daum&eacute; III
HIGHLIGHT:               Through these studies, conducted on English text, we confirm that without acknowledging and building systems
that recognize the complexity of gender, we build systems that lead to many potential harms.

419, TITLE:              Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?
https://www.aclweb.org/anthology/2020.acl-main.419
AUTHORS:                 Cansu Sen, Thomas Hartvigsen, Biao Yin, Xiangnan Kong, Elke Rundensteiner
HIGHLIGHT:               In this work, we conduct the first quantitative assessment of human versus computational attention mechanisms
for the text classification task.

420, TITLE:              Information-Theoretic Probing for Linguistic Structure
https://www.aclweb.org/anthology/2020.acl-main.420
AUTHORS:                 Tiago Pimentel, Josef Valvoda, Rowan Hall Maudslay, Ran Zmigrod, Adina Williams, Ryan Cotterell
HIGHLIGHT:               We propose an information-theoretic operationalization of probing as estimating mutual information that
contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it
will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation.

421, TITLE:              On the Cross-lingual Transferability of Monolingual Representations
https://www.aclweb.org/anthology/2020.acl-main.421
AUTHORS:                 Mikel Artetxe, Sebastian Ruder, Dani Yogatama
HIGHLIGHT:               More concretely, we first train a transformer-based masked language model on one language, and transfer it to a
new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all
other layers.

422, TITLE:              Similarity Analysis of Contextual Word Representation Models
https://www.aclweb.org/anthology/2020.acl-main.422
AUTHORS:                 John Wu, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, James Glass
HIGHLIGHT:               This paper investigates contextual word representation models from the lens of similarity analysis.

423, TITLE:              SenseBERT: Driving Some Sense into BERT
https://www.aclweb.org/anthology/2020.acl-main.423
AUTHORS:                 Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua,
Yoav Shoham
HIGHLIGHT:               This paper proposes a method to employ weak-supervision directly at the word sense level.

424, TITLE:              ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting
Transformations
https://www.aclweb.org/anthology/2020.acl-main.424
AUTHORS:                 Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Beno&icirc;t Sagot, Lucia Specia
HIGHLIGHT:               To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in
English.

425, TITLE:              Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts
https://www.aclweb.org/anthology/2020.acl-main.425
AUTHORS:                 Agostina Calabrese, Michele Bevilacqua, Roberto Navigli
HIGHLIGHT:               We fill this gap by presenting BabelPic, a hand-labeled dataset built by cleaning the image-synset association
found within the BabelNet Lexical Knowledge Base (LKB).

426, TITLE:              Modeling Label Semantics for Predicting Emotional Reactions
https://www.aclweb.org/anthology/2020.acl-main.426
AUTHORS:                 Radhika Gaonkar, Heeyoung Kwon, Mohaddeseh Bastan, Niranjan Balasubramanian, Nathanael Chambers
HIGHLIGHT:               In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label
correlations both during training and inference.

427, TITLE:              CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant
https://www.aclweb.org/anthology/2020.acl-main.427
AUTHORS:                 Kavya Srinet, Yacine Jernite, Jonathan Gray, arthur szlam
HIGHLIGHT:               We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game
Minecraft.

428, TITLE:              Don't Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training
https://www.aclweb.org/anthology/2020.acl-main.428
AUTHORS:                 Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho, Jason Weston
HIGHLIGHT:               We show that appropriate loss functions which regularize generated outputs to match human distributions are
effective for the first three issues. For the last important general issue, we show applying unlikelihood to collected data of what a
model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater
reasoning ability.

429, TITLE:              How does BERT's attention change when you fine-tune? An analysis methodology and a case study in negation
scope
https://www.aclweb.org/anthology/2020.acl-main.429
AUTHORS:                 Yiyun Zhao, Steven Bethard
HIGHLIGHT:               We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might
encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related
downstream tasks with downstream control tasks, and measurement of cross-dataset consistency.

430, TITLE:              Influence Paths for Characterizing Subject-Verb Number Agreement in LSTM Language Models
https://www.aclweb.org/anthology/2020.acl-main.430
AUTHORS:                 Kaiji Lu, Piotr Mardziel, Klas Leino, Matt Fredrikson, Anupam Datta
HIGHLIGHT:               We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and
neurons of a recurrent neural network.

431, TITLE:              Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings
https://www.aclweb.org/anthology/2020.acl-main.431
AUTHORS:                 Rishi Bommasani, Kelly Davis, Claire Cardie
HIGHLIGHT:               Consequently, we introduce simple and fully general methods for converting from contextualized
representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights.

432, TITLE:            Learning to Deceive with Attention-Based Explanations
https://www.aclweb.org/anthology/2020.acl-main.432
AUTHORS:               Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary C. Lipton
HIGHLIGHT:             We call the latter use of attention mechanisms into question by demonstrating a simple method for training
models to produce deceptive attention masks.

433, TITLE:            On the Spontaneous Emergence of Discrete and Compositional Signals
https://www.aclweb.org/anthology/2020.acl-main.433
AUTHORS:               Nur Geffen Lan, Emmanuel Chemla, Shane Steinert-Threlkeld
HIGHLIGHT:             We propose a general framework to study language emergence through signaling games with neural agents.

434, TITLE:            Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding
Words
https://www.aclweb.org/anthology/2020.acl-main.434
AUTHORS:               Josef Klafka, Allyson Ettinger
HIGHLIGHT:             To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual
embeddings for encoding of information about surrounding words.

435, TITLE:            Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA
https://www.aclweb.org/anthology/2020.acl-main.435
AUTHORS:               Hyounghun Kim, Zineng Tang, Mohit Bansal
HIGHLIGHT:             In this paper, we propose a video question answering model which effectively integrates multi-modal input
sources and finds the temporally relevant information to answer questions.

436, TITLE:            Shaping Visual Representations with Language for Few-Shot Classification
https://www.aclweb.org/anthology/2020.acl-main.436
AUTHORS:               Jesse Mu, Percy Liang, Noah Goodman
HIGHLIGHT:             Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual
representations to predict language.

437, TITLE:            Discrete Latent Variable Representations for Low-Resource Text Classification
https://www.aclweb.org/anthology/2020.acl-main.437
AUTHORS:               Shuning Jin, Sam Wiseman, Karl Stratos, Karen Livescu
HIGHLIGHT:             We consider several approaches to learning discrete latent variable models for text in the case where exact
marginalization over these variables is intractable.

438, TITLE:            Learning Constraints for Structured Prediction Using Rectifier Networks
https://www.aclweb.org/anthology/2020.acl-main.438
AUTHORS:               Xingyuan Pan, Maitrey Mehta, Vivek Srikumar
HIGHLIGHT:             We frame the problem as that of training a two-layer rectifier network to identify valid structures or
substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables.

439, TITLE:            Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models
https://www.aclweb.org/anthology/2020.acl-main.439
AUTHORS:               Dan Iter, Kelvin Guu, Larry Lansing, Dan Jurafsky
HIGHLIGHT:             We propose Conpono, an inter-sentence objective for pretraining language models that models discourse
coherence and the distance between sentences.

440, TITLE:            A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks
https://www.aclweb.org/anthology/2020.acl-main.440
AUTHORS:               Angela Lin, Sudha Rao, Asli Celikyilmaz, Elnaz Nouri, Chris Brockett, Debadeepta Dey, Bill Dolan
HIGHLIGHT:             To address these challenges, we use an unsupervised alignment algorithm that learns pairwise alignments
between instructions of different recipes for the same dish.

441, TITLE:            Adversarial NLI: A New Benchmark for Natural Language Understanding
https://www.aclweb.org/anthology/2020.acl-main.441
AUTHORS:               Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, Douwe Kiela
HIGHLIGHT:             We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-
model-in-the-loop procedure.

442, TITLE:            Beyond Accuracy: Behavioral Testing of NLP Models with CheckList
https://www.aclweb.org/anthology/2020.acl-main.442
AUTHORS:               Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, Sameer Singh
HIGHLIGHT:             Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic
methodology for testing NLP models.

443, TITLE:            Code and Named Entity Recognition in StackOverflow
https://www.aclweb.org/anthology/2020.acl-main.443
AUTHORS:               Jeniya Tabassum, Mounica Maddela, Wei Xu, Alan Ritter
HIGHLIGHT:             In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming
domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types.

444, TITLE:            Dialogue-Based Relation Extraction
https://www.aclweb.org/anthology/2020.acl-main.444
AUTHORS:               Dian Yu, Kai Sun, Claire Cardie, Dong Yu
HIGHLIGHT:             We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to
support the prediction of relation(s) between two arguments that appear in a dialogue.

445, TITLE:            Facet-Aware Evaluation for Extractive Summarization
https://www.aclweb.org/anthology/2020.acl-main.445
AUTHORS:               Yuning Mao, Liyuan Liu, Qi Zhu, Xiang Ren, Jiawei Han
HIGHLIGHT:             In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in
extracted summaries.

446, TITLE:            More Diverse Dialogue Datasets via Diversity-Informed Data Collection
https://www.aclweb.org/anthology/2020.acl-main.446
AUTHORS:               Katherine Stasaski, Grace Hui Yang, Marti A. Hearst
HIGHLIGHT:             We introduce a new strategy to address this problem, called Diversity-Informed Data Collection.

447, TITLE:            S2ORC: The Semantic Scholar Open Research Corpus
https://www.aclweb.org/anthology/2020.acl-main.447
AUTHORS:               Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld
HIGHLIGHT:             We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic
disciplines.

448, TITLE:            Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics
https://www.aclweb.org/anthology/2020.acl-main.448
AUTHORS:               Nitika Mathur, Timothy Baldwin, Trevor Cohn
HIGHLIGHT:             We show that current methods for judging metrics are highly sensitive to the translations used for assessment,
particularly the presence of outliers, which often leads to falsely confident conclusions about a metric's efficacy.

449, TITLE:            A Transformer-based Approach for Source Code Summarization
https://www.aclweb.org/anthology/2020.acl-main.449
AUTHORS:               Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang
HIGHLIGHT:             To learn code representation for summarization, we explore the Transformer model that uses a self-attention
mechanism and has shown to be effective in capturing long-range dependencies.

450, TITLE:            Asking and Answering Questions to Evaluate the Factual Consistency of Summaries
https://www.aclweb.org/anthology/2020.acl-main.450
AUTHORS:               Alex Wang, Kyunghyun Cho, Mike Lewis
HIGHLIGHT:             We propose QAGS (pronounced "kags"), an automatic evaluation protocol that is designed to identify factual
inconsistencies in a generated summary.

451, TITLE:            Discourse-Aware Neural Extractive Text Summarization
https://www.aclweb.org/anthology/2020.acl-main.451
AUTHORS:               Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu
HIGHLIGHT:             To address these issues, we present a discourse-aware neural summarization model - DiscoBert.

452, TITLE:            Discrete Optimization for Unsupervised Sentence Summarization with Word-Level Extraction
https://www.aclweb.org/anthology/2020.acl-main.452
AUTHORS:                Raphael Schumann, Lili Mou, Yao Lu, Olga Vechtomova, Katja Markert
HIGHLIGHT:              A good summary is characterized by language fluency and high information overlap with the source sentence.
We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics.

453, TITLE:             Exploring Content Selection in Summarization of Novel Chapters
https://www.aclweb.org/anthology/2020.acl-main.453
AUTHORS:                Faisal Ladhak, Bryan Li, Yaser Al-Onaizan, Kathleen McKeown
HIGHLIGHT:              We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs
from online study guides.

454, TITLE:             FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive
Summarization
https://www.aclweb.org/anthology/2020.acl-main.454
AUTHORS:                Esin Durmus, He He, Mona Diab
HIGHLIGHT:              We tackle the problem of evaluating faithfulness of a generated summary given its source document.

455, TITLE:             Fact-based Content Weighting for Evaluating Abstractive Summarisation
https://www.aclweb.org/anthology/2020.acl-main.455
AUTHORS:                Xinnuo Xu, OndÅ™ej DuÅ¡ek, Jingyi Li, Verena Rieser, Ioannis Konstas
HIGHLIGHT:              We introduce a new evaluation metric which is based on fact-level content weighting, i.e. relating the facts of
the document to the facts of the summary.

456, TITLE:             Hooks in the Headline: Learning to Generate Headlines with Controlled Styles
https://www.aclweb.org/anthology/2020.acl-main.456
AUTHORS:                Di Jin, Zhijing Jin, Joey Tianyi Zhou, Lisa Orii, Peter Szolovits
HIGHLIGHT:              We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options
(humor, romance and clickbait), thus attracting more readers.

457, TITLE:             Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward
https://www.aclweb.org/anthology/2020.acl-main.457
AUTHORS:                Luyang Huang, Lingfei Wu, Lu Wang
HIGHLIGHT:              In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-
Augmentation and semantic-driven RewarD.

458, TITLE:             Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports
https://www.aclweb.org/anthology/2020.acl-main.458
AUTHORS:                Yuhao Zhang, Derek Merck, Emily Tsai, Christopher D. Manning, Curtis Langlotz
HIGHLIGHT:              In this work, we develop a general framework where we evaluate the factual correctness of a generated
summary by fact-checking it automatically against its reference using an information extraction module.

459, TITLE:             Storytelling with Dialogue: A Critical Role Dungeons and Dragons Dataset
https://www.aclweb.org/anthology/2020.acl-main.459
AUTHORS:                Revanth Rameshkumar, Peter Bailey
HIGHLIGHT:              This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses.

460, TITLE:             The Summary Loop: Learning to Write Abstractive Summaries Without Examples
https://www.aclweb.org/anthology/2020.acl-main.460
AUTHORS:                Philippe Laban, Andrew Hsi, John Canny, Marti A. Hearst
HIGHLIGHT:              This work presents a new approach to unsupervised abstractive summarization based on maximizing a
combination of coverage and fluency for a given length constraint.

461, TITLE:             Unsupervised Opinion Summarization as Copycat-Review Generation
https://www.aclweb.org/anthology/2020.acl-main.461
AUTHORS:                Arthur BraÅ¾inskas, Mirella Lapata, Ivan Titov
HIGHLIGHT:              We define a generative model for a review collection which capitalizes on the intuition that when generating a
new review given a set of other reviews of a product, we should be able to control the "amount of novelty" going into the new review
or, equivalently, vary the extent to which it deviates from the input.

462, TITLE:             (Re)construing Meaning in NLP
https://www.aclweb.org/anthology/2020.acl-main.462
AUTHORS:              Sean Trott, Tiago Timponi Torrent, Nancy Chang, Nathan Schneider
HIGHLIGHT:            In this paper, we engage with an idea largely absent from discussions of meaning in natural language
understanding-namely, that the way something is expressed reflects different ways of conceptualizing or construing the information
being conveyed.

463, TITLE:           Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data
https://www.aclweb.org/anthology/2020.acl-main.463
AUTHORS:              Emily M. Bender, Alexander Koller
HIGHLIGHT:            In this position paper, we argue that a system trained only on form has a priori no way to learn meaning.

464, TITLE:           Examining Citations of Natural Language Processing Literature
https://www.aclweb.org/anthology/2020.acl-main.464
AUTHORS:              Saif M. Mohammad
HIGHLIGHT:            We extracted information from the ACL Anthology (AA) and Google Scholar (GS) to examine trends in
citations of NLP papers.

465, TITLE:           How Can We Accelerate Progress Towards Human-like Linguistic Generalization?
https://www.aclweb.org/anthology/2020.acl-main.465
AUTHORS:              Tal Linzen
HIGHLIGHT:            This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation
paradigm, which has become a central tool for measuring progress in natural language understanding.

466, TITLE:           How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence
https://www.aclweb.org/anthology/2020.acl-main.466
AUTHORS:              Haoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, Maosong Sun
HIGHLIGHT:            In this paper, we introduce the history, the current state, and the future directions of research in LegalAI.

467, TITLE:           Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?
https://www.aclweb.org/anthology/2020.acl-main.467
AUTHORS:              Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara
Vania, Katharina Kann, Samuel R. Bowman
HIGHLIGHT:            To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-
target task combinations.

468, TITLE:           Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview
https://www.aclweb.org/anthology/2020.acl-main.468
AUTHORS:              Deven Santosh Shah, H. Andrew Schwartz, Dirk Hovy
HIGHLIGHT:            In this paper, we propose a unifying predictive bias framework for NLP.

469, TITLE:           What Does BERT with Vision Look At?
https://www.aclweb.org/anthology/2020.acl-main.469
AUTHORS:              Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang
HIGHLIGHT:            In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground
elements of language to image regions.

470, TITLE:           Balancing Objectives in Counseling Conversations: Advancing Forwards or Looking Backwards
https://www.aclweb.org/anthology/2020.acl-main.470
AUTHORS:              Justine Zhang, Cristian Danescu-Niculescu-Mizil
HIGHLIGHT:            In this work, we develop an unsupervised methodology to quantify how counselors manage this balance.

471, TITLE:           Detecting Perceived Emotions in Hurricane Disasters
https://www.aclweb.org/anthology/2020.acl-main.471
AUTHORS:              Shrey Desai, Cornelia Caragea, Junyi Jessy Li
HIGHLIGHT:            In this paper, we introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning three
hurricanes: Harvey, Irma, and Maria.

472, TITLE:           Hierarchical Modeling for User Personality Prediction: The Role of Message-Level Attention
https://www.aclweb.org/anthology/2020.acl-main.472
AUTHORS:              Veronica Lynn, Niranjan Balasubramanian, H. Andrew Schwartz
HIGHLIGHT:              In this paper, we present a novel model that uses message-level attention to learn the relative weight of users'
social media posts for assessing their five factor personality traits.

473, TITLE:             Measuring Forecasting Skill from Text
https://www.aclweb.org/anthology/2020.acl-main.473
AUTHORS:                Shi Zong, Alan Ritter, Eduard Hovy
HIGHLIGHT:              In this paper we explore connections between the language people use to describe their predictions and their
forecasting skill.

474, TITLE:             Text and Causal Inference: A Review of Using Text to Remove Confounding from Causal Estimates
https://www.aclweb.org/anthology/2020.acl-main.474
AUTHORS:                Katherine Keith, David Jensen, Brendan Oâ€™Connor
HIGHLIGHT:              Despite increased attention on adjusting for confounding using text, there are still many open problems, which
we highlight in this paper.

475, TITLE:             Text-Based Ideal Points
https://www.aclweb.org/anthology/2020.acl-main.475
AUTHORS:                Keyon Vafa, Suresh Naidu, David Blei
HIGHLIGHT:              In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model
that analyzes texts to quantify the political positions of its authors.

476, TITLE:             Understanding the Language of Political Agreement and Disagreement in Legislative Texts
https://www.aclweb.org/anthology/2020.acl-main.476
AUTHORS:                Maryam Davoodi, Eric Waltenburg, Dan Goldwasser
HIGHLIGHT:              In this paper, we take the first step towards a better understanding of these processes and the underlying
dynamics that shape them, using data-driven methods.

477, TITLE:             Would you Rather? A New Benchmark for Learning Machine Alignment with Cultural Values and Social
Preferences
https://www.aclweb.org/anthology/2020.acl-main.477
AUTHORS:                Yi Tay, Donovan Ong, Jie Fu, Alvin Chan, Nancy Chen, Anh Tuan Luu, Chris Pal
HIGHLIGHT:              Concretely, we present a new task and corpus for learning alignments between machine and human preferences.

478, TITLE:             Discourse as a Function of Event: Profiling Discourse Structure in News Articles around the Main Event
https://www.aclweb.org/anthology/2020.acl-main.478
AUTHORS:                Prafulla Kumar Choubey, Aaron Lee, Ruihong Huang, Lu Wang
HIGHLIGHT:              To enable computational modeling of news structures, we apply an existing theory of functional discourse
structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over
four domains and three media sources.

479, TITLE:             Harnessing the linguistic signal to predict scalar inferences
https://www.aclweb.org/anthology/2020.acl-main.479
AUTHORS:                Sebastian Schuster, Yuxing Chen, Judith Degen
HIGHLIGHT:              In this work, we explore to what extent neural network sentence encoders can learn to predict the strength of
scalar inferences.

480, TITLE:             Implicit Discourse Relation Classification: We Need to Talk about Evaluation
https://www.aclweb.org/anthology/2020.acl-main.480
AUTHORS:                Najoung Kim, Song Feng, Chulaka Gunasekara, Luis Lastras
HIGHLIGHT:              In this work, we highlight these inconsistencies and propose an improved evaluation protocol.

481, TITLE:             PeTra: A Sparsely Supervised Memory Model for People Tracking
https://www.aclweb.org/anthology/2020.acl-main.481
AUTHORS:                Shubham Toshniwal, Allyson Ettinger, Kevin Gimpel, Karen Livescu
HIGHLIGHT:              We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots.

482, TITLE:             ZPR2: Joint Zero Pronoun Recovery and Resolution using Multi-Task Learning and BERT
https://www.aclweb.org/anthology/2020.acl-main.482
AUTHORS:                Linfeng Song, Kun Xu, Yue Zhang, Jianshu Chen, Dong Yu
HIGHLIGHT:              We propose to better explore their interaction by solving both tasks together, while the previous work treats
them separately.

483, TITLE:              Contextualizing Hate Speech Classifiers with Post-hoc Explanation
https://www.aclweb.org/anthology/2020.acl-main.483
AUTHORS:                Brendan Kennedy, Xisen Jin, Aida Mostafazadeh Davani, Morteza Dehghani, Xiang Ren
HIGHLIGHT:              We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms. Then,
we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group
identifiers in addition to the identifiers themselves.

484, TITLE:              Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation
https://www.aclweb.org/anthology/2020.acl-main.484
AUTHORS:                Tianlu Wang, Xi Victoria Lin, Nazneen Fatema Rajani, Bryan McCann, Vicente Ordonez, Caiming Xiong
HIGHLIGHT:              We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against
such corpus regularities prior to inferring and removing the gender subspace.

485, TITLE:              Language (Technology) is Power: A Critical Survey of ``Bias'' in NLP
https://www.aclweb.org/anthology/2020.acl-main.485
AUTHORS:                Su Lin Blodgett, Solon Barocas, Hal Daum&eacute; III, Hanna Wallach
HIGHLIGHT:              Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that
should guide work analyzing "bias" in NLP systems.

486, TITLE:              Social Bias Frames: Reasoning about Social and Power Implications of Language
https://www.aclweb.org/anthology/2020.acl-main.486
AUTHORS:                Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, Yejin Choi
HIGHLIGHT:              We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in
which people project social biases and stereotypes onto others.

487, TITLE:              Social Biases in NLP Models as Barriers for Persons with Disabilities
https://www.aclweb.org/anthology/2020.acl-main.487
AUTHORS:                Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, Stephen Denuyl
HIGHLIGHT:              In this paper, we present evidence of such undesirable biases towards mentions of disability in two different
English language models: toxicity prediction and sentiment analysis.

488, TITLE:              Towards Debiasing Sentence Representations
https://www.aclweb.org/anthology/2020.acl-main.488
AUTHORS:                Paul Pu Liang, Irene Mengze Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, Louis-Philippe
Morency
HIGHLIGHT:              In this paper, we investigate the presence of social biases in sentence-level representations and propose a new
method, Sent-Debias, to reduce these biases.

489, TITLE:              A Re-evaluation of Knowledge Graph Completion Methods
https://www.aclweb.org/anthology/2020.acl-main.489
AUTHORS:                Zhiqing Sun, Shikhar Vashishth, Soumya Sanyal, Partha Talukdar, Yiming Yang
HIGHLIGHT:              In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and
propose a simple evaluation protocol to address this problem.

490, TITLE:              Cross-Linguistic Syntactic Evaluation of Word Prediction Models
https://www.aclweb.org/anthology/2020.acl-main.490
AUTHORS:                Aaron Mueller, Garrett Nicolai, Panayiota Petrou-Zeniou, Natalia Talmina, Tal Linzen
HIGHLIGHT:              To investigate how these models' ability to learn syntax varies by language, we introduce CLAMS (Cross-
Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models.

491, TITLE:              Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?
https://www.aclweb.org/anthology/2020.acl-main.491
AUTHORS:                Peter Hase, Mohit Bansal
HIGHLIGHT:              Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods:
(1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from
each method.

492, TITLE:             Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions
https://www.aclweb.org/anthology/2020.acl-main.492
AUTHORS:                Xiaochuang Han, Byron C. Wallace, Yulia Tsvetkov
HIGHLIGHT:              In this work, we investigate the use of influence functions for NLP, providing an alternative approach to
interpreting neural text classifiers.

493, TITLE:             Finding Universal Grammatical Relations in Multilingual BERT
https://www.aclweb.org/anthology/2020.acl-main.493
AUTHORS:                Ethan A. Chi, John Hewitt, Christopher D. Manning
HIGHLIGHT:              Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns
representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies
taxonomy.

494, TITLE:             Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection
https://www.aclweb.org/anthology/2020.acl-main.494
AUTHORS:                Hanjie Chen, Guangtao Zheng, Yangfeng Ji
HIGHLIGHT:              In this work, we build hierarchical explanations by detecting feature interactions.

495, TITLE:             Obtaining Faithful Interpretations from Compositional Neural Networks
https://www.aclweb.org/anthology/2020.acl-main.495
AUTHORS:                Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner
HIGHLIGHT:              In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2
and DROP, two datasets which require composing multiple reasoning steps.

496, TITLE:             Rationalizing Text Matching: Learning Sparse Alignments via Optimal Transport
https://www.aclweb.org/anthology/2020.acl-main.496
AUTHORS:                Kyle Swanson, Lili Yu, Tao Lei
HIGHLIGHT:              In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly
select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction.

497, TITLE:             Benefits of Intermediate Annotations in Reading Comprehension
https://www.aclweb.org/anthology/2020.acl-main.497
AUTHORS:                Dheeru Dua, Sameer Singh, Matt Gardner
HIGHLIGHT:              In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer
during data collection.

498, TITLE:             Crossing Variational Autoencoders for Answer Retrieval
https://www.aclweb.org/anthology/2020.acl-main.498
AUTHORS:                Wenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng, Meng Jiang
HIGHLIGHT:              In this work, we propose to cross variational auto-encoders by generating questions with aligned answers and
generating answers with aligned questions.

499, TITLE:             Logic-Guided Data Augmentation and Regularization for Consistent Question Answering
https://www.aclweb.org/anthology/2020.acl-main.499
AUTHORS:                Akari Asai, Hannaneh Hajishirzi
HIGHLIGHT:              This paper addresses the problem of improving the accuracy and consistency of responses to comparison
questions by integrating logic rules and neural models.

500, TITLE:             On the Importance of Diversity in Question Generation for QA
https://www.aclweb.org/anthology/2020.acl-main.500
AUTHORS:                Md Arafat Sultan, Shubham Chandel, Ram&oacute;n Fernandez Astudillo, Vittorio Castelli
HIGHLIGHT:              In this paper we ask: Is textual diversity in QG beneficial for downstream QA?

501, TITLE:             Probabilistic Assumptions Matter: Improved Models for Distantly-Supervised Document-Level Question
Answering
https://www.aclweb.org/anthology/2020.acl-main.501
AUTHORS:                Hao Cheng, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
HIGHLIGHT:              We address the problem of extractive question answering using document-level distant super-vision, pairing
questions and relevant documents with answer strings.

502, TITLE:             SCDE: Sentence Cloze Dataset with High Quality Distractors From Examinations
https://www.aclweb.org/anthology/2020.acl-main.502
AUTHORS:                Xiang Kong, Varun Gangal, Eduard Hovy
HIGHLIGHT:              We introduce SCDE, a dataset to evaluate the performance of computational models through sentence
prediction.

503, TITLE:             Selective Question Answering under Domain Shift
https://www.aclweb.org/anthology/2020.acl-main.503
AUTHORS:                Amita Kamath, Robin Jia, Percy Liang
HIGHLIGHT:              In this work, we propose the setting of selective question answering under domain shift, in which a QA model is
tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while
maintaining high accuracy.

504, TITLE:             The Cascade Transformer: an Application for Efficient Answer Sentence Selection
https://www.aclweb.org/anthology/2020.acl-main.504
AUTHORS:                Luca Soldaini, Alessandro Moschitti
HIGHLIGHT:              In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-
based models into a cascade of rankers.

505, TITLE:             Transformers to Learn Hierarchical Contexts in Multiparty Dialogue for Span-based Question Answering
https://www.aclweb.org/anthology/2020.acl-main.505
AUTHORS:                Changmao Li, Jinho D. Choi
HIGHLIGHT:              We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue.

506, TITLE:             Not All Claims are Created Equal: Choosing the Right Statistical Approach to Assess Hypotheses
https://www.aclweb.org/anthology/2020.acl-main.506
AUTHORS:                Erfan Sadeqi Azer, Daniel Khashabi, Ashish Sabharwal, Dan Roth
HIGHLIGHT:              We address this gap by contrasting various hypothesis assessment techniques, especially those not commonly
used in the field (such as evaluations based on Bayesian inference).

507, TITLE:             STARC: Structured Annotations for Reading Comprehension
https://www.aclweb.org/anthology/2020.acl-main.507
AUTHORS:                Yevgeni Berzak, Jonathan Malmaud, Roger Levy
HIGHLIGHT:              We present STARC (Structured Annotations for Reading Comprehension), a new annotation framework for
assessing reading comprehension with multiple choice questions.

508, TITLE:             WinoWhy: A Deep Diagnosis of Essential Commonsense Knowledge for Answering Winograd Schema
Challenge
https://www.aclweb.org/anthology/2020.acl-main.508
AUTHORS:                Hongming Zhang, Xinran Zhao, Yangqiu Song
HIGHLIGHT:              In this paper, we present the first comprehensive categorization of essential commonsense knowledge for
answering the Winograd Schema Challenge (WSC).

509, TITLE:             Agreement Prediction of Arguments in Cyber Argumentation for Detecting Stance Polarity and Intensity
https://www.aclweb.org/anthology/2020.acl-main.509
AUTHORS:                Joseph Sirrianni, Xiaoqing Liu, Douglas Adams
HIGHLIGHT:              We introduce a new research problem, stance polarity and intensity prediction in response relationships between
posts.

510, TITLE:             Cross-Lingual Unsupervised Sentiment Classification with Multi-View Transfer Learning
https://www.aclweb.org/anthology/2020.acl-main.510
AUTHORS:                Hongliang Fei, Ping Li
HIGHLIGHT:              We propose an unsupervised cross-lingual sentiment classification model named multi-view encoder-classifier
(MVEC) that leverages an unsupervised machine translation (UMT) system and a language discriminator.

511, TITLE:             Efficient Pairwise Annotation of Argument Quality
https://www.aclweb.org/anthology/2020.acl-main.511
AUTHORS:                Lukas Gienapp, Benno Stein, Matthias Hagen, Martin Potthast
HIGHLIGHT:              We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably
as per previous work.

512, TITLE:            Entity-Aware Dependency-Based Deep Graph Attention Network for Comparative Preference Classification
https://www.aclweb.org/anthology/2020.acl-main.512
AUTHORS:               Nianzu Ma, Sahisnu Mazumder, Hao Wang, Bing Liu
HIGHLIGHT:             This paper proposes a novel Entity-aware Dependency-based Deep Graph Attention Network (ED-GAT) that
employs a multi-hop graph attention over a dependency graph sentence representation to leverage both the semantic information from
word embeddings and the syntactic information from the dependency graph to solve the problem.

513, TITLE:            OpinionDigest: A Simple Framework for Opinion Summarization
https://www.aclweb.org/anthology/2020.acl-main.513
AUTHORS:               Yoshihiko Suhara, Xiaolan Wang, Stefanos Angelidis, Wang-Chiew Tan
HIGHLIGHT:             We present OpinionDigest, an abstractive opinion summarization framework, which does not rely on gold-
standard summaries for training.

514, TITLE:            A Comprehensive Analysis of Preprocessing for Word Representation Learning in Affective Tasks
https://www.aclweb.org/anthology/2020.acl-main.514
AUTHORS:               Nastaran Babanejad, Ameeta Agrawal, Aijun An, Manos Papagelis
HIGHLIGHT:             To address this limitation, we conduct a comprehensive analysis of the role of preprocessing techniques in
affective analysis based on word vector models.

515, TITLE:            Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness
https://www.aclweb.org/anthology/2020.acl-main.515
AUTHORS:               Sixing Wu, Ying Li, Dawei Zhang, Yang Zhou, Zhonghai Wu
HIGHLIGHT:             To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI.
We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation.

516, TITLE:            Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue
Generation
https://www.aclweb.org/anthology/2020.acl-main.516
AUTHORS:               Haoyu Song, Yan Wang, Wei-Nan Zhang, Xiaojiang Liu, Ting Liu
HIGHLIGHT:             In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete
inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one.

517, TITLE:            Learning to Customize Model Structures for Few-shot Dialogue Generation Tasks
https://www.aclweb.org/anthology/2020.acl-main.517
AUTHORS:               YIPING SONG, Zequn Liu, Wei Bi, Rui Yan, Ming Zhang
HIGHLIGHT:             In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot
setting.

518, TITLE:            Video-Grounded Dialogues with Pretrained Generation Language Models
https://www.aclweb.org/anthology/2020.acl-main.518
AUTHORS:               Hung Le, Steven C.H. Hoi
HIGHLIGHT:             In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue,
which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both
spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns.

519, TITLE:            A Unified MRC Framework for Named Entity Recognition
https://www.aclweb.org/anthology/2020.acl-main.519
AUTHORS:               Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, Jiwei Li
HIGHLIGHT:             In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks.

520, TITLE:            An Effective Transition-based Model for Discontinuous NER
https://www.aclweb.org/anthology/2020.acl-main.520
AUTHORS:               Xiang Dai, Sarvnaz Karimi, Ben Hachey, Cecile Paris
HIGHLIGHT:             We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER.

521, TITLE:            IMoJIE: Iterative Memory-Based Joint Open Information Extraction
https://www.aclweb.org/anthology/2020.acl-main.521
AUTHORS:               Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam, Soumen Chakrabarti
HIGHLIGHT:             We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all
previously extracted tuples.

522, TITLE:             Improving Event Detection via Open-domain Trigger Knowledge
https://www.aclweb.org/anthology/2020.acl-main.522
AUTHORS:                Meihan Tong, Bin Xu, Shuai Wang, Yixin Cao, Lei Hou, Juanzi Li, Jun Xie
HIGHLIGHT:              To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external
open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations.

523, TITLE:             Improving Low-Resource Named Entity Recognition using Joint Sentence and Token Labeling
https://www.aclweb.org/anthology/2020.acl-main.523
AUTHORS:                Canasai Kruengkrai, Thien Hai Nguyen, Sharifah Mahani Aljunied, Lidong Bing
HIGHLIGHT:              We present a joint model that supports multi-class classification and introduce a simple variant of self-attention
that allows the model to learn scaling factors.

524, TITLE:             Multi-Cell Compositional LSTM for NER Domain Adaptation
https://www.aclweb.org/anthology/2020.acl-main.524
AUTHORS:                Chen Jia, Yue Zhang
HIGHLIGHT:              We investigate a multi-cell compositional LSTM structure for multi-task learning, modeling each entity type
using a separate cell state.

525, TITLE:             Pyramid: A Layered Model for Nested Named Entity Recognition
https://www.aclweb.org/anthology/2020.acl-main.525
AUTHORS:                Jue WANG, Lidan Shou, Ke Chen, Gang Chen
HIGHLIGHT:              This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER).

526, TITLE:             ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for
Knowledge Graph Embedding
https://www.aclweb.org/anthology/2020.acl-main.526
AUTHORS:                Zhiwen Xie, Guangyou Zhou, Jin Liu, Jimmy Xiangji Huang
HIGHLIGHT:              In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception
network with joint local-global structural information for knowledge graph Embedding (ReInceptionE).

527, TITLE:             Relabel the Noise: Joint Extraction of Entities and Relations via Cooperative Multiagents
https://www.aclweb.org/anthology/2020.acl-main.527
AUTHORS:                Daoyuan Chen, Yaliang Li, Kai Lei, Ying Shen
HIGHLIGHT:              We propose a joint extraction approach to address this problem by re-labeling noisy instances with a group of
cooperative multiagents.

528, TITLE:             Simplify the Usage of Lexicon in Chinese NER
https://www.aclweb.org/anthology/2020.acl-main.528
AUTHORS:                Ruotian Ma, Minlong Peng, Qi Zhang, Zhongyu Wei, Xuanjing Huang
HIGHLIGHT:              In this work, we propose a simple but effective method for incorporating the word lexicon into the character
representations.

529, TITLE:             AdvAug: Robust Adversarial Augmentation for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.529
AUTHORS:                Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein
HIGHLIGHT:              In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT).

530, TITLE:             Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns
https://www.aclweb.org/anthology/2020.acl-main.530
AUTHORS:                KayYen Wong, Sameen Maruf, Gholamreza Haffari
HIGHLIGHT:              In this work, we investigate the effect of future sentences as context by comparing the performance of a
contextual NMT model trained with the future context to the one trained with the past context.

531, TITLE:             Improving Neural Machine Translation with Soft Template Prediction
https://www.aclweb.org/anthology/2020.acl-main.531
AUTHORS:                Jian Yang, Shuming Ma, Dongdong Zhang, Zhoujun Li, Ming Zhou
HIGHLIGHT:              Inspired by the success of template-based and syntax-based approaches in other fields, we propose to use
extracted templates from tree structures as soft target templates to guide the translation procedure.

532, TITLE:             Tagged Back-translation Revisited: Why Does It Really Work?
https://www.aclweb.org/anthology/2020.acl-main.532
AUTHORS:                Benjamin Marie, Raphael Rubino, Atsushi Fujita
HIGHLIGHT:              In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data
overfit some of the characteristics of machine-translated texts.

533, TITLE:             Worse WER, but Better BLEU? Leveraging Word Embedding as Intermediate in Multitask End-to-End Speech
Translation
https://www.aclweb.org/anthology/2020.acl-main.533
AUTHORS:                Shun-Po Chuang, Tzu-Wei Sung, Alexander H. Liu, Hung-yi Lee
HIGHLIGHT:              Because whether the output of the recognition decoder has the correct semantics is more critical than its
accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.

534, TITLE:             Neural-DINF: A Neural Network based Framework for Measuring Document Influence
https://www.aclweb.org/anthology/2020.acl-main.534
AUTHORS:                Jie Tan, Changlin Yang, Ying Li, Siliang Tang, Chen Huang, Yueting Zhuang
HIGHLIGHT:              In this paper, we use both frequency changes and word semantic shifts to measure document influence by
developing a neural network framework.

535, TITLE:             Paraphrase Generation by Learning How to Edit from Samples
https://www.aclweb.org/anthology/2020.acl-main.535
AUTHORS:                Amirhossein Kazemnejad, Mohammadreza Salehi, Mahdieh Soleymani Baghshah
HIGHLIGHT:              To address these problems, we propose a novel retrieval-based method for paraphrase generation.

536, TITLE:             Emerging Cross-lingual Structure in Pretrained Language Models
https://www.aclweb.org/anthology/2020.acl-main.536
AUTHORS:                Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, Veselin Stoyanov
HIGHLIGHT:              We study the problem of multilingual masked language modeling, i.e. the training of a single model on
concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so
effective for cross-lingual transfer.

537, TITLE:             FastBERT: a Self-distilling BERT with Adaptive Inference Time
https://www.aclweb.org/anthology/2020.acl-main.537
AUTHORS:                Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, QI JU
HIGHLIGHT:              To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT
with adaptive inference time.

538, TITLE:             Incorporating External Knowledge through Pre-training for Natural Language to Code Generation
https://www.aclweb.org/anthology/2020.acl-main.538
AUTHORS:                Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, Graham Neubig
HIGHLIGHT:              Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore
the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs
from the online programming QA forum StackOverflow and programming language API documentation.

539, TITLE:             LogicalFactChecker: Leveraging Logical Operations for Fact Checking with Graph Module Network
https://www.aclweb.org/anthology/2020.acl-main.539
AUTHORS:                Wanjun Zhong, Duyu Tang, Zhangyin Feng, Nan Duan, Ming Zhou, Ming Gong, Linjun Shou, Daxin Jiang,
Jiahai Wang, Jian Yin
HIGHLIGHT:              In this work, we propose LogicalFactChecker, a neural network approach capable of leveraging logical
operations for fact checking.

540, TITLE:             Word-level Textual Adversarial Attacking as Combinatorial Optimization
https://www.aclweb.org/anthology/2020.acl-main.540
AUTHORS:                Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun
HIGHLIGHT:              In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method
and particle swarm optimization-based search algorithm to solve the two problems separately.

541, TITLE:             Benchmarking Multimodal Regex Synthesis with Complex Structures
https://www.aclweb.org/anthology/2020.acl-main.541
AUTHORS:                Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett
HIGHLIGHT:             We introduce StructuredRegex, a new regex synthesis dataset differing from prior ones in three aspects.

542, TITLE:            Curriculum Learning for Natural Language Understanding
https://www.aclweb.org/anthology/2020.acl-main.542
AUTHORS:               Benfeng Xu, Licheng Zhang, Zhendong Mao, Quan Wang, Hongtao Xie, Yongdong Zhang
HIGHLIGHT:             However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure,
language models can benefit from an easy-to-difficult curriculum. Based on this idea, we propose our Curriculum Learning approach.

543, TITLE:            Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?
https://www.aclweb.org/anthology/2020.acl-main.543
AUTHORS:               Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui
HIGHLIGHT:             In this paper, we introduce a method for evaluating whether neural models can learn systematicity of
monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on
composition.

544, TITLE:            Evidence-Aware Inferential Text Generation with Vector Quantised Variational AutoEncoder
https://www.aclweb.org/anthology/2020.acl-main.544
AUTHORS:               Daya Guo, Duyu Tang, Nan Duan, Jian Yin, Daxin Jiang, Ming Zhou
HIGHLIGHT:             To address this, we propose an approach that automatically finds evidence for an event from a large text corpus,
and leverages the evidence to guide the generation of inferential texts.

545, TITLE:            How to Ask Good Questions? Try to Leverage Paraphrases
https://www.aclweb.org/anthology/2020.acl-main.545
AUTHORS:               Xin Jia, Wenjie Zhou, Xu SUN, Yunfang Wu
HIGHLIGHT:             Specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is
automatically conducted by a simple back-translation method.

546, TITLE:            NeuInfer: Knowledge Inference on N-ary Facts
https://www.aclweb.org/anthology/2020.acl-main.546
AUTHORS:               Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, Xueqi Cheng
HIGHLIGHT:             We represent each n-ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute-value
pair(s).

547, TITLE:            Neural Graph Matching Networks for Chinese Short Text Matching
https://www.aclweb.org/anthology/2020.acl-main.547
AUTHORS:               Lu Chen, Yanbin Zhao, Boer Lyu, Lesheng Jin, Zhi Chen, Su Zhu, Kai Yu
HIGHLIGHT:             To address this problem, we propose neural graph matching networks, a novel sentence matching framework
capable of dealing with multi-granular input information.

548, TITLE:            Neural Mixed Counting Models for Dispersed Topic Discovery
https://www.aclweb.org/anthology/2020.acl-main.548
AUTHORS:               Jiemin Wu, Yanghui Rao, Zusheng Zhang, Haoran Xie, Qing Li, Fu Lee Wang, Ziye Chen
HIGHLIGHT:             In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic
Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery.

549, TITLE:            Reasoning Over Semantic-Level Graph for Fact Checking
https://www.aclweb.org/anthology/2020.acl-main.549
AUTHORS:               Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, Jian Yin
HIGHLIGHT:             In this work, we present a method suitable for reasoning about the semantic-level structure of evidence.

550, TITLE:            Automatic Generation of Citation Texts in Scholarly Papers: A Pilot Study
https://www.aclweb.org/anthology/2020.acl-main.550
AUTHORS:               Xinyu Xing, Xiaosheng Fan, Xiaojun Wan
HIGHLIGHT:             In this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers.

551, TITLE:            Composing Elementary Discourse Units in Abstractive Summarization
https://www.aclweb.org/anthology/2020.acl-main.551
AUTHORS:               Zhenwen Li, Wenhao Wu, Sujian Li
HIGHLIGHT:             In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content
selection than the sentence unit in abstractive summarization.

552, TITLE:            Extractive Summarization as Text Matching
https://www.aclweb.org/anthology/2020.acl-main.552
AUTHORS:               Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang, Xipeng Qiu, Xuanjing Huang
HIGHLIGHT:             This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems.

553, TITLE:            Heterogeneous Graph Neural Networks for Extractive Document Summarization
https://www.aclweb.org/anthology/2020.acl-main.553
AUTHORS:               Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, Xuanjing Huang
HIGHLIGHT:             In this paper, we present a heterogeneous graph-based neural network for extractive summarization
(HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences.

554, TITLE:            Jointly Learning to Align and Summarize for Neural Cross-Lingual Summarization
https://www.aclweb.org/anthology/2020.acl-main.554
AUTHORS:               Yue Cao, Hui Liu, Xiaojun Wan
HIGHLIGHT:             In this paper, we propose to ease the cross-lingual summarization training by jointly learning to align and
summarize.

555, TITLE:            Leveraging Graph to Improve Abstractive Multi-Document Summarization
https://www.aclweb.org/anthology/2020.acl-main.555
AUTHORS:               Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng Wang, Junping Du
HIGHLIGHT:             In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage
well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple
input documents and produce abstractive summaries.

556, TITLE:            Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization
https://www.aclweb.org/anthology/2020.acl-main.556
AUTHORS:               Hanqi Jin, Tianming Wang, Xiaojun Wan
HIGHLIGHT:             In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document
summarization, which jointly learn semantic representations for words, sentences, and documents.

557, TITLE:            Tetra-Tagging: Word-Synchronous Parsing with Linear-Time Inference
https://www.aclweb.org/anthology/2020.acl-main.557
AUTHORS:               Nikita Kitaev, Dan Klein
HIGHLIGHT:             We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in
a sentence.

558, TITLE:            Are we Estimating or Guesstimating Translation Quality?
https://www.aclweb.org/anthology/2020.acl-main.558
AUTHORS:               Shuo Sun, Francisco Guzm&aacute;n, Lucia Specia
HIGHLIGHT:             Our findings suggest that although QE models might capture fluency of translated sentences and complexity of
source sentences, they cannot model adequacy of translations effectively.

559, TITLE:            Language (Re)modelling: Towards Embodied Language Understanding
https://www.aclweb.org/anthology/2020.acl-main.559
AUTHORS:               Ronen Tamari, Chen Shani, Tom Hope, Miriam R L Petruck, Omri Abend, Dafna Shahaf
HIGHLIGHT:             This work proposes an approach to representation and learning based on the tenets of embodied cognitive
linguistics (ECL).

560, TITLE:            The State and Fate of Linguistic Diversity and Inclusion in the NLP World
https://www.aclweb.org/anthology/2020.acl-main.560
AUTHORS:               Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, Monojit Choudhury
HIGHLIGHT:             In this paper we look at the relation between the types of languages, resources, and their representation in NLP
conferences to understand the trajectory that different languages have followed over time.

561, TITLE:            The Unstoppable Rise of Computational Linguistics in Deep Learning
https://www.aclweb.org/anthology/2020.acl-main.561
AUTHORS:               James Henderson
HIGHLIGHT:             In this paper, we trace the history of neural networks applied to natural language understanding tasks, and
identify key contributions which the nature of language has made to the development of neural network architectures.

562, TITLE:             To Boldly Query What No One Has Annotated Before? The Frontiers of Corpus Querying
https://www.aclweb.org/anthology/2020.acl-main.562
AUTHORS:                Markus G&auml;rtner, Kerstin Jung
HIGHLIGHT:              This paper offers a broad overview of the history of corpora and corpus query tools.

563, TITLE:             A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking
https://www.aclweb.org/anthology/2020.acl-main.563
AUTHORS:                Yong Shan, Zekang Li, Jinchao Zhang, Fandong Meng, Yang Feng, Cheng Niu, Jie Zhou
HIGHLIGHT:              In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to
not only discern relevant information at both word level and turn level but also learn contextual representations.

564, TITLE:             Data Manipulation: Towards Effective Instance Learning for Neural Dialogue Generation via Learning to
Augment and Reweight
https://www.aclweb.org/anthology/2020.acl-main.564
AUTHORS:                Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng Zhang, Xiaofang Zhao, Dawei Yin
HIGHLIGHT:              In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards
reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples
simultaneously.

565, TITLE:             Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog
https://www.aclweb.org/anthology/2020.acl-main.565
AUTHORS:                Libo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, Ting Liu
HIGHLIGHT:              To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-
private network to learn shared and specific knowledge.

566, TITLE:             Learning Efficient Dialogue Policy from Demonstrations through Shaping
https://www.aclweb.org/anthology/2020.acl-main.566
AUTHORS:                Huimin Wang, Baolin Peng, Kam-Fai Wong
HIGHLIGHT:              In this paper, we present S{\^{}}2Agent that efficiently learns dialogue policy from demonstrations through
policy shaping and reward shaping.

567, TITLE:             SAS: Dialogue State Tracking via Slot Attention and Slot Information Sharing
https://www.aclweb.org/anthology/2020.acl-main.567
AUTHORS:                Jiaying Hu, Yan Yang, Chencai Chen, liang he, Zhou Yu
HIGHLIGHT:              We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce
redundant information's interference and improve long dialogue context tracking.

568, TITLE:             Speaker Sensitive Response Evaluation Model
https://www.aclweb.org/anthology/2020.acl-main.568
AUTHORS:                JinYeong Bak, Alice Oh
HIGHLIGHT:              In this paper, we propose an automatic evaluation model based on that idea and learn the model parameters
from an unlabeled conversation corpus.

569, TITLE:             A Top-down Neural Architecture towards Text-level Parsing of Discourse Rhetorical Structure
https://www.aclweb.org/anthology/2020.acl-main.569
AUTHORS:                Longyin Zhang, Yuqing Xing, Fang Kong, Peifeng Li, Guodong Zhou
HIGHLIGHT:              In this paper, we justify from both computational and perceptive points-of-view that the top-down architecture
is more suitable for text-level DRS parsing.

570, TITLE:             Amalgamation of protein sequence, structure and textual information for improving protein-protein interaction
identification
https://www.aclweb.org/anthology/2020.acl-main.570
AUTHORS:                pratik Dutta, Sriparna Saha
HIGHLIGHT:              In this paper, we argue that incorporating multimodal cues can improve the automatic identification of PPI.

571, TITLE:             Bipartite Flat-Graph Network for Nested Named Entity Recognition
https://www.aclweb.org/anthology/2020.acl-main.571
AUTHORS:                Ying Luo, Hai Zhao
HIGHLIGHT:             In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for nested named entity recognition
(NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located
in inner layers.

572, TITLE:            Connecting Embeddings for Knowledge Graph Entity Typing
https://www.aclweb.org/anthology/2020.acl-main.572
AUTHORS:               Yu Zhao, anxiang zhang, Ruobing Xie, Kang Liu, Xiaojie WANG
HIGHLIGHT:             In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing
knowledge from existing entity type assertions and global triple knowledge in KGs.

573, TITLE:            Continual Relation Learning via Episodic Memory Activation and Reconsolidation
https://www.aclweb.org/anthology/2020.acl-main.573
AUTHORS:               Xu Han, Yi Dai, Tianyu Gao, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou
HIGHLIGHT:             Inspired by the mechanism in human long-term memory formation, we introduce episodic memory activation
and reconsolidation (EMAR) to continual relation learning.

574, TITLE:            Handling Rare Entities for Neural Sequence Labeling
https://www.aclweb.org/anthology/2020.acl-main.574
AUTHORS:               Yangming Li, Han Li, Kaisheng Yao, Xiaolong Li
HIGHLIGHT:             Most of test set entities appear only few times and are even unseen in training corpus, yielding large number of
out-of-vocabulary (OOV) and low-frequency (LF) entities during evaluation. In this work, we propose approaches to address this
problem.

575, TITLE:            Instance-Based Learning of Span Representations: A Case Study through Named Entity Recognition
https://www.aclweb.org/anthology/2020.acl-main.575
AUTHORS:               Hiroki Ouchi, Jun Suzuki, Sosuke Kobayashi, Sho Yokoi, Tatsuki Kuribayashi, Ryuto Konno, Kentaro Inui
HIGHLIGHT:             In this study, we develop models possessing interpretable inference process for structured prediction.

576, TITLE:            MIE: A Medical Information Extractor towards Medical Dialogues
https://www.aclweb.org/anthology/2020.acl-main.576
AUTHORS:               Yuanzhe Zhang, Zhongtao Jiang, Tao Zhang, Shiwan Liu, Jiarun Cao, Kang Liu, Shengping Liu, Jun Zhao
HIGHLIGHT:             We then propose a Medical Information Extractor (MIE) towards medical dialogues. MIE is able to extract
mentioned symptoms, surgeries, tests, other information and their corresponding status.

577, TITLE:            Named Entity Recognition as Dependency Parsing
https://www.aclweb.org/anthology/2020.acl-main.577
AUTHORS:               Juntao Yu, Bernd Bohnet, Massimo Poesio
HIGHLIGHT:             In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the
input via a biaffine model (Dozat and Manning, 2017).

578, TITLE:            Neighborhood Matching Network for Entity Alignment
https://www.aclweb.org/anthology/2020.acl-main.578
AUTHORS:               Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Dongyan Zhao
HIGHLIGHT:             This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling
the structural heterogeneity challenge.

579, TITLE:            Relation Extraction with Explanation
https://www.aclweb.org/anthology/2020.acl-main.579
AUTHORS:               Hamed Shahbazi, Xiaoli Fern, Reza Ghaeini, Prasad Tadepalli
HIGHLIGHT:             In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of
explanations afforded by the relation extraction models.

580, TITLE:            Representation Learning for Information Extraction from Form-like Documents
https://www.aclweb.org/anthology/2020.acl-main.580
AUTHORS:               Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, Marc Najork
HIGHLIGHT:             We propose a novel approach using representation learning for tackling the problem of extracting structured
information from form-like document images.

581, TITLE:            Single-/Multi-Source Cross-Lingual NER via Teacher-Student Learning on Unlabeled Data in Target Language
https://www.aclweb.org/anthology/2020.acl-main.581
AUTHORS:                Qianhui Wu, Zijia Lin, B&ouml;rje Karlsson, Jian-Guang LOU, Biqing Huang
HIGHLIGHT:              In this paper, we propose a teacher-student learning method to address such limitations, where NER models in
the source languages are used as teachers to train a student model on unlabeled data in the target language.

582, TITLE:             Synchronous Double-channel Recurrent Network for Aspect-Opinion Pair Extraction
https://www.aclweb.org/anthology/2020.acl-main.582
AUTHORS:                Shaowei Chen, Jie Liu, Yu Wang, Wenzheng Zhang, Ziming Chi
HIGHLIGHT:              In this paper, we explore Aspect-Opinion Pair Extraction (AOPE) task, which aims at extracting aspects and
opinion expressions in pairs.
To verify the performance of SDRN, we manually build three datasets based on SemEval 2014 and 2015 benchmarks.

583, TITLE:             Cross-modal Coherence Modeling for Caption Generation
https://www.aclweb.org/anthology/2020.acl-main.583
AUTHORS:                Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut, Matthew Stone
HIGHLIGHT:              We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show
that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware,
controllable image captioning models.

584, TITLE:             Knowledge Supports Visual Language Grounding: A Case Study on Colour Terms
https://www.aclweb.org/anthology/2020.acl-main.584
AUTHORS:                Simeon Sch&uuml;z, Sina Zarrie&szlig;
HIGHLIGHT:              We go beyond previous studies on colour terms using isolated colour swatches and study visual grounding of
colour terms in realistic objects.

585, TITLE:             Span-based Localizing Network for Natural Language Video Localization
https://www.aclweb.org/anthology/2020.acl-main.585
AUTHORS:                Hao Zhang, Aixin Sun, Wei Jing, Joey Tianyi Zhou
HIGHLIGHT:              In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage.

586, TITLE:             Words Aren't Enough, Their Order Matters: On the Robustness of Grounding Visual Referring Expressions
https://www.aclweb.org/anthology/2020.acl-main.586
AUTHORS:                Arjun Akula, Spandana Gella, Yaser Al-Onaizan, Song-Chun Zhu, Siva Reddy
HIGHLIGHT:              Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12%
to 23% lower in performance than the established progress for this task.

587, TITLE:             A Mixture of h - 1 Heads is Better than h Heads
https://www.aclweb.org/anthology/2020.acl-main.587
AUTHORS:                Hao Peng, Roy Schwartz, Dianqi Li, Noah A. Smith
HIGHLIGHT:              In this work, we instead "reallocate" them-the model learns to activate different heads on different inputs.

588, TITLE:             Dependency Graph Enhanced Dual-transformer Structure for Aspect-based Sentiment Classification
https://www.aclweb.org/anthology/2020.acl-main.588
AUTHORS:                Hao Tang, Donghong Ji, Chenliang Li, Qiji Zhou
HIGHLIGHT:              To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly
considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding
dependency graph in an iterative interaction manner.

589, TITLE:             Differentiable Window for Dynamic Local Attention
https://www.aclweb.org/anthology/2020.acl-main.589
AUTHORS:                Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty, Xiaoli Li
HIGHLIGHT:              We propose Differentiable Window, a new neural module and general purpose component for dynamic window
selection.

590, TITLE:             Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with
Adversarial Examples
https://www.aclweb.org/anthology/2020.acl-main.590
AUTHORS:                Xiaoqing Zheng, Jiehang Zeng, Yi Zhou, Cho-Jui Hsieh, Minhao Cheng, Xuanjing Huang
HIGHLIGHT:              In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches
to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and
design algorithms to construct such examples in both of the black-box and white-box settings.

591, TITLE:             Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach
https://www.aclweb.org/anthology/2020.acl-main.591
AUTHORS:                Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J. Oâ€™Donnell, Yoshua Bengio, Yue Zhang
HIGHLIGHT:              In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as
ground truth parse trees in a form called "syntactic distances", where information between these two separate objectives shares the
same intermediate representation.

592, TITLE:             Learning Architectures from an Extended Search Space for Language Modeling
https://www.aclweb.org/anthology/2020.acl-main.592
AUTHORS:                Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, changliang li
HIGHLIGHT:              Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict
search to learning architectures of a recurrent or convolutional cell. In this paper, we extend the search space of NAS.

593, TITLE:             The Right Tool for the Job: Matching Model and Instance Complexities
https://www.aclweb.org/anthology/2020.acl-main.593
AUTHORS:                Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, Noah A. Smith
HIGHLIGHT:              To better respect a given inference budget, we propose a modification to contextual representation fine-tuning
which, during inference, allows for an early (and fast) "exit" from neural network calculations for simple instances, and late (and
accurate) exit for hard instances.

594, TITLE:             Bootstrapping Techniques for Polysynthetic Morphological Analysis
https://www.aclweb.org/anthology/2020.acl-main.594
AUTHORS:                William Lane, Steven Bird
HIGHLIGHT:              To address this challenge, we offer linguistically-informed approaches for bootstrapping a neural morphological
analyzer, and demonstrate its application to Kunwinjku, a polysynthetic Australian language.

595, TITLE:             Coupling Distant Annotation and Adversarial Training for Cross-Domain Chinese Word Segmentation
https://www.aclweb.org/anthology/2020.acl-main.595
AUTHORS:                Ning Ding, Dingkun Long, Guangwei Xu, Muhua Zhu, Pengjun Xie, Xiaobin Wang, Haitao Zheng
HIGHLIGHT:              In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial
training for cross-domain CWS.

596, TITLE:             Modeling Morphological Typology for Unsupervised Learning of Language Morphology
https://www.aclweb.org/anthology/2020.acl-main.596
AUTHORS:                Hongzhi Xu, Jordan Kodner, Mitchell Marcus, Charles Yang
HIGHLIGHT:              This paper describes a language-independent model for fully unsupervised morphological analysis that exploits
a universal framework leveraging morphological typology.

597, TITLE:             Predicting Declension Class from Form and Meaning
https://www.aclweb.org/anthology/2020.acl-main.597
AUTHORS:                Adina Williams, Tiago Pimentel, Hagen Blix, Arya D. McCarthy, Eleanor Chodroff, Ryan Cotterell
HIGHLIGHT:              More specifically, we operationalize this by measuring how much information, in bits, we can glean about
declension class from knowing the form and/or meaning of nouns.

598, TITLE:             Unsupervised Morphological Paradigm Completion
https://www.aclweb.org/anthology/2020.acl-main.598
AUTHORS:                Huiming Jin, Liwei Cai, Yihui Peng, Chen Xia, Arya McCarthy, Katharina Kann
HIGHLIGHT:              We propose the task of unsupervised morphological paradigm completion.

599, TITLE:             Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension
https://www.aclweb.org/anthology/2020.acl-main.599
AUTHORS:                Bo Zheng, Haoyang Wen, Yaobo Liang, Nan Duan, Wanxiang Che, Daxin Jiang, Ming Zhou, Ting Liu
HIGHLIGHT:              To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses
on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and
tokens.

600, TITLE:             Harvesting and Refining Question-Answer Pairs for Unsupervised QA
https://www.aclweb.org/anthology/2020.acl-main.600
AUTHORS:                Zhongli Li, Wenhui Wang, Li Dong, Furu Wei, Ke Xu
HIGHLIGHT:              In this work, we introduce two approaches to improve unsupervised QA.

601, TITLE:            Low-Resource Generation of Multi-hop Reasoning Questions
https://www.aclweb.org/anthology/2020.acl-main.601
AUTHORS:               Jianxing Yu, Wei Liu, Shuang Qiu, Qinliang Su, Kai Wang, Xiaojun Quan, Jian Yin
HIGHLIGHT:             Since the labeled data is limited and insufficient for training, we propose to learn the model with the help of a
large scale of unlabeled data that is much easier to obtain.

602, TITLE:            R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason
https://www.aclweb.org/anthology/2020.acl-main.602
AUTHORS:               Naoya Inoue, Pontus Stenetorp, Kentaro Inui
HIGHLIGHT:             We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations.

603, TITLE:            Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension
https://www.aclweb.org/anthology/2020.acl-main.603
AUTHORS:               Hongyu Gong, Yelong Shen, Dian Yu, Jianshu Chen, Dong Yu
HIGHLIGHT:             In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a
lengthy document and a query, extracts a text span from the document as an answer.

604, TITLE:            RikiNet: Reading Wikipedia Pages for Natural Question Answering
https://www.aclweb.org/anthology/2020.acl-main.604
AUTHORS:               Dayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng Chen, Daxin Jiang, Jiancheng Lv, Nan Duan
HIGHLIGHT:             In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question
answering.

605, TITLE:            Parsing into Variable-in-situ Logico-Semantic Graphs
https://www.aclweb.org/anthology/2020.acl-main.605
AUTHORS:               Yufei Chen, Weiwei Sun
HIGHLIGHT:             We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form
parsing.

606, TITLE:            Semantic Parsing for English as a Second Language
https://www.aclweb.org/anthology/2020.acl-main.606
AUTHORS:               Yuanyuan Zhao, Weiwei Sun, junjie cao, Xiaojun Wan
HIGHLIGHT:             Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface
during second language acquisition, we formulate the task based on the divergence between literal and intended meanings.

607, TITLE:            Semi-Supervised Semantic Dependency Parsing Using CRF Autoencoders
https://www.aclweb.org/anthology/2020.acl-main.607
AUTHORS:               Zixia Jia, Youmi Ma, Jiong Cai, Kewei Tu
HIGHLIGHT:             We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF
autoencoder framework.

608, TITLE:            Unsupervised Dual Paraphrasing for Two-stage Semantic Parsing
https://www.aclweb.org/anthology/2020.acl-main.608
AUTHORS:               Ruisheng Cao, Su Zhu, Chenyu Yang, Chen Liu, Rao Ma, Yanbin Zhao, Lu Chen, Kai Yu
HIGHLIGHT:             Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first
stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance.

609, TITLE:            DRTS Parsing with Structure-Aware Encoding and Decoding
https://www.aclweb.org/anthology/2020.acl-main.609
AUTHORS:               Qiankun Fu, Yue Zhang, Jiangming Liu, Meishan Zhang
HIGHLIGHT:             In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the
structural information, where graph attention network (GAT) is exploited for effectively modeling.

610, TITLE:            A Two-Stage Masked LM Method for Term Set Expansion
https://www.aclweb.org/anthology/2020.acl-main.610
AUTHORS:               Guy Kushilevitz, Shaul Markovitch, Yoav Goldberg
HIGHLIGHT:             We harness the power of neural masked language models (MLM) and propose a novel TSE algorithm, which
combines the pattern-based and distributional approaches.

611, TITLE:              FLAT: Chinese NER Using Flat-Lattice Transformer
https://www.aclweb.org/anthology/2020.acl-main.611
AUTHORS:                 Xiaonan Li, Hang Yan, Xipeng Qiu, Xuanjing Huang
HIGHLIGHT:               In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice
structure into a flat structure consisting of spans.

612, TITLE:              Improving Entity Linking through Semantic Reinforced Entity Embeddings
https://www.aclweb.org/anthology/2020.acl-main.612
AUTHORS:                 Feng Hou, Ruili Wang, Jun He, Yi Zhou
HIGHLIGHT:               We propose a simple yet effective method, FGS2EE, to inject fine-grained semantic information into entity
embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality.

613, TITLE:              Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain
https://www.aclweb.org/anthology/2020.acl-main.613
AUTHORS:                 Shadi Saleh, Pavel Pecina
HIGHLIGHT:               We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval:
document translation (DT) and query translation (QT).

614, TITLE:              Learning Robust Models for e-Commerce Product Search
https://www.aclweb.org/anthology/2020.acl-main.614
AUTHORS:                 Thanh Nguyen, Nikhil Rao, Karthik Subbian
HIGHLIGHT:               In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to
generate hard mismatched examples to improve the classifier.

615, TITLE:              Generalized Entropy Regularization or: There's Nothing Special about Label Smoothing
https://www.aclweb.org/anthology/2020.acl-main.615
AUTHORS:                 Clara Meister, Elizabeth Salesky, Ryan Cotterell
HIGHLIGHT:               We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and
use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation
tasks.

616, TITLE:              Highway Transformer: Self-Gating Enhanced Self-Attentive Networks
https://www.aclweb.org/anthology/2020.acl-main.616
AUTHORS:                 Yekun Chai, Shuo Jin, Xinwen Hou
HIGHLIGHT:               Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that
incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of
individual representations.

617, TITLE:              Low-Dimensional Hyperbolic Knowledge Graph Embeddings
https://www.aclweb.org/anthology/2020.acl-main.617
AUTHORS:                 Ines Chami, Adva Wolf, Da-Cheng Juan, Frederic Sala, Sujith Ravi, Christopher R&eacute;
HIGHLIGHT:               In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical
and logical patterns.

618, TITLE:              Classification-Based Self-Learning for Weakly Supervised Bilingual Lexicon Induction
https://www.aclweb.org/anthology/2020.acl-main.618
AUTHORS:                 Mladen Karan, Ivan VuliÄ‡, Anna Korhonen, Goran GlavaÅ¡
HIGHLIGHT:               In this work, we present ClassyMap, a classification-based approach to self-learning, yielding a more robust and
a more effective induction of projection-based CLWEs.

619, TITLE:              Gender in Danger? Evaluating Speech Translation Technology on the MuST-SHE Corpus
https://www.aclweb.org/anthology/2020.acl-main.619
AUTHORS:                 Luisa Bentivogli, Beatrice Savoldi, Matteo Negri, Mattia A. Di Gangi, Roldano Cattoni, Marco Turchi
HIGHLIGHT:               We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release
of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language
directions (English-Italian/French).

620, TITLE:              Uncertainty-Aware Curriculum Learning for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.620
AUTHORS:                 Yikai Zhou, Baosong Yang, Derek F. Wong, Yu Wan, Lidia S. Chao
HIGHLIGHT:               We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the
uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model
uncertainty indicates the completeness of current training stage.

621, TITLE:              Closing the Gap: Joint De-Identification and Concept Extraction in the Clinical Domain
https://www.aclweb.org/anthology/2020.acl-main.621
AUTHORS:                 Lukas Lange, Heike Adel, Jannik Str&ouml;tgen
HIGHLIGHT:               In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data
and investigating joint models for de-identification and concept extraction.

622, TITLE:              CorefQA: Coreference Resolution as Query-based Span Prediction
https://www.aclweb.org/anthology/2020.acl-main.622
AUTHORS:                 Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, Jiwei Li
HIGHLIGHT:               In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task.

623, TITLE:              Estimating predictive uncertainty for rumour verification models
https://www.aclweb.org/anthology/2020.acl-main.623
AUTHORS:                 Elena Kochkina, Maria Liakata
HIGHLIGHT:               We propose two methods for uncertainty-based instance rejection, supervised and unsupervised.

624, TITLE:              From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains
https://www.aclweb.org/anthology/2020.acl-main.624
AUTHORS:                 Jan-Christoph Klie, Richard Eckart de Castilho, Iryna Gurevych
HIGHLIGHT:               We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders
that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less
tedious for users.

625, TITLE:              Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions
https://www.aclweb.org/anthology/2020.acl-main.625
AUTHORS:                 Tian Jin, Zhun Liu, Shengjia Yan, Alexandre Eichenberger, Louis-Philippe Morency
HIGHLIGHT:               In this paper, we propose \textbf{N3} (\textbf{N}eural \textbf{N}etworks from \textbf{N}atural Language) - a
new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model.

626, TITLE:              Controlled Crowdsourcing for High-Quality QA-SRL Annotation
https://www.aclweb.org/anthology/2020.acl-main.626
AUTHORS:                 Paul Roit, Ayal Klein, Daniela Stepanov, Jonathan Mamou, Julian Michael, Gabriel Stanovsky, Luke
Zettlemoyer, Ido Dagan
HIGHLIGHT:               In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving
worker selection and training, and a data consolidation phase.

627, TITLE:              Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus
https://www.aclweb.org/anthology/2020.acl-main.627
AUTHORS:                 Hao Fei, Meishan Zhang, Donghong Ji
HIGHLIGHT:               In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training
datasets for the target languages from the source gold-standard SRL annotations.

628, TITLE:              Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity
https://www.aclweb.org/anthology/2020.acl-main.628
AUTHORS:                 Nina Poerner, Ulli Waltinger, Hinrich Sch&uuml;tze
HIGHLIGHT:               We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained
sentence encoders into sentence meta-embeddings.

629, TITLE:              Transition-based Semantic Dependency Parsing with Pointer Networks
https://www.aclweb.org/anthology/2020.acl-main.629
AUTHORS:                 Daniel Fern&aacute;ndez-Gonz&aacute;lez, Carlos G&oacute;mez-Rodr&iacute;guez
HIGHLIGHT:               In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose
a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform
semantic dependency parsing.

630, TITLE:              tBERT: Topic Models and BERT Joining Forces for Semantic Similarity Detection
https://www.aclweb.org/anthology/2020.acl-main.630
AUTHORS:                Nicole Peinelt, Dong Nguyen, Maria Liakata
HIGHLIGHT:              We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and
show that our model improves performance over strong neural baselines across a variety of English language datasets.

631, TITLE:             Conditional Augmentation for Aspect Term Extraction via Masked Sequence-to-Sequence Generation
https://www.aclweb.org/anthology/2020.acl-main.631
AUTHORS:                Kun Li, Chengbo Chen, Xiaojun Quan, Qing Ling, Yan Song
HIGHLIGHT:              In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence
while preserving the original opinion targets and labels.

632, TITLE:             Exploiting Personal Characteristics of Debaters for Predicting Persuasiveness
https://www.aclweb.org/anthology/2020.acl-main.632
AUTHORS:                Khalid Al Khatib, Michael V&ouml;lske, Shahbaz Syed, Nikolay Kolyada, Benno Stein
HIGHLIGHT:              In this paper, we model debaters' prior beliefs, interests, and personality traits based on their previous activity,
without dependence on explicit user profiles or questionnaires.

633, TITLE:             Out of the Echo Chamber: Detecting Countering Debate Speeches
https://www.aclweb.org/anthology/2020.acl-main.633
AUTHORS:                Matan Orbach, Yonatan Bilu, Assaf Toledo, Dan Lahav, Michal Jacovi, Ranit Aharonov, Noam Slonim
HIGHLIGHT:              Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing
stance, the ones that directly counter it.

634, TITLE:             Diversifying Dialogue Generation with Non-Conversational Text
https://www.aclweb.org/anthology/2020.acl-main.634
AUTHORS:                Hui Su, Xiaoyu Shen, Sanqiang Zhao, Zhou Xiao, Pengwei Hu, randy zhong, Cheng Niu, Jie Zhou
HIGHLIGHT:              In this paper, we propose a new perspective to diversify dialogue generation by leveraging \textit{non-
conversational} text.

635, TITLE:             KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation
https://www.aclweb.org/anthology/2020.acl-main.635
AUTHORS:                Hao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang, Xiaoyan Zhu
HIGHLIGHT:              In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which
grounds the topics in multi-turn conversations to knowledge graphs.

636, TITLE:             Meta-Reinforced Multi-Domain State Generator for Dialogue Systems
https://www.aclweb.org/anthology/2020.acl-main.636
AUTHORS:                Yi Huang, Junlan Feng, Min Hu, Xiaoting Wu, Xiaoyu Du, Shuo Ma
HIGHLIGHT:              In this paper, we propose a Meta-Reinforced Multi-Domain State Generator (MERET).

637, TITLE:             Modeling Long Context for Task-Oriented Dialogue State Generation
https://www.aclweb.org/anthology/2020.acl-main.637
AUTHORS:                Jun Quan, Deyi Xiong
HIGHLIGHT:              Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states
from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging
technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation.

638, TITLE:             Multi-Domain Dialogue Acts and Response Co-Generation
https://www.aclweb.org/anthology/2020.acl-main.638
AUTHORS:                Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, Jianxing Yu
HIGHLIGHT:              To address these issues, we propose a neural co-generation model that generates dialogue acts and responses
concurrently.

639, TITLE:             Exploring Contextual Word-level Style Relevance for Unsupervised Style Transfer
https://www.aclweb.org/anthology/2020.acl-main.639
AUTHORS:                Chulun Zhou, Liangyu Chen, Jiachen Liu, Xinyan Xiao, Jinsong Su, Sheng Guo, Hua Wu
HIGHLIGHT:              In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits
the relevance of each output word to the target style for unsupervised style transfer.

640, TITLE:             Heterogeneous Graph Transformer for Graph-to-Sequence Learning
https://www.aclweb.org/anthology/2020.acl-main.640
AUTHORS:               Shaowei Yao, Tianming Wang, Xiaojun Wan
HIGHLIGHT:             In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations
in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between
nodes.

641, TITLE:            Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence
https://www.aclweb.org/anthology/2020.acl-main.641
AUTHORS:               Xiaoyu Shen, Ernie Chang, Hui Su, Cheng Niu, Dietrich Klakow
HIGHLIGHT:             To address this concern, we propose to explicitly segment target text into fragment units and align them with
their data correspondences.

642, TITLE:            Aligned Dual Channel Graph Convolutional Network for Visual Question Answering
https://www.aclweb.org/anthology/2020.acl-main.642
AUTHORS:               Qingbao Huang, Jielong Wei, Yi Cai, Changmeng Zheng, Junying Chen, Ho-fung Leung, Qing Li
HIGHLIGHT:             To simultaneously capture the relations between objects in an image and the syntactic dependency relations
between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual
and textual advantages.

643, TITLE:            Multimodal Neural Graph Memory Networks for Visual Question Answering
https://www.aclweb.org/anthology/2020.acl-main.643
AUTHORS:               Mahmoud Khademi
HIGHLIGHT:             We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for
visual question answering.

644, TITLE:            Refer360$^circ$: A Referring Expression Recognition Dataset in 360$^circ$ Images
https://www.aclweb.org/anthology/2020.acl-main.644
AUTHORS:               Volkan Cirik, Taylor Berg-Kirkpatrick, Louis-Philippe Morency
HIGHLIGHT:             We propose a novel large-scale referring expression recognition dataset, Refer360{\mbox{$^\circ$}},
consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360{\mbox{$^\circ$}}
scenes.

645, TITLE:            CamemBERT: a Tasty French Language Model
https://www.aclweb.org/anthology/2020.acl-main.645
AUTHORS:               Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su&aacute;rez, Yoann Dupont, Laurent Romary,
&Eacute;ric de la Clergerie, Djam&eacute; Seddah, Beno&icirc;t Sagot
HIGHLIGHT:             In this paper, we investigate the feasibility of training monolingual Transformer-based language models for
other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing,
named entity recognition and natural language inference tasks.

646, TITLE:            Effective Estimation of Deep Generative Language Models
https://www.aclweb.org/anthology/2020.acl-main.646
AUTHORS:               Tom Pelsmaeker, Wilker Aziz
HIGHLIGHT:             We concentrate on one such model, the variational auto-encoder, which we argue is an important building block
in hierarchical probabilistic models of language.

647, TITLE:            Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection
https://www.aclweb.org/anthology/2020.acl-main.647
AUTHORS:               Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg
HIGHLIGHT:             We present Iterative Null-space Projection (INLP), a novel method for removing information from neural
representations.

648, TITLE:            2kenize: Tying Subword Sequences for Chinese Script Conversion
https://www.aclweb.org/anthology/2020.acl-main.648
AUTHORS:               Pranav A, Isabelle Augenstein
HIGHLIGHT:             Here, we propose a model that can disambiguate between mappings and convert between the two scripts.

649, TITLE:            Predicting the Growth of Morphological Families from Social and Linguistic Factors
https://www.aclweb.org/anthology/2020.acl-main.649
AUTHORS:               Valentin Hofmann, Janet Pierrehumbert, Hinrich Sch&uuml;tze
HIGHLIGHT:               We present the first study that examines the evolution of morphological families, i.e., sets of morphologically
related words such as "trump", "antitrumpism", and "detrumpify", in social media.

650, TITLE:              Semi-supervised Contextual Historical Text Normalization
https://www.aclweb.org/anthology/2020.acl-main.650
AUTHORS:                 Peter Makarov, Simon Clematide
HIGHLIGHT:               By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-
side language model, we train accurate models with unlabeled historical data.

651, TITLE:              ClarQ: A large-scale and diverse dataset for Clarification Question Generation
https://www.aclweb.org/anthology/2020.acl-main.651
AUTHORS:                 Vaibhav Kumar, Alan W Black
HIGHLIGHT:               In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision)
that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from
stackexchange.

652, TITLE:              DoQA - Accessing Domain-Specific FAQs via Conversational QA
https://www.aclweb.org/anthology/2020.acl-main.652
AUTHORS:                 Jon Ander Campos, Arantxa Otegi, Aitor Soroa, Jan Deriu, Mark Cieliebak, Eneko Agirre
HIGHLIGHT:               The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of
domain-specific information available in FAQ sites.

653, TITLE:              MLQA: Evaluating Cross-lingual Extractive Question Answering
https://www.aclweb.org/anthology/2020.acl-main.653
AUTHORS:                 Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, Holger Schwenk
HIGHLIGHT:               We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this
area.

654, TITLE:              Multi-source Meta Transfer for Low Resource Multiple-Choice Question Answering
https://www.aclweb.org/anthology/2020.acl-main.654
AUTHORS:                 Ming Yan, Hao Zhang, Di Jin, Joey Tianyi Zhou
HIGHLIGHT:               To address this challenge, we propose a multi-source meta transfer (MMT) for low-resource MCQA.

655, TITLE:              Fine-grained Fact Verification with Kernel Graph Attention Network
https://www.aclweb.org/anthology/2020.acl-main.655
AUTHORS:                 Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu
HIGHLIGHT:               This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact
verification with kernel-based attentions.

656, TITLE:              Generating Fact Checking Explanations
https://www.aclweb.org/anthology/2020.acl-main.656
AUTHORS:                 Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein
HIGHLIGHT:               This paper provides the first study of how these explanations can be generated automatically based on available
claim context, and how this task can be modelled jointly with veracity prediction.

657, TITLE:              Premise Selection in Natural Language Mathematical Texts
https://www.aclweb.org/anthology/2020.acl-main.657
AUTHORS:                 Deborah Ferreira, Andr&eacute; Freitas
HIGHLIGHT:               We propose an approach to solve this task as a link prediction problem, using Deep Convolutional Graph
Neural Networks.

658, TITLE:              A Call for More Rigor in Unsupervised Cross-lingual Learning
https://www.aclweb.org/anthology/2020.acl-main.658
AUTHORS:                 Mikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka Labaka, Eneko Agirre
HIGHLIGHT:               We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and
call for a more rigorous position in each of them.

659, TITLE:              A Tale of a Probe and a Parser
https://www.aclweb.org/anthology/2020.acl-main.659
AUTHORS:                 Rowan Hall Maudslay, Josef Valvoda, Tiago Pimentel, Adina Williams, Ryan Cotterell
HIGHLIGHT:              To explore whether syntactic probes would do better to make use of existing techniques, we compare the
structural probe to a more traditional parser with an identical lightweight parameterisation.

660, TITLE:             From SPMRL to NMRL: What Did We Learn (and Unlearn) in a Decade of Parsing Morphologically-Rich
Languages (MRLs)?
https://www.aclweb.org/anthology/2020.acl-main.660
AUTHORS:                Reut Tsarfaty, Dan Bareket, Stav Klein, Amit Seker
HIGHLIGHT:              Here we reflect on parsing MRLs in that decade, highlight the solutions and lessons learned for the
architectural, modeling and lexical challenges in the pre-neural era, and argue that similar challenges re-emerge in neural architectures
for MRLs.

661, TITLE:             Speech Translation and the End-to-End Promise: Taking Stock of Where We Are
https://www.aclweb.org/anthology/2020.acl-main.661
AUTHORS:                Matthias Sperber, Matthias Paulik
HIGHLIGHT:              This paper provides a unifying categorization and nomenclature that covers both traditional and recent
approaches and that may help researchers by highlighting both trade-offs and open research questions.

662, TITLE:             What Question Answering can Learn from Trivia Nerds
https://www.aclweb.org/anthology/2020.acl-main.662
AUTHORS:                Jordan Boyd-Graber, Benjamin B&ouml;rschinger
HIGHLIGHT:              We argue that creating a QA dataset-and the ubiquitous leaderboard that goes with it-closely resembles running
a trivia tournament: you write questions, have agents (either humans or machines) answer the questions, and declare a winner.

663, TITLE:             What are the Goals of Distributional Semantics?
https://www.aclweb.org/anthology/2020.acl-main.663
AUTHORS:                Guy Emerson
HIGHLIGHT:              In this paper, I take a broad linguistic perspective, looking at how well current models can deal with various
semantic challenges.

664, TITLE:             Improving Image Captioning with Better Use of Caption
https://www.aclweb.org/anthology/2020.acl-main.664
AUTHORS:                Zhan Shi, Xu Zhou, Xipeng Qiu, Xiaodan Zhu
HIGHLIGHT:              In this paper, we present a novel image captioning architecture to better explore semantics available in captions
and leverage that to enhance both image representation and caption generation.

665, TITLE:             Shape of Synth to Come: Why We Should Use Synthetic Data for English Surface Realization
https://www.aclweb.org/anthology/2020.acl-main.665
AUTHORS:                Henry Elder, Robert Burke, Alexander Oâ€™Connor, Jennifer Foster
HIGHLIGHT:              We analyse the effects of synthetic data, and we argue that its use should be encouraged rather than prohibited
so that future research efforts continue to explore systems that can take advantage of such data.

666, TITLE:             Toward Better Storylines with Sentence-Level Language Models
https://www.aclweb.org/anthology/2020.acl-main.666
AUTHORS:                Daphne Ippolito, David Grangier, Douglas Eck, Chris Callison-Burch
HIGHLIGHT:              We propose a sentence-level language model which selects the next sentence in a story from a finite set of
fluent alternatives.

667, TITLE:             A Two-Step Approach for Implicit Event Argument Detection
https://www.aclweb.org/anthology/2020.acl-main.667
AUTHORS:                Zhisong Zhang, Xiang Kong, Zhengzhong Liu, Xuezhe Ma, Eduard Hovy
HIGHLIGHT:              In this work, we explore the implicit event argument detection task, which studies event arguments beyond
sentence boundaries.

668, TITLE:             Machine Reading of Historical Events
https://www.aclweb.org/anthology/2020.acl-main.668
AUTHORS:                Or Honovich, Lucas Torroba Hennigen, Omri Abend, Shay B. Cohen
HIGHLIGHT:              Within this broad framework, we address the task of machine reading the time of historical events, compile
datasets for the task, and develop a model for tackling it.

669, TITLE:             Revisiting Unsupervised Relation Extraction
https://www.aclweb.org/anthology/2020.acl-main.669
AUTHORS:                 Thy Thy Tran, Phong Le, Sophia Ananiadou
HIGHLIGHT:               However, we demonstrate that by using only named entities to induce relation types, we can outperform
existing methods on two popular datasets.

670, TITLE:              SciREX: A Challenge Dataset for Document-Level Information Extraction
https://www.aclweb.org/anthology/2020.acl-main.670
AUTHORS:                 Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, Iz Beltagy
HIGHLIGHT:               In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including
salient entity identification and document level N-ary relation identification from scientific articles.

671, TITLE:              Contrastive Self-Supervised Learning for Commonsense Reasoning
https://www.aclweb.org/anthology/2020.acl-main.671
AUTHORS:                 Tassilo Klein, Moin Nabi
HIGHLIGHT:               We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge
problems.

672, TITLE:              Do Transformers Need Deep Long-Range Memory?
https://www.aclweb.org/anthology/2020.acl-main.672
AUTHORS:                 Jack Rae, Ali Razavi
HIGHLIGHT:               We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long
range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.

673, TITLE:              Improving Disentangled Text Representation Learning with Information-Theoretic Guidance
https://www.aclweb.org/anthology/2020.acl-main.673
AUTHORS:                 Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li, Lawrence
Carin
HIGHLIGHT:               Inspired by information theory, we propose a novel method that effectively manifests disentangled
representations of text, without any supervision on semantics.

674, TITLE:              Understanding Advertisements with BERT
https://www.aclweb.org/anthology/2020.acl-main.674
AUTHORS:                 Kanika Kalra, Bhargav Kurma, Silpa Vadakkeeveetil Sreelatha, Manasi Patwardhan, Shirish Karande
HIGHLIGHT:               We consider a task based on CVPR 2018 challenge dataset on advertisement (Ad) understanding. The task
involves detecting the viewer’s interpretation of an Ad image captured as text.

675, TITLE:              Non-Linear Instance-Based Cross-Lingual Mapping for Non-Isomorphic Embedding Spaces
https://www.aclweb.org/anthology/2020.acl-main.675
AUTHORS:                 Goran GlavaÅ¡, Ivan VuliÄ‡
HIGHLIGHT:               We present InstaMap, an instance-based method for learning projection-based cross-lingual word embeddings.

676, TITLE:              Good-Enough Compositional Data Augmentation
https://www.aclweb.org/anthology/2020.acl-main.676
AUTHORS:                 Jacob Andreas
HIGHLIGHT:               We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in
conditional and unconditional sequence models.

677, TITLE:              RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers
https://www.aclweb.org/anthology/2020.acl-main.677
AUTHORS:                 Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, Matthew Richardson
HIGHLIGHT:               We present a unified framework, based on the relation-aware self-attention mechanism, to address schema
encoding, schema linking, and feature representation within a text-to-SQL encoder.

678, TITLE:              Temporal Common Sense Acquisition with Minimal Supervision
https://www.aclweb.org/anthology/2020.acl-main.678
AUTHORS:                 Ben Zhou, Qiang Ning, Daniel Khashabi, Dan Roth
HIGHLIGHT:               This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of
temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model.

679, TITLE:              The Sensitivity of Language Models and Humans to Winograd Schema Perturbations
https://www.aclweb.org/anthology/2020.acl-main.679
AUTHORS:               Mostafa Abdou, Vinit Ravishankar, Maria Barrett, Yonatan Belinkov, Desmond Elliott, Anders S&oslash;gaard
HIGHLIGHT:             Our results highlight interesting differences between humans and language models: language models are more
sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their
predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones.

680, TITLE:            Temporally-Informed Analysis of Named Entity Recognition
https://www.aclweb.org/anthology/2020.acl-main.680
AUTHORS:               Shruti Rijhwani, Daniel Preotiuc-Pietro
HIGHLIGHT:             We analyze and propose methods that make better use of temporally-diverse training data, with a focus on the
task of named entity recognition.
To support these experiments, we introduce a novel data set of English tweets annotated with named entities.

681, TITLE:            Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation
https://www.aclweb.org/anthology/2020.acl-main.681
AUTHORS:               Aakanksha Naik, Carolyn Rose
HIGHLIGHT:             We tackle the task of building supervised event trigger identification models which can generalize better across
domains.

682, TITLE:            CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning
https://www.aclweb.org/anthology/2020.acl-main.682
AUTHORS:               Alessandro Suglia, Ioannis Konstas, Andrea Vanzo, Emanuele Bastianelli, Desmond Elliott, Stella Frank,
Oliver Lemon
HIGHLIGHT:             To remedy this, we present GroLLA, an evaluation framework for Grounded Language Learning with
Attributes based on three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot
evaluation.

683, TITLE:            Cross-Modality Relevance for Reasoning on Language and Vision
https://www.aclweb.org/anthology/2020.acl-main.683
AUTHORS:               Chen Zheng, Quan Guo, Parisa Kordjamshidi
HIGHLIGHT:             This work deals with the challenge of learning and reasoning over language and vision data for the related
downstream tasks such as visual question answering (VQA) and natural language for visual reasoning (NLVR).

684, TITLE:            Learning Web-based Procedures by Reasoning over Explanations and Demonstrations in Context
https://www.aclweb.org/anthology/2020.acl-main.684
AUTHORS:               Shashank Srivastava, Oleksandr Polozov, Nebojsa Jojic, Christopher Meek
HIGHLIGHT:             We explore learning web-based tasks from a human teacher through natural language explanations and a single
demonstration.

685, TITLE:            Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language
Learning
https://www.aclweb.org/anthology/2020.acl-main.685
AUTHORS:               Angeliki Lazaridou, Anna Potapenko, Olivier Tieleman
HIGHLIGHT:             We present a method for combining multi-agent communication and traditional data-driven approaches to
natural language learning, with an end goal of teaching agents to communicate with humans in natural language.

686, TITLE:            HAT: Hardware-Aware Transformers for Efficient Natural Language Processing
https://www.aclweb.org/anthology/2020.acl-main.686
AUTHORS:               Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han
HIGHLIGHT:             To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-
Aware Transformers (HAT) with neural architecture search.

687, TITLE:            Hard-Coded Gaussian Attention for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.687
AUTHORS:               Weiqiu You, Simeng Sun, Mohit Iyyer
HIGHLIGHT:             We push further in this direction by developing a “hard-coded” attention variant without any learned
parameters.

688, TITLE:            In Neural Machine Translation, What Does Transfer Learning Transfer?
https://www.aclweb.org/anthology/2020.acl-main.688
AUTHORS:               Alham Fikri Aji, Nikolay Bogoychev, Kenneth Heafield, Rico Sennrich
HIGHLIGHT:             We perform several ablation studies that limit information transfer, then measure the quality impact across three
language pairs to gain a black-box understanding of transfer learning.

689, TITLE:            Learning a Multi-Domain Curriculum for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.689
AUTHORS:               Wei Wang, Ye Tian, Jiquan Ngiam, Yinfei Yang, Isaac Caswell, Zarana Parekh
HIGHLIGHT:             This is achieved by carefully introducing instance-level domain-relevance features and automatically
constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches.

690, TITLE:            Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem
https://www.aclweb.org/anthology/2020.acl-main.690
AUTHORS:               Danielle Saunders, Bill Byrne
HIGHLIGHT:             At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky
et al, 2019 on WinoMT with no degradation of general test set BLEU.

691, TITLE:            Translationese as a Language in ``Multilingual'' NMT
https://www.aclweb.org/anthology/2020.acl-main.691
AUTHORS:               Parker Riley, Isaac Caswell, Markus Freitag, David Grangier
HIGHLIGHT:             Motivated by this, we model translationese and original (i.e. natural) text as separate languages in a multilingual
model, and pose the question: can we perform zero-shot translation between original source text and original target text?

692, TITLE:            Unsupervised Domain Clusters in Pretrained Language Models
https://www.aclweb.org/anthology/2020.acl-main.692
AUTHORS:               Roee Aharoni, Yoav Goldberg
HIGHLIGHT:             We harness this property and propose domain data selection methods based on such models, which require only
a small set of in-domain monolingual data.

693, TITLE:            Using Context in Neural Machine Translation Training Objectives
https://www.aclweb.org/anthology/2020.acl-main.693
AUTHORS:               Danielle Saunders, Felix Stahlberg, Bill Byrne
HIGHLIGHT:             We present Neural Machine Translation (NMT) training using document-level metrics with batch-level
documents.

694, TITLE:            Variational Neural Machine Translation with Normalizing Flows
https://www.aclweb.org/anthology/2020.acl-main.694
AUTHORS:               Hendra Setiawan, Matthias Sperber, Udhyakumar Nallasamy, Matthias Paulik
HIGHLIGHT:             In this paper, we propose to apply the VNMT framework to the state-of-the-art Transformer and introduce a
more flexible approximate posterior based on normalizing flows.

695, TITLE:            The Paradigm Discovery Problem
https://www.aclweb.org/anthology/2020.acl-main.695
AUTHORS:               Alexander Erdmann, Micha Elsner, Shijie Wu, Ryan Cotterell, Nizar Habash
HIGHLIGHT:             This work treats the paradigm discovery problem (PDP), the task of learning an inflectional morphological
system from unannotated sentences.

696, TITLE:            Supervised Grapheme-to-Phoneme Conversion of Orthographic Schwas in Hindi and Punjabi
https://www.aclweb.org/anthology/2020.acl-main.696
AUTHORS:               Aryaman Arora, Luke Gessler, Nathan Schneider
HIGHLIGHT:             We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the
input and outperforms previous approaches.

697, TITLE:            Automated Evaluation of Writing -- 50 Years and Counting
https://www.aclweb.org/anthology/2020.acl-main.697
AUTHORS:               Beata Beigman Klebanov, Nitin Madnani
HIGHLIGHT:             In this theme paper, we focus on Automated Writing Evaluation (AWE), using Ellis Page's seminal 1966 paper
to frame the presentation.

698, TITLE:            Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly
https://www.aclweb.org/anthology/2020.acl-main.698
AUTHORS:               Nora Kassner, Hinrich Sch&uuml;tze
HIGHLIGHT:             Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in
Pretrained Language Models (PLMs).

699, TITLE:            On Forgetting to Cite Older Papers: An Analysis of the ACL Anthology
https://www.aclweb.org/anthology/2020.acl-main.699
AUTHORS:               Marcel Bollmann, Desmond Elliott
HIGHLIGHT:             In this paper, we address this question through bibliographic analysis.

700, TITLE:            Returning the N to NLP: Towards Contextually Personalized Classification Models
https://www.aclweb.org/anthology/2020.acl-main.700
AUTHORS:               Lucie Flek
HIGHLIGHT:             This paper surveys the landscape of personalization in natural language processing and related fields, and offers
a path forward to mitigate the decades of deviation of the NLP tools from sociolingustic findings, allowing to flexibly process the
“natural” language of each user rather than enforcing a uniform NLP treatment.

701, TITLE:            To Test Machine Comprehension, Start by Defining Comprehension
https://www.aclweb.org/anthology/2020.acl-main.701
AUTHORS:               Jesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, Dave Ferrucci
HIGHLIGHT:             First, we argue that existing approaches do not adequately define comprehension; they are too unsystematic
about what content is tested. Second, we present a detailed definition of comprehension—a “Template of Understanding”—for a
widely useful class of texts, namely short narratives.

702, TITLE:            Gender Gap in Natural Language Processing Research: Disparities in Authorship and Citations
https://www.aclweb.org/anthology/2020.acl-main.702
AUTHORS:               Saif M. Mohammad
HIGHLIGHT:             In this work, we examine female first author percentages and the citations to their papers in Natural Language
Processing.

703, TITLE:            BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and
Comprehension
https://www.aclweb.org/anthology/2020.acl-main.703
AUTHORS:               Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin
Stoyanov, Luke Zettlemoyer
HIGHLIGHT:             We present BART, a denoising autoencoder for pretraining sequence-to-sequence models.

704, TITLE:            BLEURT: Learning Robust Metrics for Text Generation
https://www.aclweb.org/anthology/2020.acl-main.704
AUTHORS:               Thibault Sellam, Dipanjan Das, Ankur Parikh
HIGHLIGHT:             We propose BLEURT, a learned evaluation metric for English based on BERT.

705, TITLE:            Distilling Knowledge Learned in BERT for Text Generation
https://www.aclweb.org/anthology/2020.acl-main.705
AUTHORS:               Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu
HIGHLIGHT:             In this paper, we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the
finetuning of BERT on target generation tasks.

706, TITLE:            ESPRIT: Explaining Solutions to Physical Reasoning Tasks
https://www.aclweb.org/anthology/2020.acl-main.706
AUTHORS:               Nazneen Fatema Rajani, Rui Zhang, Yi Chern Tan, Stephan Zheng, Jeremy Weiss, Aadit Vyas, Abhijit Gupta,
Caiming Xiong, Richard Socher, Dragomir Radev
HIGHLIGHT:             We propose ESPRIT, a framework for commonsense reasoning about qualitative physics in natural language
that generates interpretable descriptions of physical events.

707, TITLE:            Iterative Edit-Based Unsupervised Sentence Simplification
https://www.aclweb.org/anthology/2020.acl-main.707
AUTHORS:               Dhruv Kumar, Lili Mou, Lukasz Golab, Olga Vechtomova
HIGHLIGHT:             We present a novel iterative, edit-based approach to unsupervised sentence simplification.

708, TITLE:            Logical Natural Language Generation from Open-Domain Tables
https://www.aclweb.org/anthology/2020.acl-main.708
AUTHORS:                Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, William Yang Wang
HIGHLIGHT:              In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements
that can be \textit{logically entailed} by the facts in an open-domain semi-structured table.

709, TITLE:              Neural CRF Model for Sentence Alignment in Text Simplification
https://www.aclweb.org/anthology/2020.acl-main.709
AUTHORS:                Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, Wei Xu
HIGHLIGHT:              We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in
parallel documents but also utilizes a neural sentence pair model to capture semantic similarity.

710, TITLE:              One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases
https://www.aclweb.org/anthology/2020.acl-main.710
AUTHORS:                Xingdi Yuan, Tong Wang, Rui Meng, Khushboo Thaker, Peter Brusilovsky, Daqing He, Adam Trischler
HIGHLIGHT:              In this study, we address this problem from both modeling and evaluation perspectives.

711, TITLE:              R^3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge
https://www.aclweb.org/anthology/2020.acl-main.711
AUTHORS:                Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, Nanyun Peng
HIGHLIGHT:              We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence.

712, TITLE:              Structural Information Preserving for Graph-to-Text Generation
https://www.aclweb.org/anthology/2020.acl-main.712
AUTHORS:                Linfeng Song, Ante Wang, Jinsong Su, Yue Zhang, Kun Xu, Yubin Ge, Dong Yu
HIGHLIGHT:              We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving
input information.

713, TITLE:              A Joint Neural Model for Information Extraction with Global Features
https://www.aclweb.org/anthology/2020.acl-main.713
AUTHORS:                Ying Lin, Heng Ji, Fei Huang, Lingfei Wu
HIGHLIGHT:              In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural
framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence.

714, TITLE:              Document-Level Event Role Filler Extraction using Multi-Granularity Contextualized Encoding
https://www.aclweb.org/anthology/2020.acl-main.714
AUTHORS:                Xinya Du, Claire Cardie
HIGHLIGHT:              To dynamically aggregate information captured by neural representations learned at different levels of
granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader.

715, TITLE:              Exploiting the Syntax-Model Consistency for Neural Relation Extraction
https://www.aclweb.org/anthology/2020.acl-main.715
AUTHORS:                Amir Pouran Ben Veyseh, Franck Dernoncourt, Dejing Dou, Thien Huu Nguyen
HIGHLIGHT:              In order to overcome these issues, we propose a novel deep learning model for RE that uses the dependency
trees to extract the syntax-based importance scores for the words, serving as a tree representation to introduce syntactic information
into the models with greater generalization.

716, TITLE:              From English to Code-Switching: Transfer Learning with Strong Morphological Clues
https://www.aclweb.org/anthology/2020.acl-main.716
AUTHORS:                Gustavo Aguilar, Thamar Solorio
HIGHLIGHT:              In this paper, we aim at adapting monolingual models to code-switched text in various tasks.

717, TITLE:              Learning Interpretable Relationships between Entities, Relations and Concepts via Bayesian Structure Learning
on Open Domain Facts
https://www.aclweb.org/anthology/2020.acl-main.717
AUTHORS:                Jingyuan Zhang, Mingming Sun, Yue Feng, Ping Li
HIGHLIGHT:              In this paper, we propose the task of learning interpretable relationships from open-domain facts to enrich and
refine concept graphs.

718, TITLE:              Multi-Sentence Argument Linking
https://www.aclweb.org/anthology/2020.acl-main.718
AUTHORS:                Seth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, Benjamin Van Durme
HIGHLIGHT:             We present a novel document-level model for finding argument spans that fill an event's roles, connecting
related ideas in sentence-level semantic role labeling and coreference resolution.

719, TITLE:            Rationalizing Medical Relation Prediction from Corpus-level Statistics
https://www.aclweb.org/anthology/2020.acl-main.719
AUTHORS:               Zhen Wang, Jennifer Lee, Simon Lin, Huan Sun
HIGHLIGHT:             Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable
framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition.

720, TITLE:            Sources of Transfer in Multilingual Named Entity Recognition
https://www.aclweb.org/anthology/2020.acl-main.720
AUTHORS:               David Mueller, Nicholas Andrews, Mark Dredze
HIGHLIGHT:             To explain this phenomena, we explore the sources of multilingual transfer in polyglot NER models and
examine the weight structure of polyglot models compared to their monolingual counterparts.

721, TITLE:            ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured Webpages
https://www.aclweb.org/anthology/2020.acl-main.721
AUTHORS:               Colin Lockard, Prashant Shiralkar, Xin Luna Dong, Hannaneh Hajishirzi
HIGHLIGHT:             In this work, we propose a solution for "zero-shot" open-domain relation extraction from webpages with a
previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and
websites in entirely new subject verticals.

722, TITLE:            Soft Gazetteers for Low-Resource Named Entity Recognition
https://www.aclweb.org/anthology/2020.acl-main.722
AUTHORS:               Shruti Rijhwani, Shuyan Zhou, Graham Neubig, Jaime Carbonell
HIGHLIGHT:             To address this problem, we propose a method of "soft gazetteers" that incorporates ubiquitously available
information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual
entity linking.

723, TITLE:            A Prioritization Model for Suicidality Risk Assessment
https://www.aclweb.org/anthology/2020.acl-main.723
AUTHORS:               Han-Chin Shing, Philip Resnik, Douglas Oard
HIGHLIGHT:             Building on measures developed for resource-bounded document retrieval, we introduce a well founded
evaluation paradigm, and demonstrate using an expert-annotated test collection that meaningful improvements over plausible cascade
model baselines can be achieved using an approach that jointly ranks individuals and their social media posts.

724, TITLE:            CluHTM - Semantic Hierarchical Topic Modeling based on CluWords
https://www.aclweb.org/anthology/2020.acl-main.724
AUTHORS:               Felipe Viegas, Washington Cunha, Christian Gomes, Ant&ocirc;nio Pereira, Leonardo Rocha, Marcos
Goncalves
HIGHLIGHT:             In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a
novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM.

725, TITLE:            Empower Entity Set Expansion via Language Model Probing
https://www.aclweb.org/anthology/2020.acl-main.725
AUTHORS:               Yunyi Zhang, Jiaming Shen, Jingbo Shang, Jiawei Han
HIGHLIGHT:             In this study, we propose a novel iterative set expansion framework that leverages automatically generated class
names to address the semantic drift issue.

726, TITLE:            Feature Projection for Improved Text Classification
https://www.aclweb.org/anthology/2020.acl-main.726
AUTHORS:               Qi Qin, Wenpeng Hu, Bing Liu
HIGHLIGHT:             In this paper, we propose a novel angle to further improve this representation learning, i.e., feature projection.

727, TITLE:            A negative case analysis of visual grounding methods for VQA
https://www.aclweb.org/anthology/2020.acl-main.727
AUTHORS:               Robik Shrestha, Kushal Kafle, Christopher Kanan
HIGHLIGHT:             However, we show that the performance improvements are not a result of improved visual grounding, but a
regularization effect which prevents over-fitting to linguistic priors.

728, TITLE:            History for Visual Dialog: Do we really need it?
https://www.aclweb.org/anthology/2020.acl-main.728
AUTHORS:               Shubham Agarwal, Trung Bui, Joon-Young Lee, Ioannis Konstas, Verena Rieser
HIGHLIGHT:             In this paper, we show that co-attention models which explicitly encode dialoh history outperform models that
don't, achieving state-of-the-art performance (72 % NDCG on val set).

729, TITLE:            Mapping Natural Language Instructions to Mobile UI Action Sequences
https://www.aclweb.org/anthology/2020.acl-main.729
AUTHORS:               Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, Jason Baldridge
HIGHLIGHT:             We present a new problem: grounding natural language instructions to mobile user interface actions, and create
three new datasets for it.

730, TITLE:            TVQA+: Spatio-Temporal Grounding for Video Question Answering
https://www.aclweb.org/anthology/2020.acl-main.730
AUTHORS:               Jie Lei, Licheng Yu, Tamara Berg, Mohit Bansal
HIGHLIGHT:             We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to
simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language
questions about videos.

731, TITLE:            Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting
https://www.aclweb.org/anthology/2020.acl-main.731
AUTHORS:               Po-Yao Huang, Junjie Hu, Xiaojun Chang, Alexander Hauptmann
HIGHLIGHT:             In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space
alignment in unsupervised MMT.

732, TITLE:            A Multitask Learning Approach for Diacritic Restoration
https://www.aclweb.org/anthology/2020.acl-main.732
AUTHORS:               Sawsan Alqahtani, Ajay Mishra, Mona Diab
HIGHLIGHT:             Thus, to compensate for this loss, we investigate the use of multi-task learning to jointly optimize diacritic
restoration with related NLP problems namely word segmentation, part-of-speech tagging, and syntactic diacritization.

733, TITLE:            Frugal Paradigm Completion
https://www.aclweb.org/anthology/2020.acl-main.733
AUTHORS:               Alexander Erdmann, Tom Kenter, Markus Becker, Christian Schallhart
HIGHLIGHT:             We propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm
from as few manually provided forms as possible.

734, TITLE:            Improving Chinese Word Segmentation with Wordhood Memory Networks
https://www.aclweb.org/anthology/2020.acl-main.734
AUTHORS:               Yuanhe Tian, Yan Song, Fei Xia, Tong Zhang, Yonggang Wang
HIGHLIGHT:             In this paper, we therefore propose a neural framework, WMSeg, which uses memory networks to incorporate
wordhood information with several popular encoder-decoder combinations for CWS.

735, TITLE:            Joint Chinese Word Segmentation and Part-of-speech Tagging via Two-way Attentions of Auto-analyzed
Knowledge
https://www.aclweb.org/anthology/2020.acl-main.735
AUTHORS:               Yuanhe Tian, Yan Song, Xiang Ao, Fei Xia, Xiaojun Quan, Tong Zhang, Yonggang Wang
HIGHLIGHT:             In this paper, we propose a neural model named TwASP for joint CWS and POS tagging following the
character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and
their corresponding syntactic knowledge for each input character.

736, TITLE:            Joint Diacritization, Lemmatization, Normalization, and Fine-Grained Morphological Tagging
https://www.aclweb.org/anthology/2020.acl-main.736
AUTHORS:               Nasser Zalmout, Nizar Habash
HIGHLIGHT:             Our approach models the different features jointly, whether lexicalized (on the character-level), or non-
lexicalized (on the word-level).

737, TITLE:            Phonetic and Visual Priors for Decipherment of Informal Romanization
https://www.aclweb.org/anthology/2020.acl-main.737
AUTHORS:               Maria Ryskina, Matthew R. Gormley, Taylor Berg-Kirkpatrick
HIGHLIGHT:              We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from observed
romanized text in an unsupervised fashion.

738, TITLE:             Active Learning for Coreference Resolution using Discrete Annotation
https://www.aclweb.org/anthology/2020.acl-main.738
AUTHORS:                Belinda Z. Li, Gabriel Stanovsky, Luke Zettlemoyer
HIGHLIGHT:              We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to
identify mention antecedents if a presented mention pair is deemed not coreferent.

739, TITLE:             Beyond Possession Existence: Duration and Co-Possession
https://www.aclweb.org/anthology/2020.acl-main.739
AUTHORS:                Dhivya Chinnappa, Srikala Murugan, Eduardo Blanco
HIGHLIGHT:              This paper introduces two tasks: determining (a) the duration of possession relations and (b) co-possessions,
i.e., whether multiple possessors possess a possessee at the same time.

740, TITLE:             Don't Stop Pretraining: Adapt Language Models to Domains and Tasks
https://www.aclweb.org/anthology/2020.acl-main.740
AUTHORS:                Suchin Gururangan, Ana MarasoviÄ‡, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A.
Smith
HIGHLIGHT:              We present a study across four domains (biomedical and computer science publications, news, and reviews) and
eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance
gains, under both high- and low-resource settings.

741, TITLE:             Estimating Mutual Information Between Dense Word Embeddings
https://www.aclweb.org/anthology/2020.acl-main.741
AUTHORS:                Vitalii Zhelezniak, Aleksandar Savkov, Nils Hammerla
HIGHLIGHT:              In this work we go through a vast literature on estimating MI in such cases and single out the most promising
methods, yielding a simple and elegant similarity measure for word embeddings.

742, TITLE:             Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing
https://www.aclweb.org/anthology/2020.acl-main.742
AUTHORS:                Alane Suhr, Ming-Wei Chang, Peter Shaw, Kenton Lee
HIGHLIGHT:              We propose a challenging evaluation setup for cross-database semantic parsing, focusing on variation across
database schemas and in-domain language use.

743, TITLE:             Predicting the Focus of Negation: Model and Error Analysis
https://www.aclweb.org/anthology/2020.acl-main.743
AUTHORS:                Md Mosharaf Hossain, Kathleen Hamilton, Alexis Palmer, Eduardo Blanco
HIGHLIGHT:              In this paper, we experiment with neural networks to predict the focus of negation.

744, TITLE:             Structured Tuning for Semantic Role Labeling
https://www.aclweb.org/anthology/2020.acl-main.744
AUTHORS:                Tao Li, Parth Anand Jawale, Martha Palmer, Vivek Srikumar
HIGHLIGHT:              In this paper, we present a structured tuning framework to improve models using softened constraints only at
training time.

745, TITLE:             TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data
https://www.aclweb.org/anthology/2020.acl-main.745
AUTHORS:                Pengcheng Yin, Graham Neubig, Wen-tau Yih, Sebastian Riedel
HIGHLIGHT:              In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and
(semi-)structured tables.

746, TITLE:             Universal Decompositional Semantic Parsing
https://www.aclweb.org/anthology/2020.acl-main.746
AUTHORS:                Elias Stengel-Eskin, Aaron Steven White, Sheng Zhang, Benjamin Van Durme
HIGHLIGHT:              We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS)
representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with
decompositional semantic attribute scores.

747, TITLE:             Unsupervised Cross-lingual Representation Learning at Scale
https://www.aclweb.org/anthology/2020.acl-main.747
AUTHORS:               Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzm&aacute;n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov
HIGHLIGHT:             This paper shows that pretraining multilingual language models at scale leads to significant performance gains
for a wide range of cross-lingual transfer tasks.

748, TITLE:            A Generate-and-Rank Framework with Semantic Type Regularization for Biomedical Concept Normalization
https://www.aclweb.org/anthology/2020.acl-main.748
AUTHORS:               Dongfang Xu, Zeyu Zhang, Steven Bethard
HIGHLIGHT:             In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on
BERT.

749, TITLE:            Hierarchical Entity Typing via Multi-level Learning to Rank
https://www.aclweb.org/anthology/2020.acl-main.749
AUTHORS:               Tongfei Chen, Yunmo Chen, Benjamin Van Durme
HIGHLIGHT:             We propose a novel method for hierarchical entity classification that embraces ontological structure at both
training and during prediction.

750, TITLE:            Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference
https://www.aclweb.org/anthology/2020.acl-main.750
AUTHORS:               Jing Wang, Mayank Kulkarni, Daniel Preotiuc-Pietro
HIGHLIGHT:             We introduce a new architecture tailored to this task by using shared and private domain parameters and multi-
task learning.

751, TITLE:            TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories
https://www.aclweb.org/anthology/2020.acl-main.751
AUTHORS:               Giannis Karamanolakis, Jun Ma, Xin Luna Dong
HIGHLIGHT:             This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of
product categories organized in a hierarchical taxonomy.

752, TITLE:            TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition
https://www.aclweb.org/anthology/2020.acl-main.752
AUTHORS:               Bill Yuchen Lin, Dong-Ho Lee, Ming Shen, Ryan Moreno, Xiao Huang, Prashant Shiralkar, Xiang Ren
HIGHLIGHT:             In this paper, we introduce "entity triggers," an effective proxy of human explanations for facilitating label-
efficient learning of NER models.

753, TITLE:            Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.753
AUTHORS:               Arya D. McCarthy, Xian Li, Jiatao Gu, Ning Dong
HIGHLIGHT:             This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional
variational autoencoders (CVAEs).

754, TITLE:            Balancing Training for Multilingual Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.754
AUTHORS:               Xinyi Wang, Yulia Tsvetkov, Graham Neubig
HIGHLIGHT:             In this paper, we propose a method that instead automatically learns how to weight training data through a data
scorer that is optimized to maximize performance on all test languages.

755, TITLE:            Evaluating Robustness to Input Perturbations for Neural Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.755
AUTHORS:               Xing Niu, Prashant Mathur, Georgiana Dinu, Yaser Al-Onaizan
HIGHLIGHT:             This paper proposes additional metrics which measure the relative degradation and changes in translation when
small perturbations are added to the input.

756, TITLE:            Parallel Corpus Filtering via Pre-trained Language Models
https://www.aclweb.org/anthology/2020.acl-main.756
AUTHORS:               Boliang Zhang, Ajay Nagesh, Kevin Knight
HIGHLIGHT:             In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-
trained language models.

757, TITLE:             Regularized Context Gates on Transformer for Machine Translation
https://www.aclweb.org/anthology/2020.acl-main.757
AUTHORS:                Xintong Li, Lemao Liu, Rui Wang, Guoping Huang, Max Meng
HIGHLIGHT:              This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to
control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this
paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise
mutual information.

758, TITLE:             A Multi-Perspective Architecture for Semantic Code Search
https://www.aclweb.org/anthology/2020.acl-main.758
AUTHORS:                Rajarshi Haldar, Lingfei Wu, JinJun Xiong, Julia Hockenmaier
HIGHLIGHT:              In this paper, we propose a novel multi-perspective cross-lingual neural framework for code-text matching,
inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities.

759, TITLE:             Automated Topical Component Extraction Using Neural Network Attention Scores from Source-based Essay
Scoring
https://www.aclweb.org/anthology/2020.acl-main.759
AUTHORS:                Haoran Zhang, Diane Litman
HIGHLIGHT:              This paper presents a method for linking AWE and neural AES, by extracting Topical Components (TCs)
representing evidence from a source text using the intermediate output of attention layers.

760, TITLE:             Clinical Concept Linking with Contextualized Neural Representations
https://www.aclweb.org/anthology/2020.acl-main.760
AUTHORS:                Elliot Schumacher, Andriy Mulyar, Mark Dredze
HIGHLIGHT:              We propose an approach to concept linking that leverages recent work in contextualized neural models, such as
ELMo (Peters et al. 2018), which create a token representation that integrates the surrounding context of the mention and concept
name.

761, TITLE:             DeSePtion: Dual Sequence Prediction and Adversarial Examples for Improved Fact-Checking
https://www.aclweb.org/anthology/2020.acl-main.761
AUTHORS:                Christopher Hidey, Tuhin Chakrabarty, Tariq Alhindi, Siddharth Varia, Kriste Krstovski, Mona Diab, Smaranda
Muresan
HIGHLIGHT:              We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-
checking – multiple propositions, temporal reasoning, and ambiguity and lexical variation – and introduce a resource with these types
of claims. Then we present a system designed to be resilient to these “attacks” using multiple pointer networks for document selection
and jointly modeling a sequence of evidence sentences and veracity relation predictions.

762, TITLE:             Let Me Choose: From Verbal Context to Font Selection
https://www.aclweb.org/anthology/2020.acl-main.762
AUTHORS:                Amirreza Shirani, Franck Dernoncourt, Jose Echevarria, Paul Asente, Nedim Lipka, Thamar Solorio
HIGHLIGHT:              In this paper, we aim to learn associations between visual attributes of fonts and the verbal context of the texts
they are typically applied to.
We introduce a new dataset, containing examples of different topics in social media posts and ads, labeled through crowd-sourcing.

763, TITLE:             Multi-Label and Multilingual News Framing Analysis
https://www.aclweb.org/anthology/2020.acl-main.763
AUTHORS:                Afra Feyza Aky&uuml;rek, Lei Guo, Randa Elanwar, Prakash Ishwar, Margrit Betke, Derry Tanti Wijaya
HIGHLIGHT:              In this work, we explore multilingual transfer learning to detect multiple frames from just the news headline in a
genuinely low-resource context where there are few/no frame annotations in the target language.

764, TITLE:             Predicting Performance for Natural Language Processing Tasks
https://www.aclweb.org/anthology/2020.acl-main.764
AUTHORS:                Mengzhou Xia, Antonios Anastasopoulos, Ruochen Xu, Yiming Yang, Graham Neubig
HIGHLIGHT:              In this work, we attempt to explore the possibility of gaining plausible judgments of how well an NLP model
can perform under an experimental setting, \textit{without actually training or testing the model}.

765, TITLE:             ScriptWriter: Narrative-Guided Script Generation
https://www.aclweb.org/anthology/2020.acl-main.765
AUTHORS:                Yutao Zhu, Ruihua Song, Zhicheng Dou, Jian-Yun NIE, Jin Zhou
HIGHLIGHT:              In this paper, we address a key problem involved in these applications - guiding a dialogue by a narrative.

766, TITLE:             Should All Cross-Lingual Embeddings Speak English?
https://www.aclweb.org/anthology/2020.acl-main.766
AUTHORS:                Antonios Anastasopoulos, Graham Neubig
HIGHLIGHT:              First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot
POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all
language pairs using triangulation, and create new dictionaries for under-represented languages.

767, TITLE:             Smart To-Do: Automatic Generation of To-Do Items from Emails
https://www.aclweb.org/anthology/2020.acl-main.767
AUTHORS:                Sudipto Mukherjee, Subhabrata Mukherjee, Marcello Hasegawa, Ahmed Hassan Awadallah, Ryen White
HIGHLIGHT:              In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails.

768, TITLE:             Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition
https://www.aclweb.org/anthology/2020.acl-main.768
AUTHORS:                Paloma Jeretic, Alex Warstadt, Suvrat Bhooshan, Adina Williams
HIGHLIGHT:              We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams
et al., 2018) learn to make pragmatic inferences.

769, TITLE:             End-to-End Bias Mitigation by Modelling Biases in Corpora
https://www.aclweb.org/anthology/2020.acl-main.769
AUTHORS:                Rabeeh Karimi Mahabadi, Yonatan Belinkov, James Henderson
HIGHLIGHT:              We propose two learning strategies to train neural models, which are more robust to such biases and transfer
better to out-of-domain datasets.

770, TITLE:             Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance
https://www.aclweb.org/anthology/2020.acl-main.770
AUTHORS:                Prasetya Ajie Utama, Nafise Sadat Moosavi, Iryna Gurevych
HIGHLIGHT:              In this paper, we address this trade-off by introducing a novel debiasing method, called confidence
regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the
training examples.

771, TITLE:             NILE : Natural Language Inference with Faithful Natural Language Explanations
https://www.aclweb.org/anthology/2020.acl-main.771
AUTHORS:                Sawan Kumar, Partha Talukdar
HIGHLIGHT:              We propose Natural-language Inference over Label-specific Explanations (NILE), a novel NLI method which
utilizes auto-generated label-specific NL explanations to produce labels along with its faithful explanation.

772, TITLE:             QuASE: Question-Answer Driven Sentence Encoding
https://www.aclweb.org/anthology/2020.acl-main.772
AUTHORS:                Hangfeng He, Qiang Ning, Dan Roth
HIGHLIGHT:              This paper studies a natural question: Can we get supervision from QA data for other tasks (typically, non-QA
ones)?

773, TITLE:             Towards Robustifying NLI Models Against Lexical Dataset Biases
https://www.aclweb.org/anthology/2020.acl-main.773
AUTHORS:                Xiang Zhou, Mohit Bansal
HIGHLIGHT:              Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both
data-level and model-level debiasing methods to robustify models against lexical dataset biases.

774, TITLE:             Uncertain Natural Language Inference
https://www.aclweb.org/anthology/2020.acl-main.774
AUTHORS:                Tongfei Chen, Zhengping Jiang, Adam Poliak, Keisuke Sakaguchi, Benjamin Van Durme
HIGHLIGHT:              We introduce Uncertain Natural Language Inference (UNLI), a refinement of Natural Language Inference
(NLI) that shifts away from categorical labels, targeting instead the direct prediction of subjective probability assessments.

775, TITLE:             Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging, and Joint Modeling Approaches
https://www.aclweb.org/anthology/2020.acl-main.775
AUTHORS:                Tianze Shi, Lillian Lee
HIGHLIGHT:              We empirically compare these two common strategies—parsing and tagging—for predicting flat MWEs.
Additionally, we propose an efficient joint decoding algorithm that combines scores from both strategies.

776, TITLE:            Revisiting Higher-Order Dependency Parsers
https://www.aclweb.org/anthology/2020.acl-main.776
AUTHORS:               Erick Fonseca, Andr&eacute; F. T. Martins
HIGHLIGHT:             We tested this hypothesis and found that neural parsers may benefit from higher-order features, even when
employing a powerful pre-trained encoder, such as BERT.

777, TITLE:            SeqVAT: Virtual Adversarial Training for Semi-Supervised Sequence Labeling
https://www.aclweb.org/anthology/2020.acl-main.777
AUTHORS:               Luoxin Chen, Weitong Ruan, Xinyue Liu, Jianhua Lu
HIGHLIGHT:             In this paper, we propose SeqVAT, a method which naturally applies VAT to sequence labeling models with
CRF.

778, TITLE:            Treebank Embedding Vectors for Out-of-domain Dependency Parsing
https://www.aclweb.org/anthology/2020.acl-main.778
AUTHORS:               Joachim Wagner, James Barry, Jennifer Foster
HIGHLIGHT:             We build on this idea by 1) introducing a method to predict a treebank vector for sentences that do not come
from a treebank used in training, and 2) exploring what happens when we move away from predefined treebank embedding vectors
during test time and instead devise tailored interpolations.
